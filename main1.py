import urllib.request
from urllib.parse import urlparse
import re
import os
from datetime import datetime, timedelta, timezone
import random
import opencc

import socket
import time

# åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
os.makedirs('output/custom1/', exist_ok=True)

# ç®€ç¹è½¬æ¢
def traditional_to_simplified(text: str) -> str:
    converter = opencc.OpenCC('t2s')  # t2sï¼šç¹ä½“è½¬ç®€ä½“
    return converter.convert(text)

# æ‰§è¡Œå¼€å§‹æ—¶é—´
timestart = datetime.now()

# è¯»å–æ–‡æœ¬æ–¹æ³•ï¼ˆé€šç”¨ï¼‰
def read_txt_to_array(file_name):
    try:
        with open(file_name, 'r', encoding='utf-8') as file:
            # è·³è¿‡ç©ºè¡Œå¹¶å»é™¤é¦–å°¾ç©ºæ ¼
            return [line.strip() for line in file.readlines() if line.strip()]
    except FileNotFoundError:
        print(f"File '{file_name}' not found.")
        return []
    except Exception as e:
        print(f"Read file error: {e}")
        return []

# è¯»å–é»‘åå•ï¼ˆä¿®å¤ï¼šé¿å…splitç´¢å¼•è¶Šç•Œï¼‰
def read_blacklist_from_txt(file_path):
    blacklist = []
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file.readlines():
            line = line.strip()
            if ',' in line and len(line.split(',')) >= 2:  # ç¡®ä¿æœ‰2ä¸ªä»¥ä¸Šå…ƒç´ 
                blacklist.append(line.split(',')[1].strip())
    return blacklist

# åŠ è½½é»‘åå•ï¼ˆæ”¹ç”¨é›†åˆæå‡æ£€ç´¢é€Ÿåº¦ï¼‰
blacklist_auto = read_blacklist_from_txt('assets/blacklist1/blacklist_auto.txt') 
blacklist_manual = read_blacklist_from_txt('assets/blacklist1/blacklist_manual.txt') 
combined_blacklist = set(blacklist_auto + blacklist_manual)  # é›†åˆæ£€ç´¢æ•ˆç‡é«˜äºåˆ—è¡¨

# å®šä¹‰é¢‘é“åˆ†ç±»å­˜å‚¨åˆ—è¡¨
ys_lines = []  # CCTV
ws_lines = []  # å«è§†é¢‘é“
sh_lines = []  # åœ°æ–¹å°-ä¸Šæµ·
zj_lines = []  # åœ°æ–¹å°-æµ™æ±Ÿ
jsu_lines = []  # åœ°æ–¹å°-æ±Ÿè‹
gd_lines = []  # åœ°æ–¹å°-å¹¿ä¸œ
hn_lines = []  # åœ°æ–¹å°-æ¹–å—
ah_lines = []  # åœ°æ–¹å°-å®‰å¾½
hain_lines = []  # åœ°æ–¹å°-æµ·å—
nm_lines = []  # åœ°æ–¹å°-å†…è’™
hb_lines = []  # åœ°æ–¹å°-æ¹–åŒ—
ln_lines = []  # åœ°æ–¹å°-è¾½å®
sx_lines = []  # åœ°æ–¹å°-é™•è¥¿
shanxi_lines = []  # åœ°æ–¹å°-å±±è¥¿
shandong_lines = []  # åœ°æ–¹å°-å±±ä¸œ
yunnan_lines = []  # åœ°æ–¹å°-äº‘å—
bj_lines = []  # åœ°æ–¹å°-åŒ—äº¬
cq_lines = []  # åœ°æ–¹å°-é‡åº†
fj_lines = []  # åœ°æ–¹å°-ç¦å»º
gs_lines = []  # åœ°æ–¹å°-ç”˜è‚ƒ
gx_lines = []  # åœ°æ–¹å°-å¹¿è¥¿
gz_lines = []  # åœ°æ–¹å°-è´µå·
heb_lines = []  # åœ°æ–¹å°-æ²³åŒ—
hen_lines = []  # åœ°æ–¹å°-æ²³å—
hlj_lines = []  # åœ°æ–¹å°-é»‘é¾™æ±Ÿ
jl_lines = []  # åœ°æ–¹å°-å‰æ—
jx_lines = []  # åœ°æ–¹å°-æ±Ÿè¥¿
nx_lines = []  # åœ°æ–¹å°-å®å¤
qh_lines = []  # åœ°æ–¹å°-é’æµ·
sc_lines = []  # åœ°æ–¹å°-å››å·
tj_lines = []  # åœ°æ–¹å°-å¤©æ´¥
xj_lines = []  # åœ°æ–¹å°-æ–°ç–†

ty_lines = []  # ä½“è‚²é¢‘é“
tyss_lines = []  # ä½“è‚²èµ›äº‹
sz_lines = []  # æ•°å­—é¢‘é“
yy_lines = []  # éŸ³ä¹é¢‘é“
gj_lines = []  # å›½é™…é¢‘é“
js_lines = []  # è§£è¯´
cw_lines = []  # æ˜¥æ™š
dy_lines = []  # ç”µå½±
dsj_lines = []  # ç”µè§†å‰§
gat_lines = []  # æ¸¯æ¾³å°
xg_lines = []  # é¦™æ¸¯
aomen_lines = []  # æ¾³é—¨
tw_lines = []  # å°æ¹¾
dhp_lines = []  # åŠ¨ç”»ç‰‡
douyu_lines = []  # æ–—é±¼ç›´æ’­
huya_lines = []  # è™ç‰™ç›´æ’­
radio_lines = []  # æ”¶éŸ³æœº
zb_lines = []  # ç›´æ’­ä¸­å›½
zy_lines = []  # ç»¼è‰ºé¢‘é“
game_lines = []  # æ¸¸æˆé¢‘é“
xq_lines = []  # æˆæ›²é¢‘é“
jlp_lines = []  # è®°å½•ç‰‡

other_lines = []
other_lines_url = []  # å»é‡ç”¨ï¼šå­˜å‚¨å·²åŠ å…¥otherçš„URL

# é¢‘é“åç§°æ ¼å¼åŒ–ï¼ˆCCTV/å«è§†ç‰¹æ®Šå¤„ç†ï¼‰
def process_name_string(input_str):
    return ','.join([process_part(part) for part in input_str.split(',')])

def process_part(part_str):
    # å¤„ç†CCTVé¢‘é“ï¼ˆæ¸…ç†å†—ä½™å­—ç¬¦+æ ‡å‡†åŒ–æ ¼å¼ï¼‰
    if "CCTV" in part_str and "://" not in part_str:
        part_str = part_str.replace("IPV6", "").replace("PLUS", "+").replace("1080", "")
        # æå–æ•°å­—ã€Kã€+å­—ç¬¦
        filtered_str = ''.join([c for c in part_str if c.isdigit() or c in ('K', '+')])
        # æ— æœ‰æ•ˆå­—ç¬¦æ—¶ä¿ç•™åŸå§‹åç§°ï¼ˆå»é™¤CCTVï¼‰
        if not filtered_str.strip():
            filtered_str = part_str.replace("CCTV", "")
        # 4K/8Kæ ¼å¼æ ‡å‡†åŒ–ï¼ˆå¦‚CCTV4Kâ†’CCTV(4K)ï¼‰
        if len(filtered_str) > 2 and re.search(r'4K|8K', filtered_str):
            filtered_str = re.sub(r'(4K|8K).*', r'\1', filtered_str)
            if len(filtered_str) > 2:
                filtered_str = re.sub(r'(4K|8K)', r'(\1)', filtered_str)
        return f"CCTV{filtered_str}"
    
    # å¤„ç†å«è§†é¢‘é“ï¼ˆæ¸…ç†â€œå«è§†ã€Œxxxã€â€æ ¼å¼ï¼‰
    elif "å«è§†" in part_str:
        return re.sub(r'å«è§†ã€Œ.*ã€', 'å«è§†', part_str)
    
    return part_str

# M3Uæ ¼å¼å¤„ç†ï¼ˆè½¬æ¢ä¸ºâ€œé¢‘é“å,URLâ€æ–‡æœ¬æ ¼å¼ï¼‰
def get_url_file_extension(url):
    return os.path.splitext(urlparse(url).path)[1]

def convert_m3u_to_txt(m3u_content):
    txt_lines = []
    channel_name = ""
    for line in m3u_content.split('\n'):
        line = line.strip()
        if line.startswith("#EXTM3U"):
            continue
        # æå–é¢‘é“åï¼ˆEXTINFè¡Œï¼‰
        elif line.startswith("#EXTINF"):
            channel_name = line.split(',')[-1].strip()
        # æå–URLï¼ˆhttp/rtmp/p3på¼€å¤´ï¼‰
        elif line.startswith(("http", "rtmp", "p3p")):
            txt_lines.append(f"{channel_name},{line}")
        # å…¼å®¹â€œé¢‘é“å,URLâ€æ ¼å¼çš„M3Uæ–‡ä»¶
        elif "#genre#" not in line and "," in line and "://" in line:
            if re.match(r'^[^,]+,[^\s]+://[^\s]+$', line):
                txt_lines.append(line)
    return '\n'.join(txt_lines)

# URLå»é‡æ£€æŸ¥ï¼ˆåˆ¤æ–­URLæ˜¯å¦å·²å­˜åœ¨äºåˆ—è¡¨ï¼‰
def check_url_existence(data_list, url):
    return url not in [item.split(',')[1] for item in data_list]

# URLæ¸…ç†ï¼ˆå»é™¤$åŠå…¶åé¢çš„å‚æ•°ï¼‰
def clean_url(url):
    last_dollar_idx = url.rfind('$')
    return url[:last_dollar_idx] if last_dollar_idx != -1 else url

# é¢‘é“åæ¸…ç†ï¼ˆå»é™¤å†—ä½™å…³é”®è¯ï¼‰
removal_list = [
    "_ç”µä¿¡", "ç”µä¿¡", "é«˜æ¸…", "é¢‘é“", "ï¼ˆHDï¼‰", "-HD", "è‹±é™†", "_ITV", "(åŒ—ç¾)", "(HK)", "AKtv",
    "ã€ŒIPV4ã€", "ã€ŒIPV6ã€", "é¢‘é™†", "å¤‡é™†", "å£¹é™†", "è´°é™†", "åé™†", "è‚†é™†", "ä¼é™†", "é™†é™†", "æŸ’é™†",
    "é¢‘æ™´", "é¢‘ç²¤", "[è¶…æ¸…]", "è¶…æ¸…", "æ ‡æ¸…", "æ–¯ç‰¹", "ç²¤é™†", "å›½é™†", "è‚†æŸ’", "é¢‘è‹±", "é¢‘ç‰¹",
    "é¢‘å›½", "é¢‘å£¹", "é¢‘è´°", "è‚†è´°", "é¢‘æµ‹", "å’ªå’•", "é—½ç‰¹", "é«˜ç‰¹", "é¢‘é«˜", "é¢‘æ ‡", "æ±é˜³"
]
def clean_channel_name(channel_name):
    for item in removal_list:
        channel_name = channel_name.replace(item, "")
    # å»é™¤æœ«å°¾HD/å°ï¼ˆæ»¡è¶³é•¿åº¦æ¡ä»¶æ—¶ï¼‰
    if channel_name.endswith("HD"):
        channel_name = channel_name[:-2]
    if channel_name.endswith("å°") and len(channel_name) > 3:
        channel_name = channel_name[:-1]
    return channel_name

# é¢‘é“åˆ†ç±»åˆ†å‘ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼šæŒ‰åç§°åŒ¹é…å­—å…¸åˆ†ç±»ï¼‰
def process_channel_line(line):
    if "#genre#" not in line and "#EXTINF:" not in line and "," in line and "://" in line:
        # åŸºç¡€æ¸…ç†ï¼šé¢‘é“åå»å†—ä½™+ç®€ç¹è½¬æ¢ï¼ŒURLæ¸…ç†$å‚æ•°
        channel_name = clean_channel_name(line.split(',')[0].strip())
        channel_name = traditional_to_simplified(channel_name)
        channel_address = clean_url(line.split(',')[1].strip())
        line = f"{channel_name},{channel_address}"
        
        # é»‘åå•è¿‡æ»¤+URLå»é‡ååˆ†ç±»å­˜å‚¨
        if channel_address not in combined_blacklist:
            # CCTVé¢‘é“
            if "CCTV" in channel_name and check_url_existence(ys_lines, channel_address):
                ys_lines.append(process_name_string(line.strip()))
            # å«è§†é¢‘é“
            elif channel_name in ws_dictionary and check_url_existence(ws_lines, channel_address):
                ws_lines.append(process_name_string(line.strip()))
            # æµ™æ±Ÿé¢‘é“
            elif channel_name in zj_dictionary and check_url_existence(zj_lines, channel_address):
                zj_lines.append(process_name_string(line.strip()))
            # æ±Ÿè‹é¢‘é“
            elif channel_name in jsu_dictionary and check_url_existence(jsu_lines, channel_address):
                jsu_lines.append(process_name_string(line.strip()))
            # å¹¿ä¸œé¢‘é“
            elif channel_name in gd_dictionary and check_url_existence(gd_lines, channel_address):
                gd_lines.append(process_name_string(line.strip()))
            # æ¹–å—é¢‘é“
            elif channel_name in hn_dictionary and check_url_existence(hn_lines, channel_address):
                hn_lines.append(process_name_string(line.strip()))
            # æ¹–åŒ—é¢‘é“
            elif channel_name in hb_dictionary and check_url_existence(hb_lines, channel_address):
                hb_lines.append(process_name_string(line.strip()))
            # å®‰å¾½é¢‘é“
            elif channel_name in ah_dictionary and check_url_existence(ah_lines, channel_address):
                ah_lines.append(process_name_string(line.strip()))
            # æµ·å—é¢‘é“
            elif channel_name in hain_dictionary and check_url_existence(hain_lines, channel_address):
                hain_lines.append(process_name_string(line.strip()))
            # å†…è’™é¢‘é“
            elif channel_name in nm_dictionary and check_url_existence(nm_lines, channel_address):
                nm_lines.append(process_name_string(line.strip()))
            # è¾½å®é¢‘é“
            elif channel_name in ln_dictionary and check_url_existence(ln_lines, channel_address):
                ln_lines.append(process_name_string(line.strip()))
            # é™•è¥¿é¢‘é“
            elif channel_name in sx_dictionary and check_url_existence(sx_lines, channel_address):
                sx_lines.append(process_name_string(line.strip()))
            # å±±è¥¿é¢‘é“
            elif channel_name in shanxi_dictionary and check_url_existence(shanxi_lines, channel_address):
                shanxi_lines.append(process_name_string(line.strip()))
            # å±±ä¸œé¢‘é“
            elif channel_name in shandong_dictionary and check_url_existence(shandong_lines, channel_address):
                shandong_lines.append(process_name_string(line.strip()))
            # äº‘å—é¢‘é“
            elif channel_name in yunnan_dictionary and check_url_existence(yunnan_lines, channel_address):
                yunnan_lines.append(process_name_string(line.strip()))
            # åŒ—äº¬é¢‘é“
            elif channel_name in bj_dictionary and check_url_existence(bj_lines, channel_address):
                bj_lines.append(process_name_string(line.strip()))
            # é‡åº†é¢‘é“
            elif channel_name in cq_dictionary and check_url_existence(cq_lines, channel_address):
                cq_lines.append(process_name_string(line.strip()))
            # ç¦å»ºé¢‘é“ï¼ˆä¿®å¤ï¼šç¼©è¿›ä¸å…¶ä»–é¢‘é“ä¸€è‡´ï¼‰
            elif channel_name in fj_dictionary and check_url_existence(fj_lines, channel_address):
                fj_lines.append(process_name_string(line.strip()))
            # ç”˜è‚ƒé¢‘é“
            elif channel_name in gs_dictionary and check_url_existence(gs_lines, channel_address):
                gs_lines.append(process_name_string(line.strip()))
            # å¹¿è¥¿é¢‘é“
            elif channel_name in gx_dictionary and check_url_existence(gx_lines, channel_address):
                gx_lines.append(process_name_string(line.strip()))
            # è´µå·é¢‘é“
            elif channel_name in gz_dictionary and check_url_existence(gz_lines, channel_address):
                gz_lines.append(process_name_string(line.strip()))
            # æ²³åŒ—é¢‘é“
            elif channel_name in heb_dictionary and check_url_existence(heb_lines, channel_address):
                heb_lines.append(process_name_string(line.strip()))
            # æ²³å—é¢‘é“
            elif channel_name in hen_dictionary and check_url_existence(hen_lines, channel_address):
                hen_lines.append(process_name_string(line.strip()))
            # é»‘é¾™æ±Ÿé¢‘é“
            elif channel_name in hlj_dictionary and check_url_existence(hlj_lines, channel_address):
                hlj_lines.append(process_name_string(line.strip()))
            # å‰æ—é¢‘é“
            elif channel_name in jl_dictionary and check_url_existence(jl_lines, channel_address):
                jl_lines.append(process_name_string(line.strip()))
            # å®å¤é¢‘é“
            elif channel_name in nx_dictionary and check_url_existence(nx_lines, channel_address):
                nx_lines.append(process_name_string(line.strip()))
            # æ±Ÿè¥¿é¢‘é“
            elif channel_name in jx_dictionary and check_url_existence(jx_lines, channel_address):
                jx_lines.append(process_name_string(line.strip()))
            # é’æµ·é¢‘é“
            elif channel_name in qh_dictionary and check_url_existence(qh_lines, channel_address):
                qh_lines.append(process_name_string(line.strip()))
            # å››å·é¢‘é“
            elif channel_name in sc_dictionary and check_url_existence(sc_lines, channel_address):
                sc_lines.append(process_name_string(line.strip()))
            # ä¸Šæµ·é¢‘é“
            elif channel_name in sh_dictionary and check_url_existence(sh_lines, channel_address):
                sh_lines.append(process_name_string(line.strip()))
            # å¤©æ´¥é¢‘é“
            elif channel_name in tj_dictionary and check_url_existence(tj_lines, channel_address):
                tj_lines.append(process_name_string(line.strip()))
            # æ–°ç–†é¢‘é“
            elif channel_name in xj_dictionary and check_url_existence(xj_lines, channel_address):
                xj_lines.append(process_name_string(line.strip()))
            # æ•°å­—é¢‘é“
            elif channel_name in sz_dictionary and check_url_existence(sz_lines, channel_address):
                sz_lines.append(process_name_string(line.strip()))
            # å›½é™…é¢‘é“
            elif channel_name in gj_dictionary and check_url_existence(gj_lines, channel_address):
                gj_lines.append(process_name_string(line.strip()))
            # ä½“è‚²é¢‘é“
            elif channel_name in ty_dictionary and check_url_existence(ty_lines, channel_address):
                ty_lines.append(process_name_string(line.strip()))
            # ä½“è‚²èµ›äº‹ï¼ˆä¿®å¤ï¼šå¾ªç¯å˜é‡ä¸åˆ—è¡¨åé‡åé—®é¢˜ï¼‰
            elif any(tyss_key in channel_name for tyss_key in tyss_dictionary) and check_url_existence(tyss_lines, channel_address):
                tyss_lines.append(process_name_string(line.strip()))
            # ç”µå½±é¢‘é“
            elif channel_name in dy_dictionary and check_url_existence(dy_lines, channel_address):
                dy_lines.append(process_name_string(line.strip()))
            # ç”µè§†å‰§é¢‘é“
            elif channel_name in dsj_dictionary and check_url_existence(dsj_lines, channel_address):
                dsj_lines.append(process_name_string(line.strip()))
            # æ¸¯æ¾³å°é¢‘é“
            elif channel_name in gat_dictionary and check_url_existence(gat_lines, channel_address):
                gat_lines.append(process_name_string(line.strip()))
            # é¦™æ¸¯é¢‘é“
            elif channel_name in xg_dictionary and check_url_existence(xg_lines, channel_address):
                xg_lines.append(process_name_string(line.strip()))
            # æ¾³é—¨é¢‘é“
            elif channel_name in aomen_dictionary and check_url_existence(aomen_lines, channel_address):
                aomen_lines.append(process_name_string(line.strip()))
            # å°æ¹¾é¢‘é“
            elif channel_name in tw_dictionary and check_url_existence(tw_lines, channel_address):
                tw_lines.append(process_name_string(line.strip()))
            # çºªå½•ç‰‡é¢‘é“
            elif channel_name in jlp_dictionary and check_url_existence(jlp_lines, channel_address):
                jlp_lines.append(process_name_string(line.strip()))
            # åŠ¨ç”»ç‰‡é¢‘é“
            elif channel_name in dhp_dictionary and check_url_existence(dhp_lines, channel_address):
                dhp_lines.append(process_name_string(line.strip()))
            # æˆæ›²é¢‘é“
            elif channel_name in xq_dictionary and check_url_existence(xq_lines, channel_address):
                xq_lines.append(process_name_string(line.strip()))
            # è§£è¯´é¢‘é“
            elif channel_name in js_dictionary and check_url_existence(js_lines, channel_address):
                js_lines.append(process_name_string(line.strip()))
            # æ˜¥æ™šé¢‘é“
            elif channel_name in cw_dictionary and check_url_existence(cw_lines, channel_address):
                cw_lines.append(process_name_string(line.strip()))
            # æ–—é±¼ç›´æ’­
            elif channel_name in douyu_dictionary and check_url_existence(douyu_lines, channel_address):
                douyu_lines.append(process_name_string(line.strip()))
            # è™ç‰™ç›´æ’­
            elif channel_name in huya_dictionary and check_url_existence(huya_lines, channel_address):
                huya_lines.append(process_name_string(line.strip()))
            # ç»¼è‰ºé¢‘é“
            elif channel_name in zy_dictionary and check_url_existence(zy_lines, channel_address):
                zy_lines.append(process_name_string(line.strip()))
            # éŸ³ä¹é¢‘é“
            elif channel_name in yy_dictionary and check_url_existence(yy_lines, channel_address):
                yy_lines.append(process_name_string(line.strip()))
            # æ¸¸æˆé¢‘é“
            elif channel_name in game_dictionary and check_url_existence(game_lines, channel_address):
                game_lines.append(process_name_string(line.strip()))
            # æ”¶éŸ³æœºé¢‘é“
            elif channel_name in radio_dictionary and check_url_existence(radio_lines, channel_address):
                radio_lines.append(process_name_string(line.strip()))
            # ç›´æ’­ä¸­å›½
            elif channel_name in zb_dictionary and check_url_existence(zb_lines, channel_address):
                zb_lines.append(process_name_string(line.strip()))
            # å…¶ä»–é¢‘é“ï¼ˆå»é‡ï¼‰
            else:
                if channel_address not in other_lines_url:
                    other_lines_url.append(channel_address)
                    other_lines.append(line.strip())

# éšæœºUser-Agentï¼ˆå¤‡ç”¨ï¼‰
def get_random_user_agent():
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36",
    ]
    return random.choice(USER_AGENTS)

# URLå†…å®¹çˆ¬å–ä¸è§£æ
def process_url(url):
    try:
        other_lines.append(f"â—†â—†â—†ã€€{url}")  # è®°å½•å¤„ç†è¿‡çš„URLï¼ˆç”¨äºè°ƒè¯•ï¼‰
        
        # æ„å»ºè¯·æ±‚ï¼ˆå›ºå®šUser-Agentï¼Œé¿å…è¢«æ‹¦æˆªï¼‰
        req = urllib.request.Request(url)
        req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3')
        
        # è¯»å–URLå†…å®¹
        with urllib.request.urlopen(req) as response:
            text = response.read().decode('utf-8').strip()
            
            # åˆ¤å®šM3Uæ ¼å¼ï¼ˆæ‰©å±•åæˆ–å†…å®¹å¼€å¤´ï¼‰
            is_m3u = text.startswith(("#EXTM3U", "#EXTINF"))
            if get_url_file_extension(url) in (".m3u", ".m3u8") or is_m3u:
                text = convert_m3u_to_txt(text)
            
            # é€è¡Œè§£æï¼ˆè¿‡æ»¤æ— æ•ˆæ ¼å¼ï¼Œå¤„ç†å¸¦#çš„å¤šURLï¼‰
            for line in text.split('\n'):
                line = line.strip()
                if "#genre#" not in line and "," in line and "://" in line and "tvbus://" not in line and "/udp/" not in line:
                    channel_name, channel_address = line.split(',', 1)
                    # å¤„ç†å¸¦#çš„å¤šURLï¼ˆå¦‚URL#å¤‡ç”¨1#å¤‡ç”¨2ï¼‰
                    if "#" in channel_address:
                        for sub_url in channel_address.split('#'):
                            process_channel_line(f"{channel_name},{sub_url}")
                    else:
                        process_channel_line(line)
        
        other_lines.append('\n')  # åˆ†éš”ä¸åŒURLçš„è®°å½•
    
    except Exception as e:
        print(f"Process URL error: {url} -> {e}")

# åŠ è½½é¢‘é“åˆ†ç±»å­—å…¸ï¼ˆä»æœ¬åœ°TXTè¯»å–ï¼‰
# ä¸»é¢‘é“å­—å…¸
ys_dictionary = read_txt_to_array('ä¸»é¢‘é“/CCTV.txt')
ws_dictionary = read_txt_to_array('ä¸»é¢‘é“/å«è§†é¢‘é“.txt')
cw_dictionary = read_txt_to_array('ä¸»é¢‘é“/æ˜¥æ™š.txt')
dy_dictionary = read_txt_to_array('ä¸»é¢‘é“/ç”µå½±.txt')
dsj_dictionary = read_txt_to_array('ä¸»é¢‘é“/ç”µè§†å‰§.txt')
gat_dictionary = read_txt_to_array('ä¸»é¢‘é“/æ¸¯æ¾³å°.txt')
xg_dictionary = read_txt_to_array('ä¸»é¢‘é“/é¦™æ¸¯.txt')
aomen_dictionary = read_txt_to_array('ä¸»é¢‘é“/æ¾³é—¨.txt')
tw_dictionary = read_txt_to_array('ä¸»é¢‘é“/å°æ¹¾.txt')
dhp_dictionary = read_txt_to_array('ä¸»é¢‘é“/åŠ¨ç”»ç‰‡.txt')
radio_dictionary = read_txt_to_array('ä¸»é¢‘é“/æ”¶éŸ³æœº.txt')
sz_dictionary = read_txt_to_array('ä¸»é¢‘é“/æ•°å­—é¢‘é“.txt')
gj_dictionary = read_txt_to_array('ä¸»é¢‘é“/å›½é™…é¢‘é“.txt')
ty_dictionary = read_txt_to_array('ä¸»é¢‘é“/ä½“è‚²é¢‘é“.txt')
tyss_dictionary = read_txt_to_array('ä¸»é¢‘é“/ä½“è‚²èµ›äº‹.txt')
yy_dictionary = read_txt_to_array('ä¸»é¢‘é“/éŸ³ä¹é¢‘é“.txt')
js_dictionary = read_txt_to_array('ä¸»é¢‘é“/è§£è¯´é¢‘é“.txt')
douyu_dictionary = read_txt_to_array('ä¸»é¢‘é“/æ–—é±¼ç›´æ’­.txt')
huya_dictionary = read_txt_to_array('ä¸»é¢‘é“/è™ç‰™ç›´æ’­.txt')
zb_dictionary = read_txt_to_array('ä¸»é¢‘é“/ç›´æ’­ä¸­å›½.txt')
jlp_dictionary = read_txt_to_array('ä¸»é¢‘é“/çºªå½•ç‰‡.txt')
zy_dictionary = read_txt_to_array('ä¸»é¢‘é“/ç»¼è‰ºé¢‘é“.txt')
game_dictionary = read_txt_to_array('ä¸»é¢‘é“/æ¸¸æˆé¢‘é“.txt')
xq_dictionary = read_txt_to_array('ä¸»é¢‘é“/æˆæ›²é¢‘é“.txt')

# åœ°æ–¹å°å­—å…¸
zj_dictionary = read_txt_to_array('åœ°æ–¹å°/æµ™æ±Ÿé¢‘é“.txt')
jsu_dictionary = read_txt_to_array('åœ°æ–¹å°/æ±Ÿè‹é¢‘é“.txt')
gd_dictionary = read_txt_to_array('åœ°æ–¹å°/å¹¿ä¸œé¢‘é“.txt')
gx_dictionary = read_txt_to_array('åœ°æ–¹å°/å¹¿è¥¿é¢‘é“.txt')
jx_dictionary = read_txt_to_array('åœ°æ–¹å°/æ±Ÿè¥¿é¢‘é“.txt')
hb_dictionary = read_txt_to_array('åœ°æ–¹å°/æ¹–åŒ—é¢‘é“.txt')
hn_dictionary = read_txt_to_array('åœ°æ–¹å°/æ¹–å—é¢‘é“.txt')
ah_dictionary = read_txt_to_array('åœ°æ–¹å°/å®‰å¾½é¢‘é“.txt')
hain_dictionary = read_txt_to_array('åœ°æ–¹å°/æµ·å—é¢‘é“.txt')
nm_dictionary = read_txt_to_array('åœ°æ–¹å°/å†…è’™é¢‘é“.txt')
ln_dictionary = read_txt_to_array('åœ°æ–¹å°/è¾½å®é¢‘é“.txt')
sx_dictionary = read_txt_to_array('åœ°æ–¹å°/é™•è¥¿é¢‘é“.txt')
shandong_dictionary = read_txt_to_array('åœ°æ–¹å°/å±±ä¸œé¢‘é“.txt')
shanxi_dictionary = read_txt_to_array('åœ°æ–¹å°/å±±è¥¿é¢‘é“.txt')
hen_dictionary = read_txt_to_array('åœ°æ–¹å°/æ²³å—é¢‘é“.txt')
heb_dictionary = read_txt_to_array('åœ°æ–¹å°/æ²³åŒ—é¢‘é“.txt')
yunnan_dictionary = read_txt_to_array('åœ°æ–¹å°/äº‘å—é¢‘é“.txt')
gz_dictionary = read_txt_to_array('åœ°æ–¹å°/è´µå·é¢‘é“.txt')
sc_dictionary = read_txt_to_array('åœ°æ–¹å°/å››å·é¢‘é“.txt')
fj_dictionary = read_txt_to_array('åœ°æ–¹å°/ç¦å»ºé¢‘é“.txt')
gs_dictionary = read_txt_to_array('åœ°æ–¹å°/ç”˜è‚ƒé¢‘é“.txt')
hlj_dictionary = read_txt_to_array('åœ°æ–¹å°/é»‘é¾™æ±Ÿé¢‘é“.txt')
jl_dictionary = read_txt_to_array('åœ°æ–¹å°/å‰æ—é¢‘é“.txt')
nx_dictionary = read_txt_to_array('åœ°æ–¹å°/å®å¤é¢‘é“.txt')
qh_dictionary = read_txt_to_array('åœ°æ–¹å°/é’æµ·é¢‘é“.txt')
xj_dictionary = read_txt_to_array('åœ°æ–¹å°/æ–°ç–†é¢‘é“.txt')
bj_dictionary = read_txt_to_array('åœ°æ–¹å°/åŒ—äº¬é¢‘é“.txt')
sh_dictionary = read_txt_to_array('åœ°æ–¹å°/ä¸Šæµ·é¢‘é“.txt')
tj_dictionary = read_txt_to_array('åœ°æ–¹å°/å¤©æ´¥é¢‘é“.txt')
cq_dictionary = read_txt_to_array('åœ°æ–¹å°/é‡åº†é¢‘é“.txt')

# é¢‘é“åçº é”™ï¼ˆä»TXTåŠ è½½â€œé”™è¯¯åâ†’æ­£ç¡®åâ€æ˜ å°„ï¼‰
def load_corrections_name(filename):
    corrections = {}
    with open(filename, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or ',' not in line:
                continue
            parts = line.split(',')
            correct_name = parts[0]
            for wrong_name in parts[1:]:
                corrections[wrong_name] = correct_name
    return corrections

corrections_name = load_corrections_name('assets/corrections_name.txt')

# æ‰§è¡Œé¢‘é“åçº é”™
def correct_name_data(corrections, data):
    corrected = []
    for line in data:
        line = line.strip()
        if ',' not in line:
            continue
        name, url = line.split(',', 1)
        # æ›¿æ¢é”™è¯¯åç§°
        if name in corrections:
            name = corrections[name]
        corrected.append(f"{name},{url}")
    return corrected

# æŒ‰å­—å…¸é¡ºåºæ’åºé¢‘é“
def sort_data(order_dict, data):
    order_map = {name: idx for idx, name in enumerate(order_dict)}
    # æœªåœ¨å­—å…¸ä¸­çš„é¢‘é“æ’åœ¨æœ€å
    def sort_key(line):
        return order_map.get(line.split(',')[0], len(order_map))
    return sorted(data, key=sort_key)

# å¤„ç†åŠ¨æ€URLï¼ˆæ›¿æ¢{MMdd}å’Œ{MMdd-1}ä¸ºå½“å‰/æ˜¨æ—¥æ—¥æœŸï¼‰
urls = read_txt_to_array('assets/urls-daily.txt')
for url in urls:
    if url.startswith("http"):
        # æ›¿æ¢å½“å‰æ—¥æœŸï¼ˆMMddæ ¼å¼ï¼‰
        if "{MMdd}" in url:
            url = url.replace("{MMdd}", datetime.now().strftime("%m%d"))
        # æ›¿æ¢æ˜¨æ—¥æ—¥æœŸï¼ˆMMddæ ¼å¼ï¼‰
        if "{MMdd-1}" in url:
            yesterday = datetime.now() - timedelta(days=1)
            url = url.replace("{MMdd-1}", yesterday.strftime("%m%d"))
        
        print(f"Processing URL: {url}")
        process_url(url)

# åŠ è½½é«˜å“åº”ç™½åå•ï¼ˆå“åº”æ—¶é—´<2000msçš„æºï¼‰
print("Adding whitelist_auto.txt (response < 2000ms)")
whitelist_auto_lines = read_txt_to_array('assets/blacklist1/whitelist_auto.txt')
for line in whitelist_auto_lines:
    if "#genre#" not in line and "," in line and "://" in line:
        parts = line.split(",")
        try:
            response_time = float(parts[0].replace("ms", ""))
        except ValueError:
            response_time = 60000  # è½¬æ¢å¤±è´¥é»˜è®¤60ç§’ï¼ˆä¸åŠ å…¥ï¼‰
        # ä»…ä¿ç•™2ç§’å†…çš„é«˜å“åº”æº
        if response_time < 2000:
            process_channel_line(",".join(parts[1:]))

# å¸¦é‡è¯•çš„HTTPè¯·æ±‚ï¼ˆä¼˜åŒ–çˆ¬å–ç¨³å®šæ€§ï¼‰
def get_http_response(url, timeout=8, retries=2, backoff_factor=1.0):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
    }
    for attempt in range(retries):
        try:
            # ä¿®å¤ï¼šä¼ å…¥headersï¼ˆåŸä»£ç æœªä¼ å…¥å¯¼è‡´User-Agentæ— æ•ˆï¼‰
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=timeout) as response:
                return response.read().decode('utf-8')
        except urllib.error.HTTPError as e:
            print(f"HTTPError [{e.code}]: {url} (no retry)")
            break  # HTTPé”™è¯¯æ— éœ€é‡è¯•
        except (urllib.error.URLError, socket.timeout) as e:
            print(f"Retry [{attempt+1}/{retries}]: {url} -> {e}")
        except Exception as e:
            print(f"Exception [{attempt+1}/{retries}]: {url} -> {type(e).__name__}")
        
        # æŒ‡æ•°é€€é¿é‡è¯•ï¼ˆé¿å…é¢‘ç¹è¯·æ±‚ï¼‰
        if attempt < retries - 1:
            time.sleep(backoff_factor * (2 ** attempt))
    return None

# ä½“è‚²èµ›äº‹æ—¥æœŸæ ‡å‡†åŒ–ï¼ˆç»Ÿä¸€ä¸ºMM-DDæ ¼å¼ï¼‰
def normalize_date_to_md(text):
    text = text.strip()
    # æ›¿æ¢å‡½æ•°ï¼šç¡®ä¿æ—¥æœŸååŠ ç©ºæ ¼åˆ†éš”
    def format_md(match):
        month = int(match.group(1))
        day = int(match.group(2))
        suffix = match.group(3) or ''
        if not suffix.startswith(' '):
            suffix = f' {suffix}'
        return f"{month}-{day}{suffix}"
    
    # å¤„ç†MM/DDã€YYYY-MM-DDã€ä¸­æ–‡æ—¥æœŸï¼ˆXæœˆXæ—¥ï¼‰
    text = re.sub(r'^0?(\d{1,2})/0?(\d{1,2})(.*)', format_md, text)
    text = re.sub(r'^\d{4}-0?(\d{1,2})-0?(\d{1,2})(.*)', format_md, text)
    text = re.sub(r'^0?(\d{1,2})æœˆ0?(\d{1,2})æ—¥(.*)', format_md, text)
    return text

normalized_tyss_lines = [normalize_date_to_md(line) for line in tyss_lines]

# å¤„ç†AKTVæºï¼ˆä¼˜å…ˆåœ¨çº¿è·å–ï¼Œå¤±è´¥åˆ™è¯»æœ¬åœ°ï¼‰
aktv_lines = []
aktv_url = "https://aktv.space/live.m3u"
aktv_text = get_http_response(aktv_url)
if aktv_text:
    print("AKTV: Loaded from online")
    aktv_text = convert_m3u_to_txt(aktv_text)
    aktv_lines = aktv_text.strip().split('\n')
else:
    print("AKTV: Loaded from local (online failed)")
    aktv_lines = read_txt_to_array('æ‰‹å·¥åŒº/AKTV.txt')

# ç”Ÿæˆä½“è‚²èµ›äº‹HTMLåˆ—è¡¨ï¼ˆå¸¦å¤åˆ¶åŠŸèƒ½ï¼‰
def generate_playlist_html(data_list, output_file='output/custom1/sports.html'):
    html_head = '''
<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6061710286208572" crossorigin="anonymous"></script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-BS1Z4F5BDN"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-BS1Z4F5BDN');
    </script>
    <title>æœ€æ–°ä½“è‚²èµ›äº‹</title>
    <style>
        body { font-family: sans-serif; padding: 20px; background: #f9f9f9; }
        .item { margin-bottom: 20px; padding: 12px; background: #fff; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.06); }
        .title { font-weight: bold; font-size: 1.1em; color: #333; margin-bottom: 5px; }
        .url-wrapper { display: flex; align-items: center; gap: 10px; }
        .url { max-width: 80%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; font-size: 0.9em; color: #555; background: #f0f0f0; padding: 6px; border-radius: 4px; flex-grow: 1; }
        .copy-btn { background-color: #007BFF; border: none; color: white; padding: 6px 10px; border-radius: 4px; cursor: pointer; font-size: 0.8em; }
        .copy-btn:hover { background-color: #0056b3; }
    </style>
</head>
<body>
    <h2>ğŸ“‹ æœ€æ–°ä½“è‚²èµ›äº‹åˆ—è¡¨</h2>
    '''
    
    html_body = ''
    for idx, entry in enumerate(data_list):
        if ',' not in entry:
            continue
        info, url = entry.split(',', 1)
        url_id = f"url_{idx}"
        html_body += f'''
    <div class="item">
        <div class="title">ğŸ•’ {info}</div>
        <div class="url-wrapper">
            <div class="url" id="{url_id}">{url}</div>
            <button class="copy-btn" onclick="copyToClipboard('{url_id}')">å¤åˆ¶</button>
        </div>
    </div>
        '''
    
    html_tail = '''
    <script>
        function copyToClipboard(id) {
            const text = document.getElementById(id).textContent;
            navigator.clipboard.writeText(text).then(() => alert("å·²å¤åˆ¶é“¾æ¥ï¼")).catch(err => alert("å¤åˆ¶å¤±è´¥: " + err));
        }
    </script>
</body>
</html>
    '''
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(html_head + html_body + html_tail)
    print(f"âœ… Sports HTML generated: {output_file}")

# ç”Ÿæˆä½“è‚²èµ›äº‹HTMLï¼ˆå»é‡åæ’åºï¼‰
generate_playlist_html(sorted(set(normalized_tyss_lines)))

# éšæœºè·å–URLï¼ˆç”¨äºâ€œä»Šæ—¥æ¨å°â€ï¼‰
def get_random_url(file_path):
    urls = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if ',' in line:
                urls.append(line.strip().split(',')[-1])
    return random.choice(urls) if urls else None

# ç”Ÿæˆç‰ˆæœ¬ä¿¡æ¯ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰
beijing_time = datetime.now(timezone.utc) + timedelta(hours=8)
formatted_time = beijing_time.strftime("%Y%m%d %H:%M:%S")

# ç‰ˆæœ¬ä¸æ¨èå°é…ç½®
version = f"{formatted_time},{get_random_url('assets/ä»Šæ—¥æ¨å°.txt')}"
about = f"xiaoranmuze,{get_random_url('assets/ä»Šæ—¥æ¨å°.txt')}"
daily_mtv = f"ä»Šæ—¥æ¨è,{get_random_url('assets/ä»Šæ—¥æ¨è.txt')}"
daily_mtv1 = f"ğŸ”¥ä½è°ƒ,{get_random_url('assets/ä»Šæ—¥æ¨è.txt')}"
daily_mtv2 = f"ğŸ”¥ä½¿ç”¨,{get_random_url('assets/ä»Šæ—¥æ¨è.txt')}"
daily_mtv3 = f"ğŸ”¥ç¦æ­¢,{get_random_url('assets/ä»Šæ—¥æ¨è.txt')}"
daily_mtv4 = f"ğŸ”¥è´©å–,{get_random_url('assets/ä»Šæ—¥æ¨è.txt')}"

# è¡¥å……æ‰‹å·¥åŒºé¢‘é“ï¼ˆæœ¬åœ°TXTï¼‰
print("Processing manual channels...")
zj_lines += read_txt_to_array('æ‰‹å·¥åŒº/æµ™æ±Ÿé¢‘é“.txt')
hb_lines += read_txt_to_array('æ‰‹å·¥åŒº/æ¹–åŒ—é¢‘é“.txt')
gd_lines += read_txt_to_array('æ‰‹å·¥åŒº/å¹¿ä¸œé¢‘é“.txt')
sh_lines += read_txt_to_array('æ‰‹å·¥åŒº/ä¸Šæµ·é¢‘é“.txt')
jsu_lines += read_txt_to_array('æ‰‹å·¥åŒº/æ±Ÿè‹é¢‘é“.txt')

# ------------------------------
# ç”Ÿæˆ3ç±»è¾“å‡ºæ–‡ä»¶ï¼šå…¨é›†ç‰ˆã€ç˜¦èº«ç‰ˆã€å®šåˆ¶ç‰ˆ
# ------------------------------
# 1. å…¨é›†ç‰ˆï¼ˆå«æ‰€æœ‰åˆ†ç±»ï¼‰
all_lines = ["ğŸŒå¤®è§†é¢‘é“,#genre#"] + sort_data(ys_dictionary, correct_name_data(corrections_name, ys_lines)) + ['\n'] + \
    ["ğŸ“¡å«è§†é¢‘é“,#genre#"] + sort_data(ws_dictionary, correct_name_data(corrections_name, ws_lines)) + ['\n'] + \
    ["â˜˜ï¸æ¹–åŒ—é¢‘é“,#genre#"] + sort_data(hb_dictionary, set(correct_name_data(corrections_name, hb_lines))) + ['\n'] + \
    ["â˜˜ï¸æ¹–å—é¢‘é“,#genre#"] + sort_data(hn_dictionary, set(correct_name_data(corrections_name, hn_lines))) + ['\n'] + \
    ["â˜˜ï¸æµ™æ±Ÿé¢‘é“,#genre#"] + sort_data(zj_dictionary, set(correct_name_data(corrections_name, zj_lines))) + ['\n'] + \
    ["â˜˜ï¸å¹¿ä¸œé¢‘é“,#genre#"] + sort_data(gd_dictionary, set(correct_name_data(corrections_name, gd_lines))) + ['\n'] + \
    ["â˜˜ï¸æ±Ÿè‹é¢‘é“,#genre#"] + sort_data(jsu_dictionary, set(correct_name_data(corrections_name, jsu_lines))) + ['\n'] + \
    ["â˜˜ï¸æ±Ÿè¥¿é¢‘é“,#genre#"] + sort_data(jx_dictionary, set(correct_name_data(corrections_name, jx_lines))) + ['\n'] + \
    ["â˜˜ï¸åŒ—äº¬é¢‘é“,#genre#"] + sort_data(bj_dictionary, set(correct_name_data(corrections_name, bj_lines))) + ['\n'] + \
    ["â˜˜ï¸ä¸Šæµ·é¢‘é“,#genre#"] + sort_data(sh_dictionary, set(correct_name_data(corrections_name, sh_lines))) + ['\n'] + \
    ["â˜˜ï¸å¤©æ´¥é¢‘é“,#genre#"] + sort_data(tj_dictionary, set(correct_name_data(corrections_name, tj_lines))) + ['\n'] + \
    ["â˜˜ï¸é‡åº†é¢‘é“,#genre#"] + sort_data(cq_dictionary, set(correct_name_data(corrections_name, cq_lines))) + ['\n'] + \
    ["â˜˜ï¸å®‰å¾½é¢‘é“,#genre#"] + sort_data(ah_dictionary, set(correct_name_data(corrections_name, ah_lines))) + ['\n'] + \
    ["â˜˜ï¸æµ·å—é¢‘é“,#genre#"] + sort_data(hain_dictionary, set(correct_name_data(corrections_name, hain_lines))) + ['\n'] + \
    ["â˜˜ï¸å†…è’™é¢‘é“,#genre#"] + sort_data(nm_dictionary, set(correct_name_data(corrections_name, nm_lines))) + ['\n'] + \
    ["â˜˜ï¸è¾½å®é¢‘é“,#genre#"] + sort_data(ln_dictionary, set(correct_name_data(corrections_name, ln_lines))) + ['\n'] + \
    ["â˜˜ï¸é™•è¥¿é¢‘é“,#genre#"] + sort_data(sx_dictionary, set(correct_name_data(corrections_name, sx_lines))) + ['\n'] + \
    ["â˜˜ï¸å±±ä¸œé¢‘é“,#genre#"] + sort_data(shandong_dictionary, set(correct_name_data(corrections_name, shandong_lines))) + ['\n'] + \
    ["â˜˜ï¸å±±è¥¿é¢‘é“,#genre#"] + sort_data(shanxi_dictionary, set(correct_name_data(corrections_name, shanxi_lines))) + ['\n'] + \
    ["â˜˜ï¸äº‘å—é¢‘é“,#genre#"] + sort_data(yunnan_dictionary, set(correct_name_data(corrections_name, yunnan_lines))) + ['\n'] + \
    ["â˜˜ï¸ç¦å»ºé¢‘é“,#genre#"] + sort_data(fj_dictionary, set(correct_name_data(corrections_name, fj_lines))) + ['\n'] + \
    ["â˜˜ï¸ç”˜è‚ƒé¢‘é“,#genre#"] + sort_data(gs_dictionary, set(correct_name_data(corrections_name, gs_lines))) + ['\n'] + \
    ["â˜˜ï¸å¹¿è¥¿é¢‘é“,#genre#"] + sort_data(gx_dictionary, set(correct_name_data(corrections_name, gx_lines))) + ['\n'] + \
    ["â˜˜ï¸è´µå·é¢‘é“,#genre#"] + sort_data(gz_dictionary, set(correct_name_data(corrections_name, gz_lines))) + ['\n'] + \
    ["â˜˜ï¸æ²³åŒ—é¢‘é“,#genre#"] + sort_data(heb_dictionary, set(correct_name_data(corrections_name, heb_lines))) + ['\n'] + \
    ["â˜˜ï¸æ²³å—é¢‘é“,#genre#"] + sort_data(hen_dictionary, set(correct_name_data(corrections_name, hen_lines))) + ['\n'] + \
    ["â˜˜ï¸å‰æ—é¢‘é“,#genre#"] + sort_data(jl_dictionary, set(correct_name_data(corrections_name, jl_lines))) + ['\n'] + \
    ["â˜˜ï¸å®å¤é¢‘é“,#genre#"] + sort_data(nx_dictionary, set(correct_name_data(corrections_name, nx_lines))) + ['\n'] + \
    ["â˜˜ï¸é’æµ·é¢‘é“,#genre#"] + sort_data(qh_dictionary, set(correct_name_data(corrections_name, qh_lines))) + ['\n'] + \
    ["â˜˜ï¸å››å·é¢‘é“,#genre#"] + sort_data(sc_dictionary, set(correct_name_data(corrections_name, sc_lines))) + ['\n'] + \
    ["â˜˜ï¸æ–°ç–†é¢‘é“,#genre#"] + sort_data(xj_dictionary, set(correct_name_data(corrections_name, xj_lines))) + ['\n'] + \
    ["â˜˜ï¸é»‘é¾™æ±Ÿå°,#genre#"] + sorted(set(correct_name_data(corrections_name, hlj_lines))) + ['\n'] + \
    ["ğŸï¸æ•°å­—é¢‘é“,#genre#"] + sort_data(sz_dictionary, set(correct_name_data(corrections_name, sz_lines))) + ['\n'] + \
    ["ğŸŒå›½é™…é¢‘é“,#genre#"] + sort_data(gj_dictionary, set(correct_name_data(corrections_name, gj_lines))) + ['\n'] + \
    ["âš½ä½“è‚²é¢‘é“,#genre#"] + sort_data(ty_dictionary, set(correct_name_data(corrections_name, ty_lines))) + ['\n'] + \
    ["ğŸ†ä½“è‚²èµ›äº‹,#genre#"] + normalized_tyss_lines + ['\n'] + \
    ["ğŸ¬æ–—é±¼ç›´æ’­,#genre#"] + sort_data(douyu_dictionary, set(correct_name_data(corrections_name, douyu_lines))) + ['\n'] + \
    ["ğŸ¯è™ç‰™ç›´æ’­,#genre#"] + sort_data(huya_dictionary, set(correct_name_data(corrections_name, huya_lines))) + ['\n'] + \
    ["ğŸ™ï¸è§£è¯´é¢‘é“,#genre#"] + sort_data(js_dictionary, set(correct_name_data(corrections_name, js_lines))) + ['\n'] + \
    ["ğŸ¬ç”µå½±é¢‘é“,#genre#"] + sort_data(dy_dictionary, set(correct_name_data(corrections_name, dy_lines))) + ['\n'] + \
    ["ğŸ“ºç”µÂ·è§†Â·å‰§,#genre#"] + sort_data(dsj_dictionary, set(correct_name_data(corrections_name, dsj_lines))) + ['\n'] + \
    ["ğŸ“½ï¸è®°Â·å½•Â·ç‰‡,#genre#"] + sort_data(jlp_dictionary,set(correct_name_data(corrections_name,jlp_lines)))+ ['\n'] + \
    ["ğŸ•åŠ¨Â·ç”»Â·ç‰‡,#genre#"] + sort_data(dhp_dictionary, set(correct_name_data(corrections_name, dhp_lines))) + ['\n'] + \
    ["ğŸ“»æ”¶Â·éŸ³Â·æœº,#genre#"] + sort_data(radio_dictionary, set(correct_name_data(corrections_name, radio_lines))) + ['\n'] + \
    ["ğŸ‡¨ğŸ‡³æ¸¯Â·æ¾³Â·å°,#genre#"] +read_txt_to_array('æ‰‹å·¥åŒº/â™ªæ¸¯æ¾³å°.txt') + sort_data(gat_dictionary, set(correct_name_data(corrections_name, gat_lines))) + aktv_lines + ['\n'] + \
    ["ğŸ‡­ğŸ‡°é¦™æ¸¯é¢‘é“,#genre#"] + sort_data(xg_dictionary, set(correct_name_data(corrections_name, xg_lines))) + ['\n'] + \
    ["ğŸ‡²ğŸ‡´æ¾³é—¨é¢‘é“,#genre#"] + sort_data(aomen_dictionary, set(correct_name_data(corrections_name, aomen_lines))) + aktv_lines + ['\n'] + \
    ["ğŸ‡¹ğŸ‡¼å°æ¹¾é¢‘é“,#genre#"] + sort_data(tw_dictionary, set(correct_name_data(corrections_name, tw_lines)))  + ['\n'] + \
    ["ğŸ­æˆæ›²é¢‘é“,#genre#"] + sort_data(xq_dictionary,set(correct_name_data(corrections_name,xq_lines))) + ['\n'] + \
    ["ğŸµéŸ³ä¹é¢‘é“,#genre#"] + sort_data(yy_dictionary, set(correct_name_data(corrections_name, yy_lines))) + ['\n'] + \
    ["ğŸ¤ç»¼è‰ºé¢‘é“,#genre#"] + sorted(set(correct_name_data(corrections_name,zy_lines))) + ['\n'] + \
    ["ğŸ®æ¸¸æˆé¢‘é“,#genre#"] + sorted(set(correct_name_data(corrections_name,game_lines))) + ['\n'] + \
    ["âœ¨ä¼˜è´¨å¤®è§†,#genre#"] + read_txt_to_array('æ‰‹å·¥åŒº/â™ªä¼˜è´¨å¤®è§†.txt') + ['\n'] + \
    ["ğŸ›°ï¸ä¼˜è´¨å«è§†,#genre#"] + read_txt_to_array('æ‰‹å·¥åŒº/â™ªä¼˜è´¨å«è§†.txt') + ['\n'] + \
    ["ğŸ“¹ç›´æ’­ä¸­å›½,#genre#"] + sort_data(zb_dictionary, set(correct_name_data(corrections_name, zb_lines))) + ['\n'] + \
    ["ğŸ§¨å†å±Šæ˜¥æ™š,#genre#"] + sort_data(cw_dictionary, set(correct_name_data(corrections_name, cw_lines))) + ['\n'] + \
    ["ğŸ•’æ›´æ–°æ—¶é—´,#genre#"] + [version] + [about] + [daily_mtv] + [daily_mtv1] + [daily_mtv2] + [daily_mtv3] + [daily_mtv4] + read_txt_to_array('æ‰‹å·¥åŒº/about.txt') + ['\n']

# 2. ç˜¦èº«ç‰ˆï¼ˆä»…æ ¸å¿ƒé¢‘é“ï¼‰
all_lines_simple = ["å¤®è§†é¢‘é“,#genre#"] + sort_data(ys_dictionary, correct_name_data(corrections_name, ys_lines)) + ['\n'] + \
    ["å«è§†é¢‘é“,#genre#"] + sort_data(ws_dictionary, correct_name_data(corrections_name, ws_lines)) + ['\n'] + \
    ["åœ°æ–¹é¢‘é“,#genre#"] + \
    sort_data(hb_dictionary, set(correct_name_data(corrections_name, hb_lines))) + \
    sort_data(hn_dictionary, set(correct_name_data(corrections_name, hn_lines))) + \
    sort_data(zj_dictionary, set(correct_name_data(corrections_name, zj_lines))) + \
    sort_data(gd_dictionary, set(correct_name_data(corrections_name, gd_lines))) + \
    sort_data(shandong_dictionary, set(correct_name_data(corrections_name, shandong_lines))) + \
    sorted(set(correct_name_data(corrections_name, jsu_lines))) + \
    sorted(set(correct_name_data(corrections_name, ah_lines))) + \
    sorted(set(correct_name_data(corrections_name, hain_lines))) + \
    sorted(set(correct_name_data(corrections_name, nm_lines))) + \
    sorted(set(correct_name_data(corrections_name, ln_lines))) + \
    sorted(set(correct_name_data(corrections_name, sx_lines))) + \
    sorted(set(correct_name_data(corrections_name, shanxi_lines))) + \
    sorted(set(correct_name_data(corrections_name, yunnan_lines))) + \
    sorted(set(correct_name_data(corrections_name, bj_lines))) + \
    sorted(set(correct_name_data(corrections_name, cq_lines))) + \
    sorted(set(correct_name_data(corrections_name, fj_lines))) + \
    sorted(set(correct_name_data(corrections_name, gs_lines))) + \
    sorted(set(correct_name_data(corrections_name, gx_lines))) + \
    sorted(set(correct_name_data(corrections_name, gz_lines))) + \
    sorted(set(correct_name_data(corrections_name, heb_lines))) + \
    sorted(set(correct_name_data(corrections_name, hen_lines))) + \
    sorted(set(correct_name_data(corrections_name, jl_lines))) + \
    sorted(set(correct_name_data(corrections_name, jx_lines))) + \
    sorted(set(correct_name_data(corrections_name, nx_lines))) + \
    sorted(set(correct_name_data(corrections_name, qh_lines))) + \
    sorted(set(correct_name_data(corrections_name, sc_lines))) + \
    sorted(set(correct_name_data(corrections_name, tj_lines))) + \
    sorted(set(correct_name_data(corrections_name, xj_lines))) + \
    sorted(set(correct_name_data(corrections_name, hlj_lines))) + \
    ['\n'] + \
    ["æ•°å­—é¢‘é“,#genre#"] + sort_data(sz_dictionary, set(correct_name_data(corrections_name, sz_lines))) + ['\n'] + \
    ["æ›´æ–°æ—¶é—´,#genre#"] + [version] + ['\n']

# 3. å®šåˆ¶ç‰ˆï¼ˆå…¨é›†åŸºç¡€ä¸Šä¼˜åŒ–åˆ†ç±»å±•ç¤ºï¼‰
all_lines_custom = ["ğŸŒå¤®è§†é¢‘é“,#genre#"] + sort_data(ys_dictionary, correct_name_data(corrections_name, ys_lines)) + ['\n'] + \
    ["ğŸ“¡å«è§†é¢‘é“,#genre#"] + sort_data(ws_dictionary, correct_name_data(corrections_name, ws_lines)) + ['\n'] + \
    ["ğŸ åœ°æ–¹é¢‘é“,#genre#"] + \
    sort_data(hb_dictionary, set(correct_name_data(corrections_name, hb_lines))) + \
    sort_data(hn_dictionary, set(correct_name_data(corrections_name, hn_lines))) + \
    sort_data(zj_dictionary, set(correct_name_data(corrections_name, zj_lines))) + \
    sort_data(gd_dictionary, set(correct_name_data(corrections_name, gd_lines))) + \
    sort_data(shandong_dictionary, set(correct_name_data(corrections_name, shandong_lines))) + \
    sorted(set(correct_name_data(corrections_name, jsu_lines))) + \
    sorted(set(correct_name_data(corrections_name, ah_lines))) + \
    sorted(set(correct_name_data(corrections_name, hain_lines))) + \
    sorted(set(correct_name_data(corrections_name, nm_lines))) + \
    sorted(set(correct_name_data(corrections_name, ln_lines))) + \
    sorted(set(correct_name_data(corrections_name, sx_lines))) + \
    sorted(set(correct_name_data(corrections_name, shanxi_lines))) + \
    sorted(set(correct_name_data(corrections_name, yunnan_lines))) + \
    sorted(set(correct_name_data(corrections_name, bj_lines))) + \
    sorted(set(correct_name_data(corrections_name, cq_lines))) + \
    sorted(set(correct_name_data(corrections_name, fj_lines))) + \
    sorted(set(correct_name_data(corrections_name, gs_lines))) + \
    sorted(set(correct_name_data(corrections_name, gx_lines))) + \
    sorted(set(correct_name_data(corrections_name, gz_lines))) + \
    sorted(set(correct_name_data(corrections_name, heb_lines))) + \
    sorted(set(correct_name_data(corrections_name, hen_lines))) + \
    sorted(set(correct_name_data(corrections_name, jl_lines))) + \
    sorted(set(correct_name_data(corrections_name, jx_lines))) + \
    sorted(set(correct_name_data(corrections_name, nx_lines))) + \
    sorted(set(correct_name_data(corrections_name, qh_lines))) + \
    sorted(set(correct_name_data(corrections_name, sc_lines))) + \
    sorted(set(correct_name_data(corrections_name, tj_lines))) + \
    sorted(set(correct_name_data(corrections_name, xj_lines))) + \
    sorted(set(correct_name_data(corrections_name, hlj_lines))) + \
    ['\n'] + \
    ["ğŸï¸æ•°å­—é¢‘é“,#genre#"] + sort_data(sz_dictionary, set(correct_name_data(corrections_name, sz_lines))) + ['\n'] + \
    ["ğŸŒå›½é™…é¢‘é“,#genre#"] + sort_data(gj_dictionary, set(correct_name_data(corrections_name, gj_lines))) + ['\n'] + \
    ["âš½ä½“è‚²é¢‘é“,#genre#"] + sort_data(ty_dictionary, set(correct_name_data(corrections_name, ty_lines))) + ['\n'] + \
    ["ğŸ†ä½“è‚²èµ›äº‹,#genre#"] + normalized_tyss_lines + ['\n'] + \
    ["ğŸ¬æ–—é±¼ç›´æ’­,#genre#"] + sort_data(douyu_dictionary, set(correct_name_data(corrections_name, douyu_lines))) + ['\n'] + \
    ["ğŸ¯è™ç‰™ç›´æ’­,#genre#"] + sort_data(huya_dictionary, set(correct_name_data(corrections_name, huya_lines))) + ['\n'] + \
    ["ğŸ™ï¸è§£è¯´é¢‘é“,#genre#"] + sort_data(js_dictionary, set(correct_name_data(corrections_name, js_lines))) + ['\n'] + \
    ["ğŸ¬ç”µå½±é¢‘é“,#genre#"] + sort_data(dy_dictionary, set(correct_name_data(corrections_name, dy_lines))) + ['\n'] + \
    ["ğŸ“ºç”µÂ·è§†Â·å‰§,#genre#"] + sort_data(dsj_dictionary, set(correct_name_data(corrections_name, dsj_lines))) + ['\n'] + \
    ["ğŸ“½ï¸è®°Â·å½•Â·ç‰‡,#genre#"] + sort_data(jlp_dictionary,set(correct_name_data(corrections_name,jlp_lines)))+ ['\n'] + \
    ["ğŸ•åŠ¨Â·ç”»Â·ç‰‡,#genre#"] + sort_data(dhp_dictionary, set(correct_name_data(corrections_name, dhp_lines))) + ['\n'] + \
    ["ğŸ“»æ”¶Â·éŸ³Â·æœº,#genre#"] + sort_data(radio_dictionary, set(correct_name_data(corrections_name, radio_lines))) + ['\n'] + \
    ["ğŸ‡¨ğŸ‡³æ¸¯Â·æ¾³Â·å°,#genre#"] +read_txt_to_array('æ‰‹å·¥åŒº/â™ªæ¸¯æ¾³å°.txt') + sort_data(gat_dictionary, set(correct_name_data(corrections_name, gat_lines))) + aktv_lines + ['\n'] + \
    ["ğŸ‡­ğŸ‡°é¦™æ¸¯é¢‘é“,#genre#"] + sort_data(xg_dictionary, set(correct_name_data(corrections_name, xg_lines))) + ['\n'] + \
    ["ğŸ‡²ğŸ‡´æ¾³é—¨é¢‘é“,#genre#"] + sort_data(aomen_dictionary, set(correct_name_data(corrections_name, aomen_lines))) + aktv_lines + ['\n'] + \
    ["ğŸ‡¹ğŸ‡¼å°æ¹¾é¢‘é“,#genre#"] + sort_data(tw_dictionary, set(correct_name_data(corrections_name, tw_lines)))  + ['\n'] + \
    ["ğŸ­æˆæ›²é¢‘é“,#genre#"] + sort_data(xq_dictionary,set(correct_name_data(corrections_name,xq_lines))) + ['\n'] + \
    ["ğŸµéŸ³ä¹é¢‘é“,#genre#"] + sort_data(yy_dictionary, set(correct_name_data(corrections_name, yy_lines))) + ['\n'] + \
    ["ğŸ¤ç»¼è‰ºé¢‘é“,#genre#"] + sorted(set(correct_name_data(corrections_name,zy_lines))) + ['\n'] + \
    ["ğŸ®æ¸¸æˆé¢‘é“,#genre#"] + sorted(set(correct_name_data(corrections_name,game_lines))) + ['\n'] + \
    ["âœ¨ä¼˜è´¨å¤®è§†,#genre#"] + read_txt_to_array('æ‰‹å·¥åŒº/â™ªä¼˜è´¨å¤®è§†.txt') + ['\n'] + \
    ["ğŸ›°ï¸ä¼˜è´¨å«è§†,#genre#"] + read_txt_to_array('æ‰‹å·¥åŒº/â™ªä¼˜è´¨å«è§†.txt') + ['\n'] + \
    ["ğŸ“¹ç›´æ’­ä¸­å›½,#genre#"] + sort_data(zb_dictionary, set(correct_name_data(corrections_name, zb_lines))) + ['\n'] + \
    ["ğŸ§¨å†å±Šæ˜¥æ™š,#genre#"] + sort_data(cw_dictionary, set(correct_name_data(corrections_name, cw_lines))) + ['\n'] + \
    ["ğŸ•’æ›´æ–°æ—¶é—´,#genre#"] + [version] + [about] + [daily_mtv] + [daily_mtv1] + [daily_mtv2] + [daily_mtv3] + [daily_mtv4] + read_txt_to_array('æ‰‹å·¥åŒº/about.txt') + ['\n']

# ------------------------------
# å†™å…¥TXTæ–‡ä»¶
# ------------------------------
output_paths = {
    'full': 'output/custom1/full.txt',
    'simple': 'output/custom1/simple.txt',
    'custom': 'output/custom1/custom.txt',
    'others': 'output/custom1/others.txt'
}

try:
    # å†™å…¥å…¨é›†ç‰ˆ
    with open(output_paths['full'], 'w', encoding='utf-8') as f:
        f.write('\n'.join(all_lines))
    print(f"âœ… Full TXT saved: {output_paths['full']}")

    # å†™å…¥ç˜¦èº«ç‰ˆ
    with open(output_paths['simple'], 'w', encoding='utf-8') as f:
        f.write('\n'.join(all_lines_simple))
    print(f"âœ… Simple TXT saved: {output_paths['simple']}")

    # å†™å…¥å®šåˆ¶ç‰ˆ
    with open(output_paths['custom'], 'w', encoding='utf-8') as f:
        f.write('\n'.join(all_lines_custom))
    print(f"âœ… Custom TXT saved: {output_paths['custom']}")

    # å†™å…¥å…¶ä»–é¢‘é“è®°å½•
    with open(output_paths['others'], 'w', encoding='utf-8') as f:
        f.write('\n'.join(other_lines))
    print(f"âœ… Others TXT saved: {output_paths['others']}")

except Exception as e:
    print(f"Save TXT error: {e}")

# ------------------------------
# ç”ŸæˆM3Uæ–‡ä»¶ï¼ˆå¸¦EPGå’Œé¢‘é“Logoï¼‰
# ------------------------------
def make_m3u(txt_file, m3u_file):
    try:
        # M3Uå¤´éƒ¨ï¼ˆæŒ‡å®šEPGæºï¼‰
        m3u_header = '#EXTM3U x-tvg-url="https://live.fanmingming.cn/e.xml"\n'
        m3u_content = m3u_header
        
        # è¯»å–TXTå†…å®¹å¹¶è½¬æ¢ä¸ºM3Uæ ¼å¼
        with open(txt_file, 'r', encoding='utf-8') as f:
            lines = f.read().strip().split('\n')
        
        group_name = ""  # å½“å‰åˆ†ç±»ç»„å
        channel_logos = {line.split(',')[0]: line.split(',')[1] for line in read_txt_to_array('assets/logo.txt') if ',' in line}
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            # æ›´æ–°åˆ†ç±»ç»„åï¼ˆè¯†åˆ«â€œ#genre#â€è¡Œï¼‰
            if "#genre#" in line and "," in line:
                group_name = line.split(',')[0]
            # è½¬æ¢é¢‘é“è¡Œä¸ºM3Uæ ¼å¼
            elif "," in line and "://" in line:
                channel_name, channel_url = line.split(',', 1)
                # è·å–é¢‘é“Logoï¼ˆæ— åˆ™Logoï¼ˆæ— åˆ™çœç•¥ï¼‰
                logo_url = channel_logos.get(channel_name)
                if logo_url:
                    m3u_content += f"#EXTINF:-1 tvg-name=\"{channel_name}\" tvg-logo=\"{logo_url}\" group-title=\"{group_name}\",{channel_name}\n"
                else:
                    m3u_content += f"#EXTINF:-1 group-title=\"{group_name}\",{channel_name}\n"
                m3u_content += f"{channel_url}\n"
        
        # å†™å…¥M3Uæ–‡ä»¶
        with open(m3u_file, 'w', encoding='utf-8') as f:
            f.write(m3u_content)
        print(f"âœ… M3U generated: {m3u_file}")
    
    except Exception as e:
        print(f"Make M3U error: {e}")

# ä¸º3ç±»TXTç”Ÿæˆå¯¹åº”M3U
make_m3u(output_paths['full'], output_paths['full'].replace('.txt', '.m3u'))
make_m3u(output_paths['simple'], output_paths['simple'].replace('.txt', '.m3u'))
make_m3u(output_paths['custom'], output_paths['custom'].replace('.txt', '.m3u'))

# ------------------------------
# æ‰§è¡Œä¿¡æ¯ç»Ÿè®¡
# ------------------------------
timeend = datetime.now()
elapsed_time = timeend - timestart
total_seconds = elapsed_time.total_seconds()
minutes = int(total_seconds // 60)
seconds = int(total_seconds % 60)

# è¾“å‡ºæ‰§è¡Œä¿¡æ¯
print(f"\n=== æ‰§è¡Œç»Ÿè®¡ ===")
print(f"å¼€å§‹æ—¶é—´: {timestart.strftime('%Y%m%d_%H_%M_%S')}")
print(f"ç»“æŸæ—¶é—´: {timeend.strftime('%Y%m%d_%H_%M_%S')}")
print(f"æ‰§è¡Œæ—¶é—´: {minutes} åˆ† {seconds} ç§’")
print(f"é»‘åå•æ•°é‡: {len(combined_blacklist)}")
print(f"å…¨é›†ç‰ˆè¡Œæ•°: {len(all_lines)}")
print(f"å®šåˆ¶ç‰ˆè¡Œæ•°: {len(all_lines_custom)}")
print(f"å…¶ä»–è®°å½•è¡Œæ•°: {len(other_lines)}")
