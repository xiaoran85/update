import urllib.request
from urllib.parse import urlparse
import re
import os
from datetime import datetime, timedelta, timezone
import random
import opencc
import socket
import time
import hashlib

# ======= ç¡¬ç¼–ç é…ç½®åŒºåŸŸ ========
# è¾“å…¥è·¯å¾„é…ç½®
SOURCE_BASE = "scripts/livesource"
BLACKLIST_DIR = "scripts/livesource/blacklist"
MAIN_CHANNELS_DIR = "scripts/livesource/ä¸»é¢‘é“"
LOCAL_CHANNELS_DIR = "scripts/livesource/åœ°æ–¹å°"
MANUAL_DIR = "scripts/livesource/æ‰‹å·¥åŒº"
ASSETS_DIR = "scripts/livesource"

# è¾“å‡ºè·¯å¾„é…ç½®
OUTPUT_BASE = "output/livesource"
OUTPUT_DIR = "output/livesource"

# è¾“å‡ºæ–‡ä»¶è·¯å¾„
FULL_OUTPUT = "output/livesource/full.txt"
LITE_OUTPUT = "output/livesource/lite.txt"
CUSTOM_OUTPUT = "output/livesource/custom.txt"
OTHERS_OUTPUT = "output/livesource/others.txt"

# æ‰‹å·¥åŒºæ–‡ä»¶è·¯å¾„
MANUAL_GAT = "scripts/livesource/æ‰‹å·¥åŒº/æ¸¯æ¾³å°.txt"
MANUAL_CCTV = "scripts/livesource/æ‰‹å·¥åŒº/ä¼˜è´¨å¤®è§†.txt"
MANUAL_WS = "scripts/livesource/æ‰‹å·¥åŒº/ä¼˜è´¨å«è§†.txt"
MANUAL_ABOUT = "scripts/livesource/æ‰‹å·¥åŒº/about.txt"
MANUAL_AKTV = "scripts/livesource/æ‰‹å·¥åŒº/AKTV.txt"
MANUAL_RECOMMEND = "scripts/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt"
MANUAL_CHANNEL = "scripts/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨å°.txt"

# å…¶ä»–é…ç½®
REQUEST_TIMEOUT = 10
REQUEST_RETRIES = 3
REQUEST_BACKOFF_FACTOR = 1.5

REMOVAL_LIST = [
    "_ç”µä¿¡", "ç”µä¿¡", "é«˜æ¸…", "é¢‘é“", "-HD", "-BD", "è‹±é™†", "_ITV", "(åŒ—ç¾)", "(HK)", 
    "AKtv", "ã€ŒIPV4ã€", "ã€ŒIPV6ã€", "é¢‘é™†", "å¤‡é™†", "å£¹é™†", "è´°é™†", "åé™†", "è‚†é™†", 
    "ä¼é™†", "é™†é™†", "æŸ’é™†", "é¢‘æ™´", "é¢‘ç²¤", "[è¶…æ¸…]", "é«˜æ¸…", "è¶…æ¸…", "æ ‡æ¸…", "æ–¯ç‰¹",
    "ç²¤é™†", "å›½é™†", "è‚†æŸ’", "é¢‘è‹±", "é¢‘ç‰¹", "é¢‘å›½", "é¢‘å£¹", "é¢‘è´°", "è‚†è´°", "é¢‘æµ‹",
    "å’ªå’•", "é—½ç‰¹", "é«˜ç‰¹", "é¢‘é«˜", "é¢‘æ ‡", "æ±é˜³", "[HD]", "[BD]", "[SD]", "[VGA]"
]

CRITICAL_FILES = ['full.txt', 'custom.txt']
URL_PATTERNS_TO_SKIP = ['tvbus://', '/udp/', 'rtsp://', 'rtp://']

# ====== åˆå§‹åŒ–è®¾ç½® ======
os.makedirs(OUTPUT_DIR, exist_ok=True)
# ä½¿ç”¨åŒ—äº¬æ—¶é—´
beijing_tz = timezone(timedelta(hours=8))

# ====== æ ¸å¿ƒå·¥å…·å‡½æ•° ======
def read_txt_to_array(file_name):
    """è¯»å–æ–‡æœ¬æ–‡ä»¶åˆ°æ•°ç»„"""
    try:
        with open(file_name, 'r', encoding='utf-8') as file:
            return [line.strip() for line in file if line.strip()]
    except FileNotFoundError:
        print(f"âš ï¸ æ–‡ä»¶æœªæ‰¾åˆ°: {file_name}")
        return []
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶é”™è¯¯ {file_name}: {e}")
        return []

def traditional_to_simplified(text: str) -> str:
    """ç¹ä½“è½¬ç®€ä½“"""
    try:
        converter = opencc.OpenCC('t2s')
        return converter.convert(text)
    except Exception as e:
        print(f"âŒ ç¹ç®€è½¬æ¢é”™è¯¯: {e}")
        return text

def read_blacklist_from_txt(file_path):
    """è¯»å–é»‘åå•"""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            return [line.split(',')[1].strip() for line in file if ',' in line]
    except Exception as e:
        print(f"âŒ è¯»å–é»‘åå•é”™è¯¯ {file_path}: {e}")
        return []

def get_url_hash(url):
    """è·å–URLçš„å“ˆå¸Œå€¼ç”¨äºå»é‡"""
    return hashlib.md5(url.encode('utf-8')).hexdigest()

def should_skip_url(url):
    """æ£€æŸ¥URLæ˜¯å¦åº”è¯¥è·³è¿‡"""
    return any(pattern in url for pattern in URL_PATTERNS_TO_SKIP)

def is_valid_url(url):
    """éªŒè¯URLæ ¼å¼"""
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

# ====== é¢‘é“åç§°å¤„ç†å‡½æ•° ======
def clean_channel_name(channel_name, removal_list):
    """æ¸…ç†é¢‘é“åç§°ä¸­çš„ç‰¹å®šå­—ç¬¦"""
    for item in removal_list:
        channel_name = channel_name.replace(item, "")
    
    if channel_name.endswith("HD"):
        channel_name = channel_name[:-2]
    if channel_name.endswith("å°") and len(channel_name) > 3:
        channel_name = channel_name[:-1]
    
    return channel_name.strip()

def process_name_string(input_str):
    """å¤„ç†é¢‘é“åç§°å­—ç¬¦ä¸²"""
    try:
        parts = input_str.split(',')
        processed_parts = []
        
        for part in parts:
            if "CCTV" in part and "://" not in part:
                part = part.replace("IPV6", "").replace("PLUS", "+").replace("1080", "")
                filtered_str = ''.join(char for char in part if char.isdigit() or char in 'K+')
                
                if not filtered_str.strip():
                    filtered_str = part.replace("CCTV", "")
                
                if len(filtered_str) > 2 and re.search(r'4K|8K', filtered_str):
                    filtered_str = re.sub(r'(4K|8K).*', r'\1', filtered_str)
                    if len(filtered_str) > 2: 
                        filtered_str = re.sub(r'(4K|8K)', r'(\1)', filtered_str)
                
                processed_parts.append("CCTV" + filtered_str)
            elif "å«è§†" in part:
                processed_parts.append(re.sub(r'å«è§†ã€Œ.*ã€', 'å«è§†', part))
            else:
                processed_parts.append(part)
        
        return ','.join(processed_parts)
    except Exception as e:
        print(f"âŒ å¤„ç†é¢‘é“åç§°é”™è¯¯: {e}, è¾“å…¥: {input_str}")
        return input_str

# ====== URLå¤„ç†å‡½æ•° ======
def get_url_file_extension(url):
    """è·å–URLæ–‡ä»¶æ‰©å±•å"""
    try:
        parsed_url = urlparse(url)
        return os.path.splitext(parsed_url.path)[1]
    except Exception as e:
        print(f"âŒ è§£æURLæ‰©å±•åé”™è¯¯: {e}")
        return ""

def clean_url(url):
    """æ¸…ç†URLä¸­çš„$ç¬¦å·åŠä¹‹åå†…å®¹"""
    try:
        last_dollar_index = url.rfind('$')
        return url[:last_dollar_index] if last_dollar_index != -1 else url
    except Exception as e:
        print(f"âŒ æ¸…ç†URLé”™è¯¯: {e}")
        return url

def check_url_existence(data_list, url):
    """æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨"""
    try:
        urls = [item.split(',')[1] for item in data_list if ',' in item]
        return url not in urls
    except Exception as e:
        print(f"âŒ æ£€æŸ¥URLå­˜åœ¨æ€§é”™è¯¯: {e}")
        return True

def convert_m3u_to_txt(m3u_content):
    """M3Uæ ¼å¼è½¬TXTæ ¼å¼"""
    try:
        lines = m3u_content.split('\n')
        txt_lines = []
        channel_name = ""
        
        for line in lines:
            if line.startswith("#EXTM3U"):
                continue
            elif line.startswith("#EXTINF"):
                channel_name = line.split(',')[-1].strip()
            elif line.startswith(("http", "rtmp", "p3p")):
                if channel_name:
                    txt_lines.append(f"{channel_name},{line.strip()}")
            
            if "#genre#" not in line and "," in line and "://" in line:
                pattern = r'^[^,]+,[^\s]+://[^\s]+$'
                if bool(re.match(pattern, line)):
                    txt_lines.append(line)
        
        return '\n'.join(txt_lines)
    except Exception as e:
        print(f"âŒ è½¬æ¢M3Uåˆ°TXTé”™è¯¯: {e}")
        return m3u_content

# ====== ç½‘ç»œè¯·æ±‚å‡½æ•° ======
def get_http_response(url, timeout=None, retries=None, backoff_factor=None):
    """å¸¦é‡è¯•çš„HTTPè¯·æ±‚"""
    timeout = timeout or REQUEST_TIMEOUT
    retries = retries or REQUEST_RETRIES
    backoff_factor = backoff_factor or REQUEST_BACKOFF_FACTOR
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    for attempt in range(retries):
        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=timeout) as response:
                data = response.read()
                return data.decode('utf-8')
        except urllib.error.HTTPError as e:
            print(f"âŒ [HTTPé”™è¯¯] ä»£ç : {e.code}, URL: {url}")
            break
        except urllib.error.URLError as e:
            print(f"âš ï¸ [URLé”™è¯¯] åŸå› : {e.reason}, å°è¯•: {attempt + 1}/{retries}")
        except socket.timeout:
            print(f"â° [è¶…æ—¶] URL: {url}, å°è¯•: {attempt + 1}/{retries}")
        except Exception as e:
            print(f"âš ï¸ [å¼‚å¸¸] {type(e).__name__}: {e}, å°è¯•: {attempt + 1}/{retries}")
        
        if attempt < retries - 1:
            sleep_time = backoff_factor * (2 ** attempt)
            print(f"â³ ç­‰å¾… {sleep_time} ç§’åé‡è¯•...")
            time.sleep(sleep_time)
    
    return None

# ====== æ•°æ®æ’åºå’Œçº é”™ ======
def load_corrections_name(filename):
    """åŠ è½½é¢‘é“åç§°çº é”™æ•°æ®"""
    corrections = {}
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                if not line.strip():
                    continue
                parts = line.strip().split(',')
                if len(parts) >= 2:
                    correct_name = parts[0]
                    for name in parts[1:]:
                        corrections[name] = correct_name
        print(f"âœ… çº é”™æ•°æ®åŠ è½½: {len(corrections)} æ¡è§„åˆ™")
    except Exception as e:
        print(f"âŒ åŠ è½½çº é”™æ•°æ®é”™è¯¯ {filename}: {e}")
    return corrections

def correct_name_data(corrections, data):
    """çº æ­£é¢‘é“åç§°"""
    corrected_data = []
    for line in data:
        try:
            if ',' not in line:
                continue
            name, url = line.split(',', 1)
            if name in corrections and name != corrections[name]:
                name = corrections[name]
            corrected_data.append(f"{name},{url}")
        except Exception as e:
            print(f"âš ï¸ çº æ­£åç§°é”™è¯¯: {e}, è¡Œ: {line}")
    return corrected_data

def sort_data(order, data):
    """æŒ‰æŒ‡å®šé¡ºåºæ’åºæ•°æ®"""
    try:
        order_dict = {name: i for i, name in enumerate(order)}
        def sort_key(line):
            try:
                name = line.split(',')[0]
                return order_dict.get(name, len(order))
            except:
                return len(order)
        return sorted(data, key=sort_key)
    except Exception as e:
        print(f"âš ï¸ æ’åºæ•°æ®é”™è¯¯: {e}")
        return data

# ====== æ•°æ®åŠ è½½å‡½æ•° ======
def load_all_dictionaries():
    """åŠ è½½æ‰€æœ‰å­—å…¸æ•°æ®"""
    print("ğŸ“š åŠ è½½å­—å…¸æ•°æ®...")
    
    dictionaries = {}
    
    # ä¸»é¢‘é“å­—å…¸
    dictionaries['yangshi_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/CCTV.txt")
    dictionaries['weishi_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/å«è§†é¢‘é“.txt")

    # åœ°æ–¹å°å­—å…¸
    dictionaries['beijing_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/åŒ—äº¬é¢‘é“.txt")
    dictionaries['shanghai_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/ä¸Šæµ·é¢‘é“.txt")
    dictionaries['tianjin_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/å¤©æ´¥é¢‘é“.txt")
    dictionaries['chongqing_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/é‡åº†é¢‘é“.txt")
    dictionaries['guangdong_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/å¹¿ä¸œé¢‘é“.txt")
    dictionaries['jiangsu_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/æ±Ÿè‹é¢‘é“.txt")
    dictionaries['zhejiang_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/æµ™æ±Ÿé¢‘é“.txt")
    dictionaries['shandong_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/å±±ä¸œé¢‘é“.txt")
    dictionaries['henan_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/æ²³å—é¢‘é“.txt")
    dictionaries['sichuan_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/å››å·é¢‘é“.txt")
    dictionaries['hebei_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/æ²³åŒ—é¢‘é“.txt")
    dictionaries['hunan_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/æ¹–å—é¢‘é“.txt")
    dictionaries['hubei_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/æ¹–åŒ—é¢‘é“.txt")
    dictionaries['anhui_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/å®‰å¾½é¢‘é“.txt")
    dictionaries['fujian_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/ç¦å»ºé¢‘é“.txt")
    dictionaries['shanxi1_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/é™•è¥¿é¢‘é“.txt")
    dictionaries['liaoning_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/è¾½å®é¢‘é“.txt")
    dictionaries['jiangxi_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/æ±Ÿè¥¿é¢‘é“.txt")
    dictionaries['heilongjiang_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/é»‘é¾™æ±Ÿé¢‘é“.txt")
    dictionaries['jilin_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/å‰æ—é¢‘é“.txt")
    dictionaries['shanxi2_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/å±±è¥¿é¢‘é“.txt")
    dictionaries['guangxi_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/å¹¿è¥¿é¢‘é“.txt")
    dictionaries['yunnan_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/äº‘å—é¢‘é“.txt")
    dictionaries['guizhou_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/è´µå·é¢‘é“.txt")
    dictionaries['gansu_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/ç”˜è‚ƒé¢‘é“.txt")
    dictionaries['neimenggu_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/å†…è’™é¢‘é“.txt")
    dictionaries['xinjiang_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/æ–°ç–†é¢‘é“.txt")
    dictionaries['hainan_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/æµ·å—é¢‘é“.txt")
    dictionaries['ningxia_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/å®å¤é¢‘é“.txt")
    dictionaries['qinghai_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/é’æµ·é¢‘é“.txt")
    dictionaries['xizang_dictionary'] = read_txt_to_array(f"{LOCAL_CHANNELS_DIR}/è¥¿è—é¢‘é“.txt")

    # å®šåˆ¶é¢‘é“å­—å…¸
    dictionaries['news_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/æ–°é—»é¢‘é“.txt")
    dictionaries['shuzi_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/æ•°å­—é¢‘é“.txt")
    dictionaries['dianying_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/ç”µå½±é¢‘é“.txt")
    dictionaries['jieshuo_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/è§£è¯´é¢‘é“.txt")
    dictionaries['zongyi_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/ç»¼è‰ºé¢‘é“.txt")
    dictionaries['huya_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/è™ç‰™ç›´æ’­.txt")
    dictionaries['douyu_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/æ–—é±¼ç›´æ’­.txt")
    dictionaries['xianggang_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/é¦™æ¸¯é¢‘é“.txt")
    dictionaries['aomen_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/æ¾³é—¨é¢‘é“.txt")
    dictionaries['china_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/ä¸­å›½é¢‘é“.txt")
    dictionaries['guoji_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/å›½é™…é¢‘é“.txt")
    dictionaries['gangaotai_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/æ¸¯æ¾³å°.txt")
    dictionaries['dianshiju_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/ç”µè§†å‰§.txt")
    dictionaries['radio_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/æ”¶éŸ³æœº.txt")
    dictionaries['donghuapian_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/åŠ¨ç”»ç‰‡.txt")
    dictionaries['jilupian_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/è®°å½•ç‰‡.txt")
    dictionaries['tiyu_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/ä½“è‚²é¢‘é“.txt")
    dictionaries['tiyusaishi_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/ä½“è‚²èµ›äº‹.txt")
    dictionaries['youxi_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/æ¸¸æˆé¢‘é“.txt")
    dictionaries['xiqu_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/æˆæ›²é¢‘é“.txt")
    dictionaries['yinyue_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/éŸ³ä¹é¢‘é“.txt")
    dictionaries['chunwan_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/æ˜¥æ™šé¢‘é“.txt")
    dictionaries['zhibozhongguo_dictionary'] = read_txt_to_array(f"{MAIN_CHANNELS_DIR}/ç›´æ’­ä¸­å›½.txt")

    print(f"âœ… å­—å…¸æ•°æ®åŠ è½½å®Œæˆ: CCTV({len(dictionaries['yangshi_dictionary'])}) å«è§†({len(dictionaries['weishi_dictionary'])}) åœ°æ–¹å°(27ä¸ª) å®šåˆ¶é¢‘é“(23ä¸ª)")
    return dictionaries

def load_blacklist():
    """åŠ è½½é»‘åå•"""
    print("ğŸ”§ åŠ è½½é»‘åå•...")
    blacklist_auto = read_blacklist_from_txt(f"{BLACKLIST_DIR}/blacklist_auto.txt") 
    blacklist_manual = read_blacklist_from_txt(f"{BLACKLIST_DIR}/blacklist_manual.txt") 
    combined_blacklist = set(blacklist_auto + blacklist_manual)
    print(f"âœ… é»‘åå•åŠ è½½å®Œæˆ: {len(combined_blacklist)} æ¡è®°å½•")
    return combined_blacklist

# ====== æ ¸å¿ƒåˆ†å‘é€»è¾‘ ======
def process_channel_line(line, data_containers, dictionaries, blacklist):
    """å¤„ç†å•è¡Œé¢‘é“æ•°æ®å¹¶åˆ†ç±» - æ”¯æŒåŒé¢‘é“å¤šåˆ†ç±»ä¸”å»é‡"""
    try:
        if "#genre#" not in line and "#EXTINF:" not in line and "," in line and "://" in line:
            channel_name = line.split(',')[0].strip()
            original_name = channel_name  # ä¿å­˜åŸå§‹åç§°
            channel_name = clean_channel_name(channel_name, REMOVAL_LIST)
            channel_name = traditional_to_simplified(channel_name)
            channel_address = clean_url(line.split(',')[1].strip())
            
            # è·³è¿‡é»‘åå•å’Œç‰¹å®šåè®®
            if channel_address in blacklist or should_skip_url(channel_address):
                return

            url_hash = get_url_hash(channel_address)
            processed_line = channel_name + "," + channel_address

            # ä¸»é¢‘é“åˆ†å‘ - æ”¯æŒåŒé¢‘é“å¤šåˆ†ç±»
            channel_added = False
            
            if "CCTV" in channel_name and check_url_existence(data_containers['yangshi_lines'], channel_address):
                data_containers['yangshi_lines'].append(process_name_string(processed_line))
                channel_added = True
            
            if channel_name in dictionaries['weishi_dictionary'] and check_url_existence(data_containers['weishi_lines'], channel_address):
                data_containers['weishi_lines'].append(process_name_string(processed_line))
                channel_added = True
            
            # åœ°æ–¹å°åˆ†å‘ - æ”¯æŒåŒé¢‘é“å¤šåˆ†ç±»
            if channel_name in dictionaries['beijing_dictionary'] and check_url_existence(data_containers['beijing_lines'], channel_address):
                data_containers['beijing_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['shanghai_dictionary'] and check_url_existence(data_containers['shanghai_lines'], channel_address):
                data_containers['shanghai_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['tianjin_dictionary'] and check_url_existence(data_containers['tianjin_lines'], channel_address):
                data_containers['tianjin_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['chongqing_dictionary'] and check_url_existence(data_containers['chongqing_lines'], channel_address):
                data_containers['chongqing_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['guangdong_dictionary'] and check_url_existence(data_containers['guangdong_lines'], channel_address):
                data_containers['guangdong_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['jiangsu_dictionary'] and check_url_existence(data_containers['jiangsu_lines'], channel_address):
                data_containers['jiangsu_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['zhejiang_dictionary'] and check_url_existence(data_containers['zhejiang_lines'], channel_address):
                data_containers['zhejiang_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['shandong_dictionary'] and check_url_existence(data_containers['shandong_lines'], channel_address):
                data_containers['shandong_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['henan_dictionary'] and check_url_existence(data_containers['henan_lines'], channel_address):
                data_containers['henan_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['sichuan_dictionary'] and check_url_existence(data_containers['sichuan_lines'], channel_address):
                data_containers['sichuan_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['hebei_dictionary'] and check_url_existence(data_containers['hebei_lines'], channel_address):
                data_containers['hebei_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['hunan_dictionary'] and check_url_existence(data_containers['hunan_lines'], channel_address):
                data_containers['hunan_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['hubei_dictionary'] and check_url_existence(data_containers['hubei_lines'], channel_address):
                data_containers['hubei_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['anhui_dictionary'] and check_url_existence(data_containers['anhui_lines'], channel_address):
                data_containers['anhui_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['fujian_dictionary'] and check_url_existence(data_containers['fujian_lines'], channel_address):
                data_containers['fujian_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['shanxi1_dictionary'] and check_url_existence(data_containers['shanxi1_lines'], channel_address):
                data_containers['shanxi1_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['liaoning_dictionary'] and check_url_existence(data_containers['liaoning_lines'], channel_address):
                data_containers['liaoning_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['jiangxi_dictionary'] and check_url_existence(data_containers['jiangxi_lines'], channel_address):
                data_containers['jiangxi_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['heilongjiang_dictionary'] and check_url_existence(data_containers['heilongjiang_lines'], channel_address):
                data_containers['heilongjiang_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['jilin_dictionary'] and check_url_existence(data_containers['jilin_lines'], channel_address):
                data_containers['jilin_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['shanxi2_dictionary'] and check_url_existence(data_containers['shanxi2_lines'], channel_address):
                data_containers['shanxi2_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['guangxi_dictionary'] and check_url_existence(data_containers['guangxi_lines'], channel_address):
                data_containers['guangxi_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['yunnan_dictionary'] and check_url_existence(data_containers['yunnan_lines'], channel_address):
                data_containers['yunnan_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['guizhou_dictionary'] and check_url_existence(data_containers['guizhou_lines'], channel_address):
                data_containers['guizhou_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['gansu_dictionary'] and check_url_existence(data_containers['gansu_lines'], channel_address):
                data_containers['gansu_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['neimenggu_dictionary'] and check_url_existence(data_containers['neimenggu_lines'], channel_address):
                data_containers['neimenggu_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['xinjiang_dictionary'] and check_url_existence(data_containers['xinjiang_lines'], channel_address):
                data_containers['xinjiang_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['hainan_dictionary'] and check_url_existence(data_containers['hainan_lines'], channel_address):
                data_containers['hainan_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['ningxia_dictionary'] and check_url_existence(data_containers['ningxia_lines'], channel_address):
                data_containers['ningxia_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['qinghai_dictionary'] and check_url_existence(data_containers['qinghai_lines'], channel_address):
                data_containers['qinghai_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['xizang_dictionary'] and check_url_existence(data_containers['xizang_lines'], channel_address):
                data_containers['xizang_lines'].append(process_name_string(processed_line))
                channel_added = True
            
            # å®šåˆ¶é¢‘é“åˆ†å‘
            if channel_name in dictionaries['news_dictionary'] and check_url_existence(data_containers['news_lines'], channel_address):
                data_containers['news_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['shuzi_dictionary'] and check_url_existence(data_containers['shuzi_lines'], channel_address):
                data_containers['shuzi_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['dianying_dictionary'] and check_url_existence(data_containers['dianying_lines'], channel_address):
                data_containers['dianying_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['jieshuo_dictionary'] and check_url_existence(data_containers['jieshuo_lines'], channel_address):
                data_containers['jieshuo_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['zongyi_dictionary'] and check_url_existence(data_containers['zongyi_lines'], channel_address):
                data_containers['zongyi_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['huya_dictionary'] and check_url_existence(data_containers['huya_lines'], channel_address):
                data_containers['huya_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['douyu_dictionary'] and check_url_existence(data_containers['douyu_lines'], channel_address):
                data_containers['douyu_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['xianggang_dictionary'] and check_url_existence(data_containers['xianggang_lines'], channel_address):
                data_containers['xianggang_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['aomen_dictionary'] and check_url_existence(data_containers['aomen_lines'], channel_address):
                data_containers['aomen_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['china_dictionary'] and check_url_existence(data_containers['china_lines'], channel_address):
                data_containers['china_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['guoji_dictionary'] and check_url_existence(data_containers['guoji_lines'], channel_address):
                data_containers['guoji_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['gangaotai_dictionary'] and check_url_existence(data_containers['gangaotai_lines'], channel_address):
                data_containers['gangaotai_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['dianshiju_dictionary'] and check_url_existence(data_containers['dianshiju_lines'], channel_address):
                data_containers['dianshiju_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['radio_dictionary'] and check_url_existence(data_containers['radio_lines'], channel_address):
                data_containers['radio_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['donghuapian_dictionary'] and check_url_existence(data_containers['donghuapian_lines'], channel_address):
                data_containers['donghuapian_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['jilupian_dictionary'] and check_url_existence(data_containers['jilupian_lines'], channel_address):
                data_containers['jilupian_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['tiyu_dictionary'] and check_url_existence(data_containers['tiyu_lines'], channel_address):
                data_containers['tiyu_lines'].append(process_name_string(processed_line))
                channel_added = True
            
            if any(tiyusaishi_item in channel_name for tiyusaishi_item in dictionaries['tiyusaishi_dictionary']) and check_url_existence(data_containers['tiyusaishi_lines'], channel_address):
                data_containers['tiyusaishi_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['youxi_dictionary'] and check_url_existence(data_containers['youxi_lines'], channel_address):
                data_containers['youxi_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['xiqu_dictionary'] and check_url_existence(data_containers['xiqu_lines'], channel_address):
                data_containers['xiqu_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['yinyue_dictionary'] and check_url_existence(data_containers['yinyue_lines'], channel_address):
                data_containers['yinyue_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['chunwan_dictionary'] and check_url_existence(data_containers['chunwan_lines'], channel_address):
                data_containers['chunwan_lines'].append(process_name_string(processed_line))
                channel_added = True
                
            if channel_name in dictionaries['zhibozhongguo_dictionary'] and check_url_existence(data_containers['zhibozhongguo_lines'], channel_address):
                data_containers['zhibozhongguo_lines'].append(process_name_string(processed_line))
                channel_added = True
            
            # å…¶ä»–é¢‘é“åˆ†å‘ - ä½¿ç”¨URLå“ˆå¸Œå»é‡
            if not channel_added and url_hash not in data_containers['others_lines_url']:
                data_containers['others_lines_url'].append(url_hash)
                data_containers['others_lines'].append(processed_line)

    except Exception as e:
        print(f"âŒ å¤„ç†é¢‘é“è¡Œé”™è¯¯: {e}, è¡Œå†…å®¹: {line}")

def process_url(url, data_containers, dictionaries, blacklist):
    """å¤„ç†å•ä¸ªURLæº"""
    try:
        print(f"ğŸŒ å¤„ç†URL: {url}")
        data_containers['others_lines'].append(f"â—†â—†â—† {url}")
        
        response_text = get_http_response(url)
        if not response_text:
            print(f"âŒ è·å–URLå†…å®¹å¤±è´¥: {url}")
            data_containers['others_lines'].append(f"âŒ è·å–å¤±è´¥: {url}\n")
            return

        # æ£€æŸ¥æ˜¯å¦ä¸ºM3Uæ ¼å¼
        is_m3u = response_text.startswith("#EXTM3U") or response_text.startswith("#EXTINF")
        if get_url_file_extension(url) in [".m3u", ".m3u8"] or is_m3u:
            response_text = convert_m3u_to_txt(response_text)

        lines = response_text.split('\n')
        valid_lines = 0
        
        for line in lines:
            if ("#genre#" not in line and "," in line and "://" in line and 
                not should_skip_url(line)):
                
                try:
                    channel_name, channel_address = line.split(',', 1)
                    
                    # å¤„ç†å¸¦#å·çš„åŠ é€Ÿæº
                    if "#" not in channel_address:
                        process_channel_line(line, data_containers, dictionaries, blacklist)
                        valid_lines += 1
                    else:
                        url_list = channel_address.split('#')
                        for channel_url in url_list:
                            process_channel_line(f'{channel_name},{channel_url}', data_containers, dictionaries, blacklist)
                            valid_lines += 1
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†è¡Œé”™è¯¯: {e}, è¡Œ: {line}")

        print(f"âœ… å¤„ç†å®Œæˆ: {valid_lines} ä¸ªæœ‰æ•ˆé¢‘é“")
        data_containers['others_lines'].append(f"âœ… å®Œæˆ: {valid_lines} ä¸ªé¢‘é“\n")

    except Exception as e:
        print(f"âŒ å¤„ç†URLæ—¶å‘ç”Ÿé”™è¯¯ {url}: {e}")
        data_containers['others_lines'].append(f"âŒ é”™è¯¯: {e}\n")

# ====== ä¸»å¤„ç†æµç¨‹å‡½æ•° ======
def process_url_sources(data_containers, dictionaries, blacklist):
    """å¤„ç†URLæº"""
    print("ğŸš€ å¼€å§‹å¤„ç†ç›´æ’­æº...")
    
    # å¤„ç†URLæº
    urls = read_txt_to_array(f"{ASSETS_DIR}/urls-daily.txt")
    print(f"ğŸ“¡ å‘ç° {len(urls)} ä¸ªURLæº")
    
    for url in urls:
        if url.startswith("http"):
            # å¤„ç†æ—¥æœŸå˜é‡ - ä½¿ç”¨åŒ—äº¬æ—¶é—´
            current_date_str = datetime.now(beijing_tz).strftime("%m%d")
            yesterday_date_str = (datetime.now(beijing_tz) - timedelta(days=1)).strftime("%m%d")
            
            if "{MMdd}" in url:
                url = url.replace("{MMdd}", current_date_str)
            if "{MMdd-1}" in url:
                url = url.replace("{MMdd-1}", yesterday_date_str)
            
            process_url(url, data_containers, dictionaries, blacklist)

def process_whitelist_and_manual(data_containers, dictionaries, blacklist):
    """å¤„ç†ç™½åå•å’Œæ‰‹å·¥åŒº"""
    # å¤„ç†ç™½åå•
    print("ğŸ“‹ å¤„ç†ç™½åå•...")
    whitelist_auto_lines = read_txt_to_array(f"{BLACKLIST_DIR}/whitelist_auto.txt")
    whitelist_count = 0
    for whitelist_line in whitelist_auto_lines:
        if ("#genre#" not in whitelist_line and "," in whitelist_line and 
            "://" in whitelist_line):
            whitelist_parts = whitelist_line.split(",")
            try:
                response_time = float(whitelist_parts[0].replace("ms", ""))
            except ValueError:
                response_time = 60000
            if response_time < 2000:  # 2ç§’ä»¥å†…çš„é«˜å“åº”æº
                process_channel_line(",".join(whitelist_parts[1:]), data_containers, dictionaries, blacklist)
                whitelist_count += 1
    print(f"âœ… ç™½åå•å¤„ç†å®Œæˆ: {whitelist_count} ä¸ªé«˜é€Ÿæº")

    # å¤„ç†æ‰‹å·¥åŒº
    print("ğŸ”§ å¤„ç†æ‰‹å·¥åŒº...")
    # å¤„ç†æ‰€æœ‰æ‰‹å·¥åŒºæ–‡ä»¶
    manual_files = {
        'å›½é™…é¢‘é“': 'guoji_lines',
        'æ¸¯Â·æ¾³Â·å°': 'gangaotai_lines',
        'åŠ¨Â·ç”»Â·ç‰‡': 'donghuapian_lines',
        'æ”¶Â·éŸ³Â·æœº': 'radio_lines',
        'è®°Â·å½•Â·ç‰‡': 'jilupian_lines',
        'é¦™æ¸¯é¢‘é“': 'xianggang_lines',
        'æ¾³é—¨é¢‘é“': 'aomen_lines',
        'ä¸­å›½é¢‘é“': 'china_lines',
        'æ¹–åŒ—é¢‘é“': 'hubei_lines', 
    }

    for region, target_list in manual_files.items():
        manual_file = f"{MANUAL_DIR}/{region}.txt"
        if os.path.exists(manual_file):
            manual_data = read_txt_to_array(manual_file)
            for line in manual_data:
                if "," in line and "://" in line:
                    process_channel_line(line, data_containers, dictionaries, blacklist)
            print(f"âœ… æ‰‹å·¥åŒº {region}: {len(manual_data)} æ¡è®°å½•")

    # å¤„ç†AKTV
    print("ğŸŒ å¤„ç†AKTV...")
    aktv_url = "https://aktv.space/live.m3u"
    aktv_text = get_http_response(aktv_url)
    if aktv_text:
        print("âœ… AKTVæˆåŠŸè·å–å†…å®¹")
        aktv_text = convert_m3u_to_txt(aktv_text)
        data_containers['aktv_lines'].extend(aktv_text.strip().split('\n'))
    else:
        print("âš ï¸ AKTVè¯·æ±‚å¤±è´¥ï¼Œä»æœ¬åœ°è·å–ï¼")
        data_containers['aktv_lines'].extend(read_txt_to_array(MANUAL_AKTV))
    print(f"âœ… AKTVå¤„ç†å®Œæˆ: {len(data_containers['aktv_lines'])} ä¸ªé¢‘é“")

def generate_output_files(data_containers, dictionaries):
    """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶"""
    print("ğŸ“„ ç”Ÿæˆè¾“å‡ºæ–‡ä»¶...")
    
    # åŠ è½½çº é”™æ•°æ®
    corrections_name = load_corrections_name(f"{ASSETS_DIR}/corrections_name.txt")
    
    # å¤„ç†ä½“è‚²èµ›äº‹æ—¥æœŸæ ¼å¼
    def normalize_date_to_md(text):
        """æ—¥æœŸæ ¼å¼æ ‡å‡†åŒ–"""
        try:
            text = text.strip()
            def format_md(m):
                month = int(m.group(1))
                day = int(m.group(2))
                after = m.group(3) or ''
                if not after.startswith(' '):
                    after = ' ' + after
                return f"{month}-{day}{after}"

            text = re.sub(r'^0?(\d{1,2})/0?(\d{1,2})(.*)', format_md, text)
            text = re.sub(r'^\d{4}-0?(\d{1,2})-0?(\d{1,2})(.*)', format_md, text)
            text = re.sub(r'^0?(\d{1,2})æœˆ0?(\d{1,2})æ—¥(.*)', format_md, text)
            return text
        except Exception as e:
            print(f"âš ï¸ æ—¥æœŸæ ¼å¼åŒ–é”™è¯¯: {e}, æ–‡æœ¬: {text}")
            return text

    normalized_tiyusaishi_lines = [normalize_date_to_md(s) for s in data_containers['tiyusaishi_lines']]

    # ç”Ÿæˆç‰ˆæœ¬ä¿¡æ¯ - ä½¿ç”¨åŒ—äº¬æ—¶é—´
    beijing_time = datetime.now(beijing_tz)
    formatted_time = beijing_time.strftime("%Y%m%d %H:%M:%S")

    def get_random_url(file_path):
        """éšæœºè·å–URL"""
        urls = []
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                for line in file:
                    url = line.strip().split(',')[-1]
                    urls.append(url)    
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ–‡ä»¶ {file_path} æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
        return random.choice(urls) if urls else ""

    version = formatted_time + "," + get_random_url(MANUAL_CHANNEL)
    about = "xiaoranmuze," + get_random_url(MANUAL_CHANNEL)
    daily_mtv = "ä»Šæ—¥æ¨è," + get_random_url(MANUAL_RECOMMEND)
    daily_mtv1 = "ğŸ”¥ä½è°ƒ," + get_random_url(MANUAL_RECOMMEND)
    daily_mtv2 = "ğŸ”¥ä½¿ç”¨," + get_random_url(MANUAL_RECOMMEND)
    daily_mtv3 = "ğŸ”¥ç¦æ­¢," + get_random_url(MANUAL_RECOMMEND)
    daily_mtv4 = "ğŸ”¥è´©å–," + get_random_url(MANUAL_RECOMMEND)

    # ç”Ÿæˆå…¨éƒ¨æº (full.txt)
    all_lines_full = []
    all_lines_full.extend(["ğŸŒå¤®è§†é¢‘é“,#genre#"] + sort_data(dictionaries['yangshi_dictionary'], correct_name_data(corrections_name, data_containers['yangshi_lines'])) + ['\n'])
    all_lines_full.extend(["ğŸ“¡å«è§†é¢‘é“,#genre#"] + sort_data(dictionaries['weishi_dictionary'], correct_name_data(corrections_name, data_containers['weishi_lines'])) + ['\n'])

    # åœ°æ–¹å°åˆ†ç±»
    all_lines_full.extend(["â˜˜ï¸åŒ—äº¬é¢‘é“,#genre#"] + sort_data(dictionaries['beijing_dictionary'], set(correct_name_data(corrections_name, data_containers['beijing_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸ä¸Šæµ·é¢‘é“,#genre#"] + sort_data(dictionaries['shanghai_dictionary'], set(correct_name_data(corrections_name, data_containers['shanghai_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸å¤©æ´¥é¢‘é“,#genre#"] + sort_data(dictionaries['tianjin_dictionary'], set(correct_name_data(corrections_name, data_containers['tianjin_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸é‡åº†é¢‘é“,#genre#"] + sort_data(dictionaries['chongqing_dictionary'], set(correct_name_data(corrections_name, data_containers['chongqing_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸å¹¿ä¸œé¢‘é“,#genre#"] + sort_data(dictionaries['guangdong_dictionary'], set(correct_name_data(corrections_name, data_containers['guangdong_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸æ±Ÿè‹é¢‘é“,#genre#"] + sort_data(dictionaries['jiangsu_dictionary'], set(correct_name_data(corrections_name, data_containers['jiangsu_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸æµ™æ±Ÿé¢‘é“,#genre#"] + sort_data(dictionaries['zhejiang_dictionary'], set(correct_name_data(corrections_name, data_containers['zhejiang_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸å±±ä¸œé¢‘é“,#genre#"] + sort_data(dictionaries['shandong_dictionary'], set(correct_name_data(corrections_name, data_containers['shandong_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸æ²³å—é¢‘é“,#genre#"] + sort_data(dictionaries['henan_dictionary'], set(correct_name_data(corrections_name, data_containers['henan_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸å››å·é¢‘é“,#genre#"] + sort_data(dictionaries['sichuan_dictionary'], set(correct_name_data(corrections_name, data_containers['sichuan_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸æ²³åŒ—é¢‘é“,#genre#"] + sort_data(dictionaries['hebei_dictionary'], set(correct_name_data(corrections_name, data_containers['hebei_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸æ¹–å—é¢‘é“,#genre#"] + sort_data(dictionaries['hunan_dictionary'], set(correct_name_data(corrections_name, data_containers['hunan_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸æ¹–åŒ—é¢‘é“,#genre#"] + sort_data(dictionaries['hubei_dictionary'], set(correct_name_data(corrections_name, data_containers['hubei_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸å®‰å¾½é¢‘é“,#genre#"] + sort_data(dictionaries['anhui_dictionary'], set(correct_name_data(corrections_name, data_containers['anhui_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸ç¦å»ºé¢‘é“,#genre#"] + sort_data(dictionaries['fujian_dictionary'], set(correct_name_data(corrections_name, data_containers['fujian_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸é™•è¥¿é¢‘é“,#genre#"] + sort_data(dictionaries['shanxi1_dictionary'], set(correct_name_data(corrections_name, data_containers['shanxi1_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸è¾½å®é¢‘é“,#genre#"] + sort_data(dictionaries['liaoning_dictionary'], set(correct_name_data(corrections_name, data_containers['liaoning_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸æ±Ÿè¥¿é¢‘é“,#genre#"] + sort_data(dictionaries['jiangxi_dictionary'], set(correct_name_data(corrections_name, data_containers['jiangxi_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸é»‘é¾™æ±Ÿå°,#genre#"] + sorted(set(correct_name_data(corrections_name, data_containers['heilongjiang_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸å‰æ—é¢‘é“,#genre#"] + sort_data(dictionaries['jilin_dictionary'], set(correct_name_data(corrections_name, data_containers['jilin_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸å±±è¥¿é¢‘é“,#genre#"] + sort_data(dictionaries['shanxi2_dictionary'], set(correct_name_data(corrections_name, data_containers['shanxi2_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸å¹¿è¥¿é¢‘é“,#genre#"] + sort_data(dictionaries['guangxi_dictionary'], set(correct_name_data(corrections_name, data_containers['guangxi_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸äº‘å—é¢‘é“,#genre#"] + sort_data(dictionaries['yunnan_dictionary'], set(correct_name_data(corrections_name, data_containers['yunnan_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸è´µå·é¢‘é“,#genre#"] + sort_data(dictionaries['guizhou_dictionary'], set(correct_name_data(corrections_name, data_containers['guizhou_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸ç”˜è‚ƒé¢‘é“,#genre#"] + sort_data(dictionaries['gansu_dictionary'], set(correct_name_data(corrections_name, data_containers['gansu_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸å†…è’™é¢‘é“,#genre#"] + sort_data(dictionaries['neimenggu_dictionary'], set(correct_name_data(corrections_name, data_containers['neimenggu_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸æ–°ç–†é¢‘é“,#genre#"] + sort_data(dictionaries['xinjiang_dictionary'], set(correct_name_data(corrections_name, data_containers['xinjiang_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸æµ·å—é¢‘é“,#genre#"] + sort_data(dictionaries['hainan_dictionary'], set(correct_name_data(corrections_name, data_containers['hainan_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸å®å¤é¢‘é“,#genre#"] + sort_data(dictionaries['ningxia_dictionary'], set(correct_name_data(corrections_name, data_containers['ningxia_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸é’æµ·é¢‘é“,#genre#"] + sort_data(dictionaries['qinghai_dictionary'], set(correct_name_data(corrections_name, data_containers['qinghai_lines']))) + ['\n'])
    all_lines_full.extend(["â˜˜ï¸è¥¿è—é¢‘é“,#genre#"] + sort_data(dictionaries['xizang_dictionary'], set(correct_name_data(corrections_name, data_containers['xizang_lines']))) + ['\n'])

    # å®šåˆ¶é¢‘é“
    all_lines_full.extend(["ğŸ“°æ–°é—»é¢‘é“,#genre#"] + sort_data(dictionaries['news_dictionary'], set(correct_name_data(corrections_name, data_containers['news_lines']))) + ['\n'])
    all_lines_full.extend(["ğŸï¸æ•°å­—é¢‘é“,#genre#"] + sort_data(dictionaries['shuzi_dictionary'], set(correct_name_data(corrections_name, data_containers['shuzi_lines']))) + ['\n'])
    all_lines_full.extend(["ğŸ¬ç”µå½±é¢‘é“,#genre#"] + sort_data(dictionaries['dianying_dictionary'], set(correct_name_data(corrections_name, data_containers['dianying_lines']))) + ['\n'])
    all_lines_full.extend(["ğŸ™ï¸è§£è¯´é¢‘é“,#genre#"] + sort_data(dictionaries['jieshuo_dictionary'], set(correct_name_data(corrections_name, data_containers['jieshuo_lines']))) + ['\n'])
    all_lines_full.extend(["ğŸ¤ç»¼è‰ºé¢‘é“,#genre#"] + sorted(set(correct_name_data(corrections_name, data_containers['zongyi_lines']))) + ['\n'])
    all_lines_full.extend(["ğŸ¯è™ç‰™ç›´æ’­,#genre#"] + sort_data(dictionaries['huya_dictionary'], set(correct_name_data(corrections_name, data_containers['huya_lines']))) + ['\n'])
    all_lines_full.extend(["ğŸ¬æ–—é±¼ç›´æ’­,#genre#"] + sort_data(dictionaries['douyu_dictionary'], set(correct_name_data(corrections_name, data_containers['douyu_lines']))) + ['\n'])
    all_lines_full.extend(["ğŸ‡­ğŸ‡°é¦™æ¸¯é¢‘é“,#genre#"] + sort_data(dictionaries['xianggang_dictionary'], set(correct_name_data(corrections_name, data_containers['xianggang_lines']))) + ['\n'])
    all_lines_full.extend(["ğŸ‡²ğŸ‡´æ¾³é—¨é¢‘é“,#genre#"] + sort_data(dictionaries['aomen_dictionary'], set(correct_name_data(corrections_name, data_containers['aomen_lines']))) + ['\n'])
    all_lines_full.extend(["ğŸ‡¨ğŸ‡³ä¸­å›½é¢‘é“,#genre#"] + sort_data(dictionaries['china_dictionary'], set(correct_name_data(corrections_name, data_containers['china_lines']))) + ['\n'])
    all_lines_full.extend(["ğŸŒå›½é™…é¢‘é“,#genre#"] + sort_data(dictionaries['guoji_dictionary'], set(correct_name_data(corrections_name, data_containers['guoji_lines']))) + ['\n'])
    all_lines_full.extend(["ğŸ‡¨ğŸ‡³æ¸¯Â·æ¾³Â·å°,#genre#"] + read_txt_to_array(MANUAL_GAT) + sort_data(dictionaries['gangaotai_dictionary'], set(correct_name_data(corrections_name, data_containers['gangaotai_lines']))) + data_containers['aktv_lines'] + ['\n'])
    all_lines_full.extend(["ğŸ“ºç”µÂ·è§†Â·å‰§,#genre#"] + sort_data(dictionaries['dianshiju_dictionary'], set(correct_name_data(corrections_name, data_containers['dianshiju_lines']))) + ['\n'])
    all_lines_full.extend(["ğŸ“»æ”¶Â·éŸ³Â·æœº,#genre#"] + sort_data(dictionaries['radio_dictionary'], set(correct_name_data(corrections_name, data_containers['radio_lines']))) + ['\n'])
    all_lines_full.extend(["ğŸ•åŠ¨Â·ç”»Â·ç‰‡,#genre#"] + sort_data(dictionaries['donghuapian_dictionary'], set(correct_name_data(corrections_name, data_containers['donghuapian_lines']))) + ['\n'])
    all_lines_full.extend(["ğŸ“½ï¸è®°Â·å½•Â·ç‰‡,#genre#"] + sort_data(dictionaries['jilupian_dictionary'], set(correct_name_data(corrections_name, data_containers['jilupian_lines']))) + ['\n'])
    all_lines_full.extend(["âš½ä½“è‚²é¢‘é“,#genre#"] + sort_data(dictionaries['tiyu_dictionary'], set(correct_name_data(corrections_name, data_containers['tiyu_lines']))) + ['\n'])
    all_lines_full.extend(["ğŸ†ä½“è‚²èµ›äº‹,#genre#"] + normalized_tiyusaishi_lines + ['\n'])
    all_lines_full.extend(["ğŸ®æ¸¸æˆé¢‘é“,#genre#"] + sorted(set(correct_name_data(corrections_name, data_containers['youxi_lines']))) + ['\n'])
    all_lines_full.extend(["ğŸ­æˆæ›²é¢‘é“,#genre#"] + sort_data(dictionaries['xiqu_dictionary'], set(correct_name_data(corrections_name, data_containers['xiqu_lines']))) + ['\n'])
    all_lines_full.extend(["ğŸµéŸ³ä¹é¢‘é“,#genre#"] + sort_data(dictionaries['yinyue_dictionary'], set(correct_name_data(corrections_name, data_containers['yinyue_lines']))) + ['\n'])
    all_lines_full.extend(["ğŸ‰æ˜¥æ™šé¢‘é“,#genre#"] + sort_data(dictionaries['chunwan_dictionary'], set(correct_name_data(corrections_name, data_containers['chunwan_lines']))) + ['\n'])
    all_lines_full.extend(["ğŸ“¡ç›´æ’­ä¸­å›½,#genre#"] + sort_data(dictionaries['zhibozhongguo_dictionary'], set(correct_name_data(corrections_name, data_containers['zhibozhongguo_lines']))) + ['\n'])

    # æ‰‹å·¥åŒºé¢‘é“
    all_lines_full.extend(["âœ¨ä¼˜è´¨å¤®è§†,#genre#"] + read_txt_to_array(MANUAL_CCTV) + ['\n'])
    all_lines_full.extend(["ğŸ›°ï¸ä¼˜è´¨å«è§†,#genre#"] + read_txt_to_array(MANUAL_WS) + ['\n'])

    # å…¶å®ƒå’Œæ›´æ–°ä¿¡æ¯
    all_lines_full.extend(["ğŸ“¦æ¼ç½‘ä¹‹é±¼,#genre#"] + data_containers['others_lines'] + ['\n'])
    all_lines_full.extend(["ğŸ•’æ›´æ–°æ—¶é—´,#genre#"] + [version, about, daily_mtv, daily_mtv1, daily_mtv2, daily_mtv3, daily_mtv4] + read_txt_to_array(MANUAL_ABOUT) + ['\n'])

    # ç²¾ç®€æº (lite.txt)
    all_lines_lite = []
    all_lines_lite.extend(["å¤®è§†é¢‘é“,#genre#"] + sort_data(dictionaries['yangshi_dictionary'], correct_name_data(corrections_name, data_containers['yangshi_lines'])) + ['\n'])
    all_lines_lite.extend(["å«è§†é¢‘é“,#genre#"] + sort_data(dictionaries['weishi_dictionary'], correct_name_data(corrections_name, data_containers['weishi_lines'])) + ['\n'])
    all_lines_lite.extend(["æ›´æ–°æ—¶é—´,#genre#"] + [version] + ['\n'])

    # å®šåˆ¶æº (custom.txt) - æ™ºèƒ½åˆå¹¶åœ°æ–¹å°
    all_lines_custom = []
    all_lines_custom.extend(["ğŸŒå¤®è§†é¢‘é“,#genre#"] + sort_data(dictionaries['yangshi_dictionary'], correct_name_data(corrections_name, data_containers['yangshi_lines'])) + ['\n'])
    all_lines_custom.extend(["ğŸ“¡å«è§†é¢‘é“,#genre#"] + sort_data(dictionaries['weishi_dictionary'], correct_name_data(corrections_name, data_containers['weishi_lines'])) + ['\n'])

    # æ™ºèƒ½åˆå¹¶åœ°æ–¹å° - ä¿æŒç»“æ„ä¸”å»é‡
    print("ğŸ”— æ™ºèƒ½åˆå¹¶åœ°æ–¹å°é¢‘é“...")
    
    # å®šä¹‰æ‰€æœ‰åœ°æ–¹å°æ•°æ®æº
    local_sources = [
        ("åŒ—äº¬", data_containers['beijing_lines']),
        ("ä¸Šæµ·", data_containers['shanghai_lines']),
        ("å¤©æ´¥", data_containers['tianjin_lines']),
        ("é‡åº†", data_containers['chongqing_lines']),
        ("å¹¿ä¸œ", data_containers['guangdong_lines']),
        ("æ±Ÿè‹", data_containers['jiangsu_lines']),
        ("æµ™æ±Ÿ", data_containers['zhejiang_lines']),
        ("å±±ä¸œ", data_containers['shandong_lines']),
        ("æ²³å—", data_containers['henan_lines']),
        ("å››å·", data_containers['sichuan_lines']),
        ("æ²³åŒ—", data_containers['hebei_lines']),
        ("æ¹–å—", data_containers['hunan_lines']),
        ("æ¹–åŒ—", data_containers['hubei_lines']),
        ("å®‰å¾½", data_containers['anhui_lines']),
        ("ç¦å»º", data_containers['fujian_lines']),
        ("é™•è¥¿", data_containers['shanxi1_lines']),
        ("è¾½å®", data_containers['liaoning_lines']),
        ("æ±Ÿè¥¿", data_containers['jiangxi_lines']),
        ("é»‘é¾™æ±Ÿ", data_containers['heilongjiang_lines']),
        ("å‰æ—", data_containers['jilin_lines']),
        ("å±±è¥¿", data_containers['shanxi2_lines']),
        ("å¹¿è¥¿", data_containers['guangxi_lines']),
        ("äº‘å—", data_containers['yunnan_lines']),
        ("è´µå·", data_containers['guizhou_lines']),
        ("ç”˜è‚ƒ", data_containers['gansu_lines']),
        ("å†…è’™", data_containers['neimenggu_lines']),
        ("æ–°ç–†", data_containers['xinjiang_lines']),
        ("æµ·å—", data_containers['hainan_lines']),
        ("å®å¤", data_containers['ningxia_lines']),
        ("é’æµ·", data_containers['qinghai_lines']),
        ("è¥¿è—", data_containers['xizang_lines'])
    ]
    
    # ä½¿ç”¨å­—å…¸æ¥å»é‡ï¼Œkeyæ˜¯é¢‘é“åç§°
    unique_channels = {}
    
    for region_name, channel_list in local_sources:
        for channel in channel_list:
            if ',' in channel:
                channel_name = channel.split(',')[0]
                # å¦‚æœè¿™ä¸ªé¢‘é“è¿˜æ²¡å‡ºç°è¿‡ï¼Œæˆ–è€…è¿™ä¸ªåœ°åŒºæœ‰æ›´å¥½çš„ç‰ˆæœ¬ï¼Œå°±æ›´æ–°
                if channel_name not in unique_channels:
                    unique_channels[channel_name] = channel
    
    # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
    merged_local_channels = sorted(unique_channels.values(), key=lambda x: x.split(',')[0])
    
    print(f"âœ… åœ°æ–¹å°æ™ºèƒ½åˆå¹¶å®Œæˆ: {len(merged_local_channels)} ä¸ªå”¯ä¸€é¢‘é“")
    
    # æ·»åŠ åˆå¹¶çš„åœ°æ–¹å°åˆ†ç±»
    all_lines_custom.extend(["ğŸ åœ°æ–¹å°,#genre#"] + merged_local_channels + ['\n'])
    
    # æ›´æ–°æ—¶é—´ä¿¡æ¯
    all_lines_custom.extend(["ğŸ•’æ›´æ–°æ—¶é—´,#genre#"] + [version, about, daily_mtv, daily_mtv1, daily_mtv2, daily_mtv3, daily_mtv4] + read_txt_to_array(MANUAL_ABOUT) + ['\n'])

    # å…¶å®ƒæº (others.txt)
    all_lines_others = []
    all_lines_others.extend(["æ¼ç½‘ä¹‹é±¼,#genre#"] + data_containers['others_lines'] + ['\n'])

    # ä¿å­˜å››ä¸ªç‰ˆæœ¬æ–‡ä»¶
    output_data = {
        'full': all_lines_full,
        'lite': all_lines_lite,
        'custom': all_lines_custom,
        'others': all_lines_others
    }

    for file_type, lines in output_data.items():
        file_path = {
            'full': FULL_OUTPUT,
            'lite': LITE_OUTPUT,
            'custom': CUSTOM_OUTPUT,
            'others': OTHERS_OUTPUT
        }[file_type]
        
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(lines))
            print(f"âœ… {file_type}æºå·²ä¿å­˜: {file_path}")
            
            if os.path.exists(file_path):
                file_size = os.path.getsize(file_path)
                lines_count = len(lines)
                print(f"   ğŸ“Š å¤§å°: {file_size} å­—èŠ‚, è¡Œæ•°: {lines_count}")
                
        except Exception as e:
            print(f"âŒ ä¿å­˜{file_type}æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

    # ç”ŸæˆM3Uæ–‡ä»¶
    def get_logo_by_channel_name(channel_name):
        """æ ¹æ®é¢‘é“åç§°è·å–logo"""
        try:
            channels_logos = read_txt_to_array(f"{ASSETS_DIR}/logo.txt")
            for line in channels_logos:
                if not line.strip():
                    continue
                if ',' in line:
                    name, url = line.split(',', 1)
                    if name == channel_name:
                        return url
        except Exception as e:
            print(f"âš ï¸ è·å–logoæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
        return None

    def make_m3u(txt_file, m3u_file):
        """ç”ŸæˆM3Uæ–‡ä»¶"""
        try:
            if not os.path.exists(txt_file):
                print(f"âŒ TXTæ–‡ä»¶ä¸å­˜åœ¨: {txt_file}")
                return
                
            output_text = '#EXTM3U x-tvg-url="https://live.fanmingming.cn/e.xml"\n'
            with open(txt_file, "r", encoding='utf-8') as file:
                input_text = file.read()

            lines = input_text.strip().split("\n")
            group_name = ""
            for line in lines:
                parts = line.split(",")
                if len(parts) == 2 and "#genre#" in line:
                    group_name = parts[0]
                elif len(parts) == 2:
                    channel_name = parts[0]
                    channel_url = parts[1]
                    logo_url = get_logo_by_channel_name(channel_name)
                    if logo_url is None:
                        output_text += f'#EXTINF:-1 group-title="{group_name}",{channel_name}\n{channel_url}\n'
                    else:
                        output_text += f'#EXTINF:-1 tvg-name="{channel_name}" tvg-logo="{logo_url}" group-title="{group_name}",{channel_name}\n{channel_url}\n'

            with open(f"{m3u_file}", "w", encoding='utf-8') as file:
                file.write(output_text)
            print(f"âœ… M3Uæ–‡ä»¶ç”Ÿæˆ: {m3u_file}")
        except Exception as e:
            print(f"âŒ ç”ŸæˆM3Uæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

    # ä¸ºå®Œæ•´ç‰ˆã€ç²¾ç®€ç‰ˆã€å®šåˆ¶ç‰ˆç”Ÿæˆå¯¹åº”çš„M3Uæ–‡ä»¶
    print("ğŸµ ç”ŸæˆM3Uæ–‡ä»¶...")
    make_m3u(FULL_OUTPUT, FULL_OUTPUT.replace(".txt", ".m3u"))
    make_m3u(LITE_OUTPUT, LITE_OUTPUT.replace(".txt", ".m3u"))
    make_m3u(CUSTOM_OUTPUT, CUSTOM_OUTPUT.replace(".txt", ".m3u"))

def generate_statistics(data_containers, timestart):
    """ç”Ÿæˆè¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯"""
    # ä½¿ç”¨åŒ—äº¬æ—¶é—´
    timeend = datetime.now(beijing_tz)
    elapsed_time = timeend - timestart
    total_seconds = elapsed_time.total_seconds()
    minutes = int(total_seconds // 60)
    seconds = int(total_seconds % 60)

    print("\nğŸ“Š ======== è¯¦ç»†æ‰§è¡Œç»Ÿè®¡ =======")
    print(f"â° å¼€å§‹æ—¶é—´: {timestart.strftime('%Y-%m-%d %H:%M:%S')} (åŒ—äº¬æ—¶é—´)")
    print(f"â° ç»“æŸæ—¶é—´: {timeend.strftime('%Y-%m-%d %H:%M:%S')} (åŒ—äº¬æ—¶é—´)")
    print(f"â±ï¸ æ‰§è¡Œæ—¶é—´: {minutes}åˆ†{seconds}ç§’")

    # ä¸»é¢‘é“ç»Ÿè®¡
    print("\nğŸ¯ ======== ä¸»é¢‘é“ç»Ÿè®¡ ========")
    print(f"ğŸŒ å¤®è§†æº: {len(data_containers['yangshi_lines'])} ä¸ª")
    print(f"ğŸ“¡ å«è§†æº: {len(data_containers['weishi_lines'])} ä¸ª")

    # åœ°æ–¹å°è¯¦ç»†ç»Ÿè®¡
    print("\nğŸ  ======== åœ°æ–¹å°ç»Ÿè®¡ ========")
    local_channels = {
        'åŒ—äº¬é¢‘é“': len(data_containers['beijing_lines']),
        'ä¸Šæµ·é¢‘é“': len(data_containers['shanghai_lines']),
        'å¤©æ´¥é¢‘é“': len(data_containers['tianjin_lines']),
        'é‡åº†é¢‘é“': len(data_containers['chongqing_lines']),
        'å¹¿ä¸œé¢‘é“': len(data_containers['guangdong_lines']),
        'æ±Ÿè‹é¢‘é“': len(data_containers['jiangsu_lines']),
        'æµ™æ±Ÿé¢‘é“': len(data_containers['zhejiang_lines']),
        'å±±ä¸œé¢‘é“': len(data_containers['shandong_lines']),
        'æ²³å—é¢‘é“': len(data_containers['henan_lines']),
        'å››å·é¢‘é“': len(data_containers['sichuan_lines']),
        'æ²³åŒ—é¢‘é“': len(data_containers['hebei_lines']),
        'æ¹–å—é¢‘é“': len(data_containers['hunan_lines']),
        'æ¹–åŒ—é¢‘é“': len(data_containers['hubei_lines']),
        'å®‰å¾½é¢‘é“': len(data_containers['anhui_lines']),
        'ç¦å»ºé¢‘é“': len(data_containers['fujian_lines']),
        'é™•è¥¿é¢‘é“': len(data_containers['shanxi1_lines']),
        'è¾½å®é¢‘é“': len(data_containers['liaoning_lines']),
        'æ±Ÿè¥¿é¢‘é“': len(data_containers['jiangxi_lines']),
        'é»‘é¾™æ±Ÿå°': len(data_containers['heilongjiang_lines']),
        'å‰æ—é¢‘é“': len(data_containers['jilin_lines']),
        'å±±è¥¿é¢‘é“': len(data_containers['shanxi2_lines']),
        'å¹¿è¥¿é¢‘é“': len(data_containers['guangxi_lines']),
        'äº‘å—é¢‘é“': len(data_containers['yunnan_lines']),
        'è´µå·é¢‘é“': len(data_containers['guizhou_lines']),
        'ç”˜è‚ƒé¢‘é“': len(data_containers['gansu_lines']),
        'å†…è’™é¢‘é“': len(data_containers['neimenggu_lines']),
        'æ–°ç–†é¢‘é“': len(data_containers['xinjiang_lines']),
        'æµ·å—é¢‘é“': len(data_containers['hainan_lines']),
        'å®å¤é¢‘é“': len(data_containers['ningxia_lines']),
        'é’æµ·é¢‘é“': len(data_containers['qinghai_lines']),
        'è¥¿è—é¢‘é“': len(data_containers['xizang_lines'])
    }
    
    total_local = 0
    for region, count in local_channels.items():
        if count > 0:
            print(f"  ğŸ  {region}: {count} ä¸ª")
            total_local += count
    print(f"  ğŸ“ˆ åœ°æ–¹å°æ€»è®¡: {total_local} ä¸ª")

    # å®šåˆ¶é¢‘é“è¯¦ç»†ç»Ÿè®¡
    print("\nğŸ¨ ======== å®šåˆ¶é¢‘é“ç»Ÿè®¡ ========")
    custom_channels = {
        'ğŸ“°æ–°é—»é¢‘é“': len(data_containers['news_lines']),
        'ğŸï¸æ•°å­—é¢‘é“': len(data_containers['shuzi_lines']),
        'ğŸ¬ç”µå½±é¢‘é“': len(data_containers['dianying_lines']),
        'ğŸ™ï¸è§£è¯´é¢‘é“': len(data_containers['jieshuo_lines']),
        'ğŸ¤ç»¼è‰ºé¢‘é“': len(data_containers['zongyi_lines']),
        'ğŸ¯è™ç‰™ç›´æ’­': len(data_containers['huya_lines']),
        'ğŸ¬æ–—é±¼ç›´æ’­': len(data_containers['douyu_lines']),
        'ğŸ‡­ğŸ‡°é¦™æ¸¯é¢‘é“': len(data_containers['xianggang_lines']),
        'ğŸ‡²ğŸ‡´æ¾³é—¨é¢‘é“': len(data_containers['aomen_lines']),
        'ğŸ‡¨ğŸ‡³ä¸­å›½é¢‘é“': len(data_containers['china_lines']),
        'ğŸŒå›½é™…é¢‘é“': len(data_containers['guoji_lines']),
        'ğŸ‡¨ğŸ‡³æ¸¯æ¾³å°': len(data_containers['gangaotai_lines']),
        'ğŸ“ºç”µè§†å‰§': len(data_containers['dianshiju_lines']),
        'ğŸ“»æ”¶éŸ³æœº': len(data_containers['radio_lines']),
        'ğŸ•åŠ¨ç”»ç‰‡': len(data_containers['donghuapian_lines']),
        'ğŸ“½ï¸çºªå½•ç‰‡': len(data_containers['jilupian_lines']),
        'âš½ä½“è‚²é¢‘é“': len(data_containers['tiyu_lines']),
        'ğŸ†ä½“è‚²èµ›äº‹': len(data_containers['tiyusaishi_lines']),
        'ğŸ®æ¸¸æˆé¢‘é“': len(data_containers['youxi_lines']),
        'ğŸ­æˆæ›²é¢‘é“': len(data_containers['xiqu_lines']),
        'ğŸµéŸ³ä¹é¢‘é“': len(data_containers['yinyue_lines']),
        'ğŸ‰æ˜¥æ™šé¢‘é“': len(data_containers['chunwan_lines']),
        'ğŸ“¡ç›´æ’­ä¸­å›½': len(data_containers['zhibozhongguo_lines'])
    }    
    total_custom = 0
    for category, count in custom_channels.items():
        if count > 0:
            print(f"  ğŸš€ {category}: {count} ä¸ª")
            total_custom += count
    print(f"  ğŸ“ˆ å®šåˆ¶é¢‘é“æ€»è®¡: {total_custom} ä¸ª")

    # å…¶ä»–åˆ†ç±»ç»Ÿè®¡
    print("\nğŸ“¦ ======== å…¶ä»–åˆ†ç±»ç»Ÿè®¡ ========")
    print(f"ğŸš€ AKTV: {len(data_containers['aktv_lines'])} ä¸ª")
    print(f"ğŸ† èµ›äº‹æº: {len(data_containers['tiyusaishi_lines'])} ä¸ª")
    print(f"ğŸ“¦ å…¶å®ƒæº: {len(data_containers['others_lines'])} ä¸ª")

    # æ€»è®¡ç»Ÿè®¡
    print("\nğŸ“ˆ ======== é¢‘é“æ€»è®¡ç»Ÿè®¡ ========")
    total_main = len(data_containers['yangshi_lines']) + len(data_containers['weishi_lines'])
    total_all = total_main + total_local + total_custom + len(data_containers['aktv_lines'])
    
    print(f"ğŸ¯ ä¸»é¢‘é“æ€»è®¡: {total_main} ä¸ª")
    print(f"ğŸ  åœ°æ–¹å°æ€»è®¡: {total_local} ä¸ª") 
    print(f"ğŸ¨ å®šåˆ¶é¢‘é“æ€»è®¡: {total_custom} ä¸ª")
    print(f"ğŸš€ ç‰¹æ®Šé¢‘é“: {len(data_containers['aktv_lines'])} ä¸ª")
    print(f"ğŸ“Š æ‰€æœ‰é¢‘é“æ€»è®¡: {total_all} ä¸ª")

    # æœ€ç»ˆæ£€æŸ¥æ‰€æœ‰è¾“å‡ºæ–‡ä»¶
    print("\nğŸ” ======== æ–‡ä»¶è¾“å‡ºæ£€æŸ¥ ========")
    all_files_ok = True
    output_files = {
        'å®Œæ•´ç‰ˆ': FULL_OUTPUT,
        'ç²¾ç®€ç‰ˆ': LITE_OUTPUT,
        'å®šåˆ¶ç‰ˆ': CUSTOM_OUTPUT,
        'å…¶ä»–æº': OTHERS_OUTPUT
    }
    
    for file_type, file_path in output_files.items():
        if os.path.exists(file_path):
            file_size = os.path.getsize(file_path)
            line_count = 0
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    line_count = len(f.readlines())
            except:
                pass
                
            status = "âœ…" if file_size > 0 else "âŒ"
            print(f"  {status} {file_type}: {file_path}")
            print(f"     å¤§å°: {file_size:,} å­—èŠ‚, è¡Œæ•°: {line_count}")
            if file_size == 0:
                all_files_ok = False
        else:
            print(f"  âŒ {file_type}: {file_path} (æ–‡ä»¶ä¸å­˜åœ¨)")
            all_files_ok = False

    # æ£€æŸ¥M3Uæ–‡ä»¶
    print("\nğŸµ ======== M3Uæ–‡ä»¶æ£€æŸ¥ ========")
    for file_type in ['å®Œæ•´ç‰ˆ', 'ç²¾ç®€ç‰ˆ', 'å®šåˆ¶ç‰ˆ']:
        txt_file = output_files[file_type]
        m3u_file = txt_file.replace(".txt", ".m3u")
        if os.path.exists(m3u_file):
            file_size = os.path.getsize(m3u_file)
            status = "âœ…" if file_size > 0 else "âŒ"
            print(f"  {status} {file_type}.m3u: {m3u_file}")
            print(f"     å¤§å°: {file_size:,} å­—èŠ‚")
        else:
            print(f"  âŒ {file_type}.m3u: {m3u_file} (æ–‡ä»¶ä¸å­˜åœ¨)")
            all_files_ok = False

    # æœ€ç»ˆçŠ¶æ€
    print("\nğŸ¯ ======== æ‰§è¡Œç»“æœ ========")
    if all_files_ok:
        print("ğŸ‰ æ‰€æœ‰æ–‡ä»¶ç”ŸæˆæˆåŠŸï¼")
        print(f"ğŸ“Š æ€»é¢‘é“æ•°: {total_all:,} ä¸ª")
        print(f"â±ï¸ å¤„ç†æ—¶é—´: {minutes}åˆ†{seconds}ç§’")
    else:
        print("âš ï¸ éƒ¨åˆ†æ–‡ä»¶ç”Ÿæˆæœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼")

    return total_all

# ====== ä¸»å‡½æ•° ======
def main():
    print(f"ğŸš€ å¼€å§‹å¤„ç†ç›´æ’­æº - è¾“å…¥: {SOURCE_BASE}, è¾“å‡º: {OUTPUT_BASE}")
    timestart = datetime.now(beijing_tz)
    
    # 1. åˆå§‹åŒ–æ‰€æœ‰æ•°æ®å®¹å™¨
    data_containers = {
        # ä¸»é¢‘é“
        'yangshi_lines': [],  # CCTV
        'weishi_lines': [],   # å«è§†é¢‘é“
        
        # åœ°æ–¹å°
        'beijing_lines': [], 'shanghai_lines': [], 'tianjin_lines': [], 
        'chongqing_lines': [], 'guangdong_lines': [], 'jiangsu_lines': [], 
        'zhejiang_lines': [], 'shandong_lines': [], 'henan_lines': [], 
        'sichuan_lines': [], 'hebei_lines': [], 'hunan_lines': [], 
        'hubei_lines': [], 'anhui_lines': [], 'fujian_lines': [], 
        'shanxi1_lines': [], 'liaoning_lines': [], 'jiangxi_lines': [], 
        'heilongjiang_lines': [], 'jilin_lines': [], 'shanxi2_lines': [], 
        'guangxi_lines': [], 'yunnan_lines': [], 'guizhou_lines': [], 
        'gansu_lines': [], 'neimenggu_lines': [], 'xinjiang_lines': [], 
        'hainan_lines': [], 'ningxia_lines': [], 'qinghai_lines': [], 'xizang_lines': [],
        
        # å®šåˆ¶é¢‘é“
        'news_lines': [], 'shuzi_lines': [], 'dianying_lines': [], 
        'jieshuo_lines': [], 'zongyi_lines': [], 'huya_lines': [], 
        'douyu_lines': [], 'xianggang_lines': [], 'aomen_lines': [], 
        'china_lines': [], 'guoji_lines': [], 'gangaotai_lines': [], 
        'dianshiju_lines': [], 'radio_lines': [], 'donghuapian_lines': [], 
        'jilupian_lines': [], 'tiyu_lines': [], 'tiyusaishi_lines': [], 
        'youxi_lines': [], 'xiqu_lines': [], 'yinyue_lines': [], 
        'chunwan_lines': [], 'zhibozhongguo_lines': [],
        
        # å…¶ä»–åˆ†ç±»
        'others_lines': [], 'others_lines_url': [], 'aktv_lines': []
    }
    
    # 2. åŠ è½½å­—å…¸æ•°æ®
    dictionaries = load_all_dictionaries()
    
    # 3. åŠ è½½é»‘åå•
    blacklist = load_blacklist()
    
    # 4. å¤„ç†URLæº
    process_url_sources(data_containers, dictionaries, blacklist)
    
    # 5. å¤„ç†ç™½åå•å’Œæ‰‹å·¥åŒº
    process_whitelist_and_manual(data_containers, dictionaries, blacklist)
    
    # 6. ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
    generate_output_files(data_containers, dictionaries)
    
    # 7. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    generate_statistics(data_containers, timestart)

if __name__ == "__main__":
    main()