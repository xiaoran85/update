import urllib.request
from urllib.parse import urlparse
import re #æ­£åˆ™
import os
from datetime import datetime, timedelta, timezone
import random
import opencc #ç®€ç¹è½¬æ¢

import socket
import time

#åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
os.makedirs('output/livesource', exist_ok=True)

#ç®€ç¹è½¬æ¢
def traditional_to_simplified(text: str) -> str:
    # åˆå§‹åŒ–è½¬æ¢å™¨ï¼Œ"t2s" è¡¨ç¤ºä»ç¹ä½“è½¬ä¸ºç®€ä½“
    converter = opencc.OpenCC('t2s')
    simplified_text = converter.convert(text)
    return simplified_text

# æ‰§è¡Œå¼€å§‹æ—¶é—´
timestart = datetime.now()

#è¯»å–æ–‡æœ¬æ–¹æ³•
def read_txt_to_array(file_name):
    try:
        with open(file_name, 'r', encoding='utf-8') as file:
            lines = file.readlines()
            lines = [line.strip() for line in lines if line.strip()]  # è·³è¿‡ç©ºè¡Œ
            return lines
    except FileNotFoundError:
        print(f"File '{file_name}' not found.")
        return []
    except Exception as e:
        print(f"An error occurred: {e}")
        return []

#read BlackList 2025-07-20 13:14
def read_blacklist_from_txt(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()

    BlackList = [line.split(',')[1].strip() for line in lines if ',' in line]
    return BlackList

blacklist_auto=read_blacklist_from_txt('scripts/livesource/blacklist/blacklist_auto.txt') 
blacklist_manual=read_blacklist_from_txt('scripts/livesource/blacklist/blacklist_manual.txt') 
combined_blacklist = set(blacklist_auto + blacklist_manual)

# å®šä¹‰å¤šä¸ªå¯¹è±¡ç”¨äºå­˜å‚¨ä¸åŒå†…å®¹çš„è¡Œæ–‡æœ¬
yangshi_lines = [] #CCTV
weishi_lines = [] #å«è§†é¢‘é“

# åœ°æ–¹å°
beijing_lines = [] #åŒ—äº¬é¢‘é“
shanghai_lines = [] #ä¸Šæµ·é¢‘é“
tianjin_lines = [] #å¤©æ´¥é¢‘é“
chongqing_lines = [] #é‡åº†é¢‘é“
guangdong_lines = [] #å¹¿ä¸œé¢‘é“
jiangsu_lines = [] #æ±Ÿè‹é¢‘é“
zhejiang_lines = [] #æµ™æ±Ÿé¢‘é“
shandong_lines = [] #å±±ä¸œé¢‘é“
henan_lines = [] #æ²³å—é¢‘é“
sichuan_lines = [] #å››å·é¢‘é“
hebei_lines = [] #æ²³åŒ—é¢‘é“
hunan_lines = [] #æ¹–å—é¢‘é“
hubei_lines = [] #æ¹–åŒ—é¢‘é“
anhui_lines = [] #å®‰å¾½é¢‘é“
fujian_lines = [] #ç¦å»ºé¢‘é“
shanxi1_lines = [] #é™•è¥¿é¢‘é“
liaoning_lines = [] #è¾½å®é¢‘é“
jiangxi_lines = [] #æ±Ÿè¥¿é¢‘é“
heilongjiang_lines = [] #é»‘é¾™æ±Ÿé¢‘é“
jilin_lines = [] #å‰æ—é¢‘é“
shanxi2_lines = [] #å±±è¥¿é¢‘é“
guangxi_lines = [] #å¹¿è¥¿é¢‘é“
yunnan_lines = [] #äº‘å—é¢‘é“
guizhou_lines = [] #è´µå·é¢‘é“
gansu_lines = [] #ç”˜è‚ƒé¢‘é“
neimenggu_lines = [] #å†…è’™é¢‘é“
xinjiang_lines = [] #æ–°ç–†é¢‘é“
hainan_lines = [] #æµ·å—é¢‘é“
ningxia_lines = [] #å®å¤é¢‘é“
qinghai_lines = [] #é’æµ·é¢‘é“
xizang_lines = [] #è¥¿è—é¢‘é“

# å®šåˆ¶é¢‘é“
news_lines = [] #æ–°é—»é¢‘é“
shuzi_lines = [] #æ•°å­—é¢‘é“
dianying_lines = [] #ç”µå½±é¢‘é“
jieshuo_lines = [] #è§£è¯´é¢‘é“
zongyi_lines = [] #ç»¼è‰ºé¢‘é“
huya_lines = [] #è™ç‰™ç›´æ’­
douyu_lines = [] #æ–—é±¼ç›´æ’­
xianggang_lines = [] #é¦™æ¸¯é¢‘é“
aomen_lines = [] #æ¾³é—¨é¢‘é“
china_lines = [] #ä¸­å›½é¢‘é“
guoji_lines = [] #å›½é™…é¢‘é“
gangaotai_lines = [] #æ¸¯æ¾³å°
dianshiju_lines = [] #ç”µè§†å‰§
radio_lines = [] #æ”¶éŸ³æœº
donghuapian_lines = [] #åŠ¨ç”»ç‰‡
jilupian_lines = [] #è®°å½•ç‰‡
tiyu_lines = [] #ä½“è‚²é¢‘é“
tiyusaishi_lines = [] #ä½“è‚²èµ›äº‹
youxi_lines = [] #æ¸¸æˆé¢‘é“
xiqu_lines = [] #æˆæ›²é¢‘é“
yinyue_lines = [] #éŸ³ä¹é¢‘é“
chunwan_lines = [] #æ˜¥æ™šé¢‘é“
zhibozhongguo_lines = [] #ç›´æ’­ä¸­å›½

other_lines = []
other_lines_url = [] # ä¸ºé™ä½otheræ–‡ä»¶å¤§å°ï¼Œå‰”é™¤é‡å¤urlæ·»åŠ 

# æ–°å¢ï¼šå…¨å±€URLè·Ÿè¸ªé›†åˆï¼Œç”¨äºè·¨åˆ†ç±»å»é‡
global_url_tracker = set()

def process_name_string(input_str):
    parts = input_str.split(',')
    processed_parts = []
    for part in parts:
        processed_part = process_part(part)
        processed_parts.append(processed_part)
    result_str = ','.join(processed_parts)
    return result_str

def process_part(part_str):
    # å¤„ç†é€»è¾‘
    if "CCTV" in part_str  and "://" not in part_str:
        part_str=part_str.replace("IPV6", "")  #å…ˆå‰”é™¤IPV6å­—æ ·
        part_str=part_str.replace("PLUS", "+")  #æ›¿æ¢PLUS
        part_str=part_str.replace("1080", "")  #æ›¿æ¢1080
        filtered_str = ''.join(char for char in part_str if char.isdigit() or char == 'K' or char == '+')
        if not filtered_str.strip(): #å¤„ç†ç‰¹æ®Šæƒ…å†µï¼Œå¦‚æœå‘ç°æ²¡æœ‰æ‰¾åˆ°é¢‘é“æ•°å­—è¿”å›åŸåç§°
            filtered_str=part_str.replace("CCTV", "")

        if len(filtered_str) > 2 and re.search(r'4K|8K', filtered_str):   # ç‰¹æ®Šå¤„ç†CCTVä¸­éƒ¨åˆ†4Kå’Œ8Kåç§°
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢ï¼Œåˆ é™¤4Kæˆ–8Kåé¢çš„å­—ç¬¦ï¼Œå¹¶ä¸”ä¿ç•™4Kæˆ–8K
            filtered_str = re.sub(r'(4K|8K).*', r'\1', filtered_str)
            if len(filtered_str) > 2: 
                # ç»™4Kæˆ–8Kæ·»åŠ æ‹¬å·
                filtered_str = re.sub(r'(4K|8K)', r'(\1)', filtered_str)

        return "CCTV"+filtered_str 
        
    elif "å«è§†" in part_str:
        # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ŒåŒ¹é…"å«è§†"åé¢çš„å†…å®¹
        pattern = r'å«è§†ã€Œ.*ã€'
        # ä½¿ç”¨subå‡½æ•°æ›¿æ¢åŒ¹é…çš„å†…å®¹ä¸ºç©ºå­—ç¬¦ä¸²
        result_str = re.sub(pattern, 'å«è§†', part_str)
        return result_str
    
    return part_str

# å‡†å¤‡æ”¯æŒm3uæ ¼å¼
def get_url_file_extension(url):
    # è§£æURL
    parsed_url = urlparse(url)
    # è·å–è·¯å¾„éƒ¨åˆ†
    path = parsed_url.path
    # æå–æ–‡ä»¶æ‰©å±•å
    extension = os.path.splitext(path)[1]
    return extension

def convert_m3u_to_txt(m3u_content):
    # åˆ†è¡Œå¤„ç†
    lines = m3u_content.split('\n')
    
    # ç”¨äºå­˜å‚¨ç»“æœçš„åˆ—è¡¨
    txt_lines = []
    
    # ä¸´æ—¶å˜é‡ç”¨äºå­˜å‚¨é¢‘é“åç§°
    channel_name = ""
    
    for line in lines:
        # è¿‡æ»¤æ‰ #EXTM3U å¼€å¤´çš„è¡Œ
        if line.startswith("#EXTM3U"):
            continue
        # å¤„ç† #EXTINF å¼€å¤´çš„è¡Œ
        if line.startswith("#EXTINF"):
            # è·å–é¢‘é“åç§°ï¼ˆå‡è®¾é¢‘é“åç§°åœ¨å¼•å·åï¼‰
            channel_name = line.split(',')[-1].strip()
        # å¤„ç† URL è¡Œ
        elif line.startswith("http") or line.startswith("rtmp") or line.startswith("p3p") :
            txt_lines.append(f"{channel_name},{line.strip()}")
        
        # å¤„ç†åç¼€åä¸ºm3uï¼Œä½†æ˜¯å†…å®¹ä¸ºtxtçš„æ–‡ä»¶
        if "#genre#" not in line and "," in line and "://" in line:
            # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ¹é…é¢‘é“åç§°,URL çš„æ ¼å¼ï¼Œå¹¶ç¡®ä¿ URL åŒ…å« "://"
            # xxxx,http://xxxxx.xx.xx
            pattern = r'^[^,]+,[^\s]+://[^\s]+$'
            if bool(re.match(pattern, line)):
                txt_lines.append(line)
    
    # å°†ç»“æœåˆå¹¶æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä»¥æ¢è¡Œç¬¦åˆ†éš”
    return '\n'.join(txt_lines)

# åœ¨listæ˜¯å¦å·²ç»å­˜åœ¨url 2025-07-20 13:14
def check_url_existence(data_list, url):
    """
    Check if a given URL exists in a list of data.

    :param data_list: List of strings containing the data
    :param url: The URL to check for existence
    :return: True if the URL exists in the list, otherwise False
    """
    # Extract URLs from the data list
    urls = [item.split(',')[1] for item in data_list]
    return url not in urls #å¦‚æœä¸å­˜åœ¨åˆ™è¿”å›trueï¼Œéœ€è¦

# å¤„ç†å¸¦$çš„URLï¼ŒæŠŠ$ä¹‹åçš„å†…å®¹éƒ½å»æ‰ï¼ˆåŒ…æ‹¬$ä¹Ÿå»æ‰ï¼‰ ã€2025-07-20 13:14ã€‘
def clean_url(url):
    last_dollar_index = url.rfind('$')  # å®‰å…¨èµ·è§æ‰¾æœ€åä¸€ä¸ª$å¤„ç†
    if last_dollar_index != -1:
        return url[:last_dollar_index]
    return url

# æ·»åŠ channel_nameå‰å‰”é™¤éƒ¨åˆ†ç‰¹å®šå­—ç¬¦
removal_list = ["_ç”µä¿¡", "ç”µä¿¡", "é«˜æ¸…", "é¢‘é“", "ï¼ˆHDï¼‰", "-HD","è‹±é™†","_ITV","(åŒ—ç¾)","(HK)","AKtv","ã€ŒIPV4ã€","ã€ŒIPV6ã€",
                "é¢‘é™†","å¤‡é™†","å£¹é™†","è´°é™†","åé™†","è‚†é™†","ä¼é™†","é™†é™†","æŸ’é™†", "é¢‘æ™´","é¢‘ç²¤","[è¶…æ¸…]","é«˜æ¸…","è¶…æ¸…","æ ‡æ¸…","æ–¯ç‰¹",
                "ç²¤é™†", "å›½é™†","è‚†æŸ’","é¢‘è‹±","é¢‘ç‰¹","é¢‘å›½","é¢‘å£¹","é¢‘è´°","è‚†è´°","é¢‘æµ‹","å’ªå’•","é—½ç‰¹","é«˜ç‰¹","é¢‘é«˜","é¢‘æ ‡","æ±é˜³","[HD]", "[BD]", "[SD]", "[VGA]"]
def clean_channel_name(channel_name, removal_list):
    for item in removal_list:
        channel_name = channel_name.replace(item, "")

    # æ£€æŸ¥å¹¶ç§»é™¤æœ«å°¾çš„ 'HD'
    if channel_name.endswith("HD"):
        channel_name = channel_name[:-2]  # å»æ‰æœ€åä¸¤ä¸ªå­—ç¬¦ "HD"
    
    if channel_name.endswith("å°") and len(channel_name) > 3:
        channel_name = channel_name[:-1]  # å»æ‰æœ€åä¸¤ä¸ªå­—ç¬¦ "å°"

    return channel_name

# æ–°å¢ï¼šé¢‘é“åç§°æ ‡å‡†åŒ–å‡½æ•°
def normalize_channel_name(channel_name):
    """
    æ ‡å‡†åŒ–é¢‘é“åç§°ï¼Œå»é™¤å¤šä½™çš„ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦ï¼Œä¾¿äºå»é‡æ¯”è¾ƒ
    """
    # å»é™¤å‰åç©ºæ ¼
    channel_name = channel_name.strip()
    
    # å°†å¤šä¸ªè¿ç»­ç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
    channel_name = re.sub(r'\s+', ' ', channel_name)
    
    # ç§»é™¤ç‰¹å®šæ¨¡å¼çš„åç¼€
    patterns_to_remove = [
        r'\(\d+\)$',  # æœ«å°¾çš„(æ•°å­—)
        r'\[\d+\]$',  # æœ«å°¾çš„[æ•°å­—]
        r'-\d+$',     # æœ«å°¾çš„-æ•°å­—
        r'_\d+$',     # æœ«å°¾çš„_æ•°å­—
    ]
    
    for pattern in patterns_to_remove:
        channel_name = re.sub(pattern, '', channel_name)
    
    return channel_name.strip()

# åˆ†å‘ç›´æ’­æºï¼Œå½’ç±»ï¼ŒæŠŠè¿™éƒ¨åˆ†ä»process_urlå‰¥ç¦»å‡ºæ¥ï¼Œä¸ºä»¥ååŠ å…¥whitelistæºæ¸…å•åšå‡†å¤‡ã€‚
def process_channel_line(line):
    if  "#genre#" not in line and "#EXTINF:" not in line and "," in line and "://" in line:
        channel_name=line.split(',')[0].strip()
        channel_name= clean_channel_name(channel_name, removal_list)  #åˆ†å‘å‰æ¸…ç†channel_nameä¸­ç‰¹å®šå­—ç¬¦
        channel_name = traditional_to_simplified(channel_name)  #ç¹è½¬ç®€
        normalized_name = normalize_channel_name(channel_name)  # æ ‡å‡†åŒ–é¢‘é“åç§°ç”¨äºå»é‡

        channel_address=clean_url(line.split(',')[1].strip())  #æŠŠURLä¸­$ä¹‹åçš„å†…å®¹éƒ½å»æ‰
        line=channel_name+","+channel_address #é‡æ–°ç»„ç»‡line
        
        # åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦ç”¨äºå…¨å±€å»é‡
        channel_identifier = f"{normalized_name}|{channel_address}"

        if channel_address not in combined_blacklist and channel_identifier not in global_url_tracker: # åˆ¤æ–­å½“å‰æºæ˜¯å¦åœ¨blacklistä¸­ä¸”æœªè¢«å…¨å±€è®°å½•
            global_url_tracker.add(channel_identifier)  # æ·»åŠ åˆ°å…¨å±€è·Ÿè¸ªå™¨
            
            # æ ¹æ®è¡Œå†…å®¹åˆ¤æ–­å­˜å…¥å“ªä¸ªå¯¹è±¡ï¼Œå¼€å§‹åˆ†å‘
            # ä¸»é¢‘é“
            if "CCTV" in channel_name and check_url_existence(yangshi_lines, channel_address):
                yangshi_lines.append(process_name_string(line.strip()))
            elif channel_name in weishi_dictionary and check_url_existence(weishi_lines, channel_address):
                weishi_lines.append(process_name_string(line.strip()))
            
            # åœ°æ–¹å°
            elif channel_name in beijing_dictionary and check_url_existence(beijing_lines, channel_address):
                beijing_lines.append(process_name_string(line.strip()))
            elif channel_name in shanghai_dictionary and check_url_existence(shanghai_lines, channel_address):
                shanghai_lines.append(process_name_string(line.strip()))
            elif channel_name in tianjin_dictionary and check_url_existence(tianjin_lines, channel_address):
                tianjin_lines.append(process_name_string(line.strip()))
            elif channel_name in chongqing_dictionary and check_url_existence(chongqing_lines, channel_address):
                chongqing_lines.append(process_name_string(line.strip()))
            elif channel_name in guangdong_dictionary and check_url_existence(guangdong_lines, channel_address):
                guangdong_lines.append(process_name_string(line.strip()))
            elif channel_name in jiangsu_dictionary and check_url_existence(jiangsu_lines, channel_address):
                jiangsu_lines.append(process_name_string(line.strip()))
            elif channel_name in zhejiang_dictionary and check_url_existence(zhejiang_lines, channel_address):
                zhejiang_lines.append(process_name_string(line.strip()))
            elif channel_name in shandong_dictionary and check_url_existence(shandong_lines, channel_address):
                shandong_lines.append(process_name_string(line.strip()))
            elif channel_name in henan_dictionary and check_url_existence(henan_lines, channel_address):
                henan_lines.append(process_name_string(line.strip()))
            elif channel_name in sichuan_dictionary and check_url_existence(sichuan_lines, channel_address):
                sichuan_lines.append(process_name_string(line.strip()))
            elif channel_name in hebei_dictionary and check_url_existence(hebei_lines, channel_address):
                hebei_lines.append(process_name_string(line.strip()))
            elif channel_name in hunan_dictionary and check_url_existence(hunan_lines, channel_address):
                hunan_lines.append(process_name_string(line.strip()))
            elif channel_name in hubei_dictionary and check_url_existence(hubei_lines, channel_address):
                hubei_lines.append(process_name_string(line.strip()))
            elif channel_name in anhui_dictionary and check_url_existence(anhui_lines, channel_address):
                anhui_lines.append(process_name_string(line.strip()))
            elif channel_name in fujian_dictionary and check_url_existence(fujian_lines, channel_address):
                fujian_lines.append(process_name_string(line.strip()))
            elif channel_name in shanxi1_dictionary and check_url_existence(shanxi1_lines, channel_address):
                shanxi1_lines.append(process_name_string(line.strip()))
            elif channel_name in liaoning_dictionary and check_url_existence(liaoning_lines, channel_address):
                liaoning_lines.append(process_name_string(line.strip()))
            elif channel_name in jiangxi_dictionary and check_url_existence(jiangxi_lines, channel_address):
                jiangxi_lines.append(process_name_string(line.strip()))
            elif channel_name in heilongjiang_dictionary and check_url_existence(heilongjiang_lines, channel_address):
                heilongjiang_lines.append(process_name_string(line.strip()))
            elif channel_name in jilin_dictionary and check_url_existence(jilin_lines, channel_address):
                jilin_lines.append(process_name_string(line.strip()))
            elif channel_name in shanxi2_dictionary and check_url_existence(shanxi2_lines, channel_address):
                shanxi2_lines.append(process_name_string(line.strip()))
            elif channel_name in guangxi_dictionary and check_url_existence(guangxi_lines, channel_address):
                guangxi_lines.append(process_name_string(line.strip()))
            elif channel_name in yunnan_dictionary and check_url_existence(yunnan_lines, channel_address):
                yunnan_lines.append(process_name_string(line.strip()))
            elif channel_name in guizhou_dictionary and check_url_existence(guizhou_lines, channel_address):
                guizhou_lines.append(process_name_string(line.strip()))
            elif channel_name in gansu_dictionary and check_url_existence(gansu_lines, channel_address):
                gansu_lines.append(process_name_string(line.strip()))
            elif channel_name in neimenggu_dictionary and check_url_existence(neimenggu_lines, channel_address):
                neimenggu_lines.append(process_name_string(line.strip()))
            elif channel_name in xinjiang_dictionary and check_url_existence(xinjiang_lines, channel_address):
                xinjiang_lines.append(process_name_string(line.strip()))
            elif channel_name in hainan_dictionary and check_url_existence(hainan_lines, channel_address):
                hainan_lines.append(process_name_string(line.strip()))
            elif channel_name in ningxia_dictionary and check_url_existence(ningxia_lines, channel_address):
                ningxia_lines.append(process_name_string(line.strip()))
            elif channel_name in qinghai_dictionary and check_url_existence(qinghai_lines, channel_address):
                qinghai_lines.append(process_name_string(line.strip()))
            elif channel_name in xizang_dictionary and check_url_existence(xizang_lines, channel_address):
                xizang_lines.append(process_name_string(line.strip()))
            
            # å®šåˆ¶é¢‘é“
            elif channel_name in news_dictionary and check_url_existence(news_lines, channel_address):
                news_lines.append(process_name_string(line.strip()))
            elif channel_name in shuzi_dictionary and check_url_existence(shuzi_lines, channel_address):
                shuzi_lines.append(process_name_string(line.strip()))
            elif channel_name in dianying_dictionary and check_url_existence(dianying_lines, channel_address):
                dianying_lines.append(process_name_string(line.strip()))
            elif channel_name in jieshuo_dictionary and check_url_existence(jieshuo_lines, channel_address):
                jieshuo_lines.append(process_name_string(line.strip()))
            elif channel_name in zongyi_dictionary and check_url_existence(zongyi_lines, channel_address):
                zongyi_lines.append(process_name_string(line.strip()))
            elif channel_name in huya_dictionary and check_url_existence(huya_lines, channel_address):
                huya_lines.append(process_name_string(line.strip()))
            elif channel_name in douyu_dictionary and check_url_existence(douyu_lines, channel_address):
                douyu_lines.append(process_name_string(line.strip()))
            elif channel_name in xianggang_dictionary and check_url_existence(xianggang_lines, channel_address):
                xianggang_lines.append(process_name_string(line.strip()))
            elif channel_name in aomen_dictionary and check_url_existence(aomen_lines, channel_address):
                aomen_lines.append(process_name_string(line.strip()))
            elif channel_name in china_dictionary and check_url_existence(china_lines, channel_address):
                china_lines.append(process_name_string(line.strip()))
            elif channel_name in guoji_dictionary and check_url_existence(guoji_lines, channel_address):
                guoji_lines.append(process_name_string(line.strip()))
            elif channel_name in gangaotai_dictionary and check_url_existence(gangaotai_lines, channel_address):
                gangaotai_lines.append(process_name_string(line.strip()))
            elif channel_name in dianshiju_dictionary and check_url_existence(dianshiju_lines, channel_address):
                dianshiju_lines.append(process_name_string(line.strip()))
            elif channel_name in radio_dictionary and check_url_existence(radio_lines, channel_address):
                radio_lines.append(process_name_string(line.strip()))
            elif channel_name in donghuapian_dictionary and check_url_existence(donghuapian_lines, channel_address):
                donghuapian_lines.append(process_name_string(line.strip()))
            elif channel_name in jilupian_dictionary and check_url_existence(jilupian_lines, channel_address):
                jilupian_lines.append(process_name_string(line.strip()))
            elif channel_name in tiyu_dictionary and check_url_existence(tiyu_lines, channel_address):
                tiyu_lines.append(process_name_string(line.strip()))
            elif any(tiyusaishi_dictionary in channel_name for tiyusaishi_dictionary in tiyusaishi_dictionary) and check_url_existence(tiyusaishi_lines, channel_address):
                tiyusaishi_lines.append(process_name_string(line.strip()))
            elif channel_name in youxi_dictionary and check_url_existence(youxi_lines, channel_address):
                youxi_lines.append(process_name_string(line.strip()))
            elif channel_name in xiqu_dictionary and check_url_existence(xiqu_lines, channel_address):
                xiqu_lines.append(process_name_string(line.strip()))
            elif channel_name in yinyue_dictionary and check_url_existence(yinyue_lines, channel_address):
                yinyue_lines.append(process_name_string(line.strip()))
            elif channel_name in chunwan_dictionary and check_url_existence(chunwan_lines, channel_address):
                chunwan_lines.append(process_name_string(line.strip()))
            elif channel_name in zhibozhongguo_dictionary and check_url_existence(zhibozhongguo_lines, channel_address):
                zhibozhongguo_lines.append(process_name_string(line.strip()))
            else:
                if channel_address not in other_lines_url:
                    other_lines_url.append(channel_address)   #è®°å½•å·²åŠ url
                    other_lines.append(line.strip())

# éšæœºè·å–User-Agent,å¤‡ç”¨ 
def get_random_user_agent():
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36",
    ]
    return random.choice(USER_AGENTS)

def process_url(url):
    try:
        other_lines.append("â—†â—†â—†ã€€"+url)  # å­˜å…¥other_linesä¾¿äºcheck 2025-07-20 13:14
        
        # åˆ›å»ºä¸€ä¸ªè¯·æ±‚å¯¹è±¡å¹¶æ·»åŠ è‡ªå®šä¹‰header
        req = urllib.request.Request(url)
        req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3')

        # æ‰“å¼€URLå¹¶è¯»å–å†…å®¹
        with urllib.request.urlopen(req) as response:
            # ä»¥äºŒè¿›åˆ¶æ–¹å¼è¯»å–æ•°æ®
            data = response.read()
            # å°†äºŒè¿›åˆ¶æ•°æ®è§£ç ä¸ºå­—ç¬¦ä¸²
            text = data.decode('utf-8')
            text = text.strip()

            #å¤„ç†m3uå’Œm3u8ï¼Œæå–channel_nameå’Œchannel_address
            #å¢åŠ æ‰©å±•åém3uå’Œm3u8ä¸ºæ‰©å±•åçš„m3uæ ¼å¼            
            is_m3u = text.startswith("#EXTM3U") or text.startswith("#EXTINF")
            if get_url_file_extension(url)==".m3u" or get_url_file_extension(url)==".m3u8" or is_m3u:
                text=convert_m3u_to_txt(text)

            # é€è¡Œå¤„ç†å†…å®¹
            lines = text.split('\n')
            print(f"è¡Œæ•°: {len(lines)}")
            for line in lines:
                if  "#genre#" not in line and "," in line and "://" in line and "tvbus://" not in line and "/udp/" not in line:
                    # tvbus://å‰”é™¤tvbus
                    # /udp/å‰”é™¤ç»„æ’­
                    # æ‹†åˆ†æˆé¢‘é“åå’ŒURLéƒ¨åˆ†
                    channel_name, channel_address = line.split(',', 1)
                    #éœ€è¦åŠ å¤„ç†å¸¦#å·æº=äºˆåŠ é€Ÿæº
                    if "#" not in channel_address:
                        process_channel_line(line) # å¦‚æœæ²¡æœ‰äº•å·ï¼Œåˆ™ç…§å¸¸æŒ‰ç…§æ¯è¡Œè§„åˆ™è¿›è¡Œåˆ†å‘
                    else: 
                        # å¦‚æœæœ‰"#"å·ï¼Œåˆ™æ ¹æ®"#"å·åˆ†éš”
                        url_list = channel_address.split('#')
                        for channel_url in url_list:
                            newline=f'{channel_name},{channel_url}'
                            process_channel_line(newline)

            other_lines.append('\n') #æ¯ä¸ªurlå¤„ç†å®Œæˆåï¼Œåœ¨other_linesåŠ ä¸ªå›è½¦ 2025-07-20 13:14

    except Exception as e:
        print(f"å¤„ç†URLæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

# æ–°å¢ï¼šæœ€ç»ˆå»é‡å‡½æ•°
def final_deduplicate_lines(lines):
    """
    æœ€ç»ˆå»é‡å‡½æ•°ï¼Œç¡®ä¿åŒä¸€åˆ†ç±»å†…æ²¡æœ‰é‡å¤çš„é¢‘é“
    """
    seen_channels = set()
    deduplicated = []
    
    for line in lines:
        if "#genre#" in line or line == '\n':
            # åˆ†ç±»æ ‡é¢˜å’Œç©ºè¡Œç›´æ¥ä¿ç•™
            deduplicated.append(line)
            continue
            
        if "," in line and "://" in line:
            parts = line.split(',', 1)
            if len(parts) == 2:
                channel_name, url = parts
                # ä½¿ç”¨æ ‡å‡†åŒ–åç§°å’ŒURLä½œä¸ºå”¯ä¸€æ ‡è¯†
                normalized_name = normalize_channel_name(channel_name)
                channel_id = f"{normalized_name}|{url}"
                
                if channel_id not in seen_channels:
                    seen_channels.add(channel_id)
                    deduplicated.append(line)
    
    return deduplicated

current_directory = os.getcwd()  #å‡†å¤‡è¯»å–txt

# è¯»å–å­—å…¸æ–‡æœ¬
# ä¸»é¢‘é“å­—å…¸
yangshi_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/CCTV.txt')
weishi_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/å«è§†é¢‘é“.txt')

# åœ°æ–¹å°å­—å…¸
beijing_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/åŒ—äº¬é¢‘é“.txt')
shanghai_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/ä¸Šæµ·é¢‘é“.txt')
tianjin_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/å¤©æ´¥é¢‘é“.txt')
chongqing_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/é‡åº†é¢‘é“.txt')
guangdong_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/å¹¿ä¸œé¢‘é“.txt')
jiangsu_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/æ±Ÿè‹é¢‘é“.txt')
zhejiang_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/æµ™æ±Ÿé¢‘é“.txt')
shandong_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/å±±ä¸œé¢‘é“.txt')
henan_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/æ²³å—é¢‘é“.txt')
sichuan_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/å››å·é¢‘é“.txt')
hebei_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/æ²³åŒ—é¢‘é“.txt')
hunan_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/æ¹–å—é¢‘é“.txt')
hubei_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/æ¹–åŒ—é¢‘é“.txt')
anhui_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/å®‰å¾½é¢‘é“.txt')
fujian_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/ç¦å»ºé¢‘é“.txt')
shanxi1_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/é™•è¥¿é¢‘é“.txt')
liaoning_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/è¾½å®é¢‘é“.txt')
jiangxi_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/æ±Ÿè¥¿é¢‘é“.txt')
heilongjiang_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/é»‘é¾™æ±Ÿé¢‘é“.txt')
jilin_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/å‰æ—é¢‘é“.txt')
shanxi2_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/å±±è¥¿é¢‘é“.txt')
guangxi_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/å¹¿è¥¿é¢‘é“.txt')
yunnan_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/äº‘å—é¢‘é“.txt')
guizhou_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/è´µå·é¢‘é“.txt')
gansu_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/ç”˜è‚ƒé¢‘é“.txt')
neimenggu_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/å†…è’™é¢‘é“.txt')
xinjiang_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/æ–°ç–†é¢‘é“.txt')
hainan_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/æµ·å—é¢‘é“.txt')
ningxia_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/å®å¤é¢‘é“.txt')
qinghai_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/é’æµ·é¢‘é“.txt')
xizang_dictionary = read_txt_to_array('scripts/livesource/åœ°æ–¹å°/è¥¿è—é¢‘é“.txt')

# å®šåˆ¶é¢‘é“å­—å…¸
news_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/æ–°é—»é¢‘é“.txt')
shuzi_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/æ•°å­—é¢‘é“.txt')
dianying_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/ç”µå½±é¢‘é“.txt')
jieshuo_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/è§£è¯´é¢‘é“.txt')
zongyi_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/ç»¼è‰ºé¢‘é“.txt')
huya_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/è™ç‰™ç›´æ’­.txt')
douyu_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/æ–—é±¼ç›´æ’­.txt')
xianggang_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/é¦™æ¸¯é¢‘é“.txt')
aomen_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/æ¾³é—¨é¢‘é“.txt')
china_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/ä¸­å›½é¢‘é“.txt')
guoji_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/å›½é™…é¢‘é“.txt')
gangaotai_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/æ¸¯æ¾³å°.txt')
dianshiju_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/ç”µè§†å‰§.txt')
radio_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/æ”¶éŸ³æœº.txt')
donghuapian_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/åŠ¨ç”»ç‰‡.txt')
jilupian_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/è®°å½•ç‰‡.txt')
tiyu_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/ä½“è‚²é¢‘é“.txt')
tiyusaishi_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/ä½“è‚²èµ›äº‹.txt')
youxi_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/æ¸¸æˆé¢‘é“.txt')
xiqu_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/æˆæ›²é¢‘é“.txt')
yinyue_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/éŸ³ä¹é¢‘é“.txt')
chunwan_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/æ˜¥æ™šé¢‘é“.txt')
zhibozhongguo_dictionary = read_txt_to_array('scripts/livesource/ä¸»é¢‘é“/ç›´æ’­ä¸­å›½.txt')

#è¯»å–çº é”™é¢‘é“åç§°æ–¹æ³•
def load_corrections_name(filename):
    corrections = {}
    with open(filename, 'r', encoding='utf-8') as f:
        for line in f:
            if not line.strip(): #è·³è¿‡ç©ºè¡Œ
                continue
            parts = line.strip().split(',')
            correct_name = parts[0]
            for name in parts[1:]:
                corrections[name] = correct_name
    return corrections

#è¯»å–çº é”™æ–‡ä»¶
corrections_name = load_corrections_name('scripts/livesource/corrections_name.txt')

#çº é”™é¢‘é“åç§°
def correct_name_data(corrections, data):
    corrected_data = []
    for line in data:
        line = line.strip()
        if ',' not in line:
            # è¡Œæ ¼å¼é”™è¯¯ï¼šè·³è¿‡æˆ–è®°å½•
            continue

        name, url = line.split(',', 1)

        # ç©º name å¤„ç†ï¼ˆå¯é€‰ï¼‰
        if name in corrections and name != corrections[name]:
            name = corrections[name]

        corrected_data.append(f"{name},{url}")
    return corrected_data

def sort_data(order, data):
    # åˆ›å»ºä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨æ¯è¡Œæ•°æ®çš„ç´¢å¼•
    order_dict = {name: i for i, name in enumerate(order)}
    
    # å®šä¹‰ä¸€ä¸ªæ’åºé”®å‡½æ•°ï¼Œå¤„ç†ä¸åœ¨ order_dict ä¸­çš„å­—ç¬¦ä¸²
    def sort_key(line):
        name = line.split(',')[0]
        return order_dict.get(name, len(order))
    
    # æŒ‰ç…§ order ä¸­çš„é¡ºåºå¯¹æ•°æ®è¿›è¡Œæ’åº
    sorted_data = sorted(data, key=sort_key)
    return sorted_data

# å®šä¹‰
urls = read_txt_to_array('scripts/livesource/urls-daily.txt')
# å¤„ç†
for url in urls:
    if url.startswith("http"):
        if "{MMdd}" in url: #ç‰¹åˆ«å¤„ç†113
            current_date_str = datetime.now().strftime("%m%d")
            url=url.replace("{MMdd}", current_date_str)

        if "{MMdd-1}" in url: #ç‰¹åˆ«å¤„ç†113
            yesterday_date_str = (datetime.now() - timedelta(days=1)).strftime("%m%d")
            url=url.replace("{MMdd-1}", yesterday_date_str)
            
        print(f"å¤„ç†URL: {url}")
        process_url(url)

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæå–æ¯è¡Œä¸­é€—å·å‰é¢çš„æ•°å­—éƒ¨åˆ†ä½œä¸ºæ’åºçš„ä¾æ®
def extract_number(s):
    num_str = s.split(',')[0].split('-')[1]  # æå–é€—å·å‰é¢çš„æ•°å­—éƒ¨åˆ†
    numbers = re.findall(r'\d+', num_str)   #å› ä¸ºæœ‰+å’ŒK
    return int(numbers[-1]) if numbers else 999

# å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰æ’åºå‡½æ•°
def custom_sort(s):
    if "CCTV-4K" in s:
        return 2  # å°†åŒ…å« "4K" çš„å­—ç¬¦ä¸²æ’åœ¨åé¢
    elif "CCTV-8K" in s:
        return 3  # å°†åŒ…å« "8K" çš„å­—ç¬¦ä¸²æ’åœ¨åé¢ 
    elif "(4K)" in s:
        return 1  # å°†åŒ…å« " (4K)" çš„å­—ç¬¦ä¸²æ’åœ¨åé¢
    else:
        return 0  # å…¶ä»–å­—ç¬¦ä¸²ä¿æŒåŸé¡ºåº

#è¯»å–whitelist,æŠŠé«˜å“åº”æºä»ç™½åå•ä¸­æŠ½å‡ºåŠ å…¥merged_outputã€‚
print(f"ADD whitelist_auto.txt")
whitelist_auto_lines=read_txt_to_array('scripts/livesource/blacklist/whitelist_auto.txt') #
for whitelist_line in whitelist_auto_lines:
    if  "#genre#" not in whitelist_line and "," in whitelist_line and "://" in whitelist_line:
        whitelist_parts = whitelist_line.split(",")
        try:
            response_time = float(whitelist_parts[0].replace("ms", ""))
        except ValueError:
            print(f"response_timeè½¬æ¢å¤±è´¥: {whitelist_line}")
            response_time = 60000  # å•ä½æ¯«ç§’ï¼Œè½¬æ¢å¤±è´¥ç»™ä¸ª60ç§’
        if response_time < 2000:  #2sä»¥å†…çš„é«˜å“åº”æº
            process_channel_line(",".join(whitelist_parts[1:]))

def get_http_response(url, timeout=8, retries=2, backoff_factor=1.0):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
    }
    for attempt in range(retries):
        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=timeout) as response:
                data = response.read()
                return data.decode('utf-8')
        except urllib.error.HTTPError as e:
            print(f"[HTTPError] Code: {e.code}, URL: {url}")
            break  # ä¸€èˆ¬æ¥è¯´ HTTP é”™è¯¯ä¸ä¼šåœ¨é‡è¯•ä¸­æ¢å¤
        except urllib.error.URLError as e:
            print(f"[URLError] Reason: {e.reason}, Attempt: {attempt + 1}")
        except socket.timeout:
            print(f"[Timeout] URL: {url}, Attempt: {attempt + 1}")
        except Exception as e:
            print(f"[Exception] {type(e).__name__}: {e}, Attempt: {attempt + 1}")
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
        if attempt < retries - 1:
            time.sleep(backoff_factor * (2 ** attempt))
    
    return None  # æ‰€æœ‰å°è¯•å¤±è´¥åè¿”å› None

# å°†æ—¥æœŸç»Ÿä¸€æ ¼å¼åŒ–ä¸º MM-DDæ ¼å¼
def normalize_date_to_md(text):
    text = text.strip()

    # å®šä¹‰æ›¿æ¢å‡½æ•°ï¼šç¡®ä¿åé¢æœ‰ä¸€ä¸ªç©ºæ ¼
    def format_md(m):
        month = int(m.group(1))
        day = int(m.group(2))
        after = m.group(3) or ''
        # å¦‚æœ after ä¸æ˜¯ä»¥ç©ºæ ¼å¼€å¤´ï¼Œå°±åŠ ä¸€ä¸ªç©ºæ ¼
        if not after.startswith(' '):
            after = ' ' + after
        return f"{month}-{day}{after}"

    # MM/DD
    text = re.sub(r'^0?(\d{1,2})/0?(\d{1,2})(.*)', format_md, text)

    # YYYY-MM-DD
    text = re.sub(r'^\d{4}-0?(\d{1,2})-0?(\d{1,2})(.*)', format_md, text)

    # ä¸­æ–‡æ—¥æœŸ
    text = re.sub(r'^0?(\d{1,2})æœˆ0?(\d{1,2})æ—¥(.*)', format_md, text)

    return text

# å°†æ—¥æœŸç»Ÿä¸€æ ¼å¼åŒ–ä¸º MM-DDæ ¼å¼
normalized_tiyusaishi_lines = [normalize_date_to_md(s) for s in tiyusaishi_lines]

#AKTV#
aktv_lines = [] #AKTV
aktv_url = "https://aktv.space/live.m3u" #AKTV

aktv_text = get_http_response(aktv_url)
if aktv_text:
    print("AKTVæˆåŠŸè·å–å†…å®¹")
    aktv_text = convert_m3u_to_txt(aktv_text)
    aktv_lines = aktv_text.strip().split('\n')
else:
    print("AKTVè¯·æ±‚å¤±è´¥ï¼Œä»æœ¬åœ°è·å–ï¼")
    aktv_lines = read_txt_to_array('scripts/livesource/æ‰‹å·¥åŒº/AKTV.txt')

def generate_playlist_html(data_list, output_file='playlist.html'):
    html_head = '''
    <!DOCTYPE html>
    <html lang="zh">
    <head>
        <meta charset="UTF-8">        
        <script async src="https://pagead2.googlesyndication.compagead/js/adsbygoogle.js?client=ca-pub-6061710286208572"
     crossorigin="anonymous"></script>
        <!-- Setup Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-BS1Z4F5BDN"></script>
        <script> 
        window.dataLayer = window.dataLayer || []; 
        function gtag(){dataLayer.push(arguments);} 
        gtag('js', new Date()); 
        gtag('config', 'G-BS1Z4F5BDN'); 
        </script>
        <title>æœ€æ–°ä½“è‚²èµ›äº‹</title>
        <style>
            body { font-family: sans-serif; padding: 20px; background: #f9f9f9; }
            .item { margin-bottom: 20px; padding: 12px; background: #fff; border-radius: 8px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.06); }
            .title { font-weight: bold; font-size: 1.1em; color: #333; margin-bottom: 5px; }
            .url-wrapper { display: flex; align-items: center; gap: 10px; }
            .url {
                max-width: 80%;
                white-space: nowrap;
                overflow: hidden;
                text-overflow: ellipsis;
                font-size: 0.9em;
                color: #555;
                background: #f0f0f0;
                padding: 6px;
                border-radius: 4px;
                flex-grow: 1;
            }
            .copy-btn {
                background-color: #007BFF;
                border: none;
                color: white;
                padding: 6px 10px;
                border-radius: 4px;
                cursor: pointer;
                font-size: 0.8em;
            }
            .copy-btn:hover {
                background-color: #0056b3;
            }
        </style>
    </head>
    <body>
    <h2>ğŸ“‹ æœ€æ–°ä½“è‚²èµ›äº‹åˆ—è¡¨</h2>
    '''

    html_body = ''
    for idx, entry in enumerate(data_list):
        if ',' not in entry:
            continue
        info, url = entry.split(',', 1)
        url_id = f"url_{idx}"
        html_body += f'''
        <div class="item">
            <div class="title">ğŸ•’ {info}</div>
            <div class="url-wrapper">
                <div class="url" id="{url_id}">{url}</div>
                <button class="copy-btn" onclick="copyToClipboard('{url_id}')">å¤åˆ¶</button>
            </div>
        </div>
        '''

    html_tail = '''
    <script>
        function copyToClipboard(id) {
            const el = document.getElementById(id);
            const text = el.textContent;
            navigator.clipboard.writeText(text).then(() => {
                alert("å·²å¤åˆ¶é“¾æ¥ï¼");
            }).catch(err => {
                alert("å¤åˆ¶å¤±è´¥: " + err);
            });
        }
    </script>
    </body>
    </html>
    '''

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(html_head + html_body + html_tail)
    print(f"âœ… ç½‘é¡µå·²ç”Ÿæˆï¼š{output_file}")

generate_playlist_html(sorted(set(normalized_tiyusaishi_lines)), 'output/livesource/sports.html')

# éšæœºå–å¾—URL
def get_random_url(file_path):
    urls = []
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            # æŸ¥æ‰¾é€—å·åé¢çš„éƒ¨åˆ†ï¼Œå³URL
            url = line.strip().split(',')[-1]
            urls.append(url)    
    # éšæœºè¿”å›ä¸€ä¸ªURL
    return random.choice(urls) if urls else None

# è·å–å½“å‰çš„ UTC æ—¶é—´
utc_time = datetime.now(timezone.utc)
# åŒ—äº¬æ—¶é—´
beijing_time = utc_time + timedelta(hours=8)
# æ ¼å¼åŒ–ä¸ºæ‰€éœ€çš„æ ¼å¼
formatted_time = beijing_time.strftime("%Y%m%d %H:%M:%S")

version=formatted_time+","+get_random_url('scripts/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨å°.txt')
about="xiaoranmuze,"+get_random_url('scripts/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨å°.txt')

daily_mtv="ä»Šæ—¥æ¨è,"+get_random_url('scripts/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt')
daily_mtv1="ğŸ”¥ä½è°ƒ,"+get_random_url('scripts/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt')
daily_mtv2="ğŸ”¥ä½¿ç”¨,"+get_random_url('scripts/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt')
daily_mtv3="ğŸ”¥ç¦æ­¢,"+get_random_url('scripts/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt')
daily_mtv4="ğŸ”¥è´©å–,"+get_random_url('scripts/livesource/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt')

# å¢åŠ æ‰‹å·¥åŒº
print(f"å¤„ç†æ‰‹å·¥åŒº...")
# æŒ‰ç…§æ‰‹å·¥åŒºæ˜ å°„è¿›è¡Œå¤„ç†
hubei_lines = hubei_lines + read_txt_to_array('scripts/livesource/æ‰‹å·¥åŒº/æ¹–åŒ—é¢‘é“.txt')
guoji_lines = guoji_lines + read_txt_to_array('scripts/livesource/æ‰‹å·¥åŒº/å›½é™…é¢‘é“.txt')
gangaotai_lines = gangaotai_lines + read_txt_to_array('scripts/livesource/æ‰‹å·¥åŒº/æ¸¯Â·æ¾³Â·å°.txt')
donghuapian_lines = donghuapian_lines + read_txt_to_array('scripts/livesource/æ‰‹å·¥åŒº/åŠ¨Â·ç”»Â·ç‰‡.txt')
radio_lines = radio_lines + read_txt_to_array('scripts/livesource/æ‰‹å·¥åŒº/æ”¶Â·éŸ³Â·æœº.txt')
jilupian_lines = jilupian_lines + read_txt_to_array('scripts/livesource/æ‰‹å·¥åŒº/è®°Â·å½•Â·ç‰‡.txt')
xianggang_lines = xianggang_lines + read_txt_to_array('scripts/livesource/æ‰‹å·¥åŒº/é¦™æ¸¯é¢‘é“.txt')
aomen_lines = aomen_lines + read_txt_to_array('scripts/livesource/æ‰‹å·¥åŒº/æ¾³é—¨é¢‘é“.txt')
china_lines = china_lines + read_txt_to_array('scripts/livesource/æ‰‹å·¥åŒº/ä¸­å›½é¢‘é“.txt')

# æœ€ç»ˆå»é‡å¤„ç†æ‰€æœ‰åˆ†ç±»åˆ—è¡¨
print("æ­£åœ¨è¿›è¡Œæœ€ç»ˆå»é‡å¤„ç†...")
all_category_lists = [
    yangshi_lines, weishi_lines, beijing_lines, shanghai_lines, tianjin_lines, chongqing_lines,
    guangdong_lines, jiangsu_lines, zhejiang_lines, shandong_lines, henan_lines, sichuan_lines,
    hebei_lines, hunan_lines, hubei_lines, anhui_lines, fujian_lines, shanxi1_lines, liaoning_lines,
    jiangxi_lines, heilongjiang_lines, jilin_lines, shanxi2_lines, guangxi_lines, yunnan_lines,
    guizhou_lines, gansu_lines, neimenggu_lines, xinjiang_lines, hainan_lines, ningxia_lines,
    qinghai_lines, xizang_lines, news_lines, shuzi_lines, dianying_lines, jieshuo_lines, zongyi_lines,
    huya_lines, douyu_lines, xianggang_lines, aomen_lines, china_lines, guoji_lines, gangaotai_lines,
    dianshiju_lines, radio_lines, donghuapian_lines, jilupian_lines, tiyu_lines, tiyusaishi_lines,
    youxi_lines, xiqu_lines, yinyue_lines, chunwan_lines, zhibozhongguo_lines
]

# å¯¹æ¯ä¸ªåˆ†ç±»åˆ—è¡¨è¿›è¡Œæœ€ç»ˆå»é‡
for i, category_list in enumerate(all_category_lists):
    all_category_lists[i] = final_deduplicate_lines(category_list)

# é‡æ–°èµ‹å€¼å»é‡åçš„åˆ—è¡¨
(yangshi_lines, weishi_lines, beijing_lines, shanghai_lines, tianjin_lines, chongqing_lines,
 guangdong_lines, jiangsu_lines, zhejiang_lines, shandong_lines, henan_lines, sichuan_lines,
 hebei_lines, hunan_lines, hubei_lines, anhui_lines, fujian_lines, shanxi1_lines, liaoning_lines,
 jiangxi_lines, heilongjiang_lines, jilin_lines, shanxi2_lines, guangxi_lines, yunnan_lines,
 guizhou_lines, gansu_lines, neimenggu_lines, xinjiang_lines, hainan_lines, ningxia_lines,
 qinghai_lines, xizang_lines, news_lines, shuzi_lines, dianying_lines, jieshuo_lines, zongyi_lines,
 huya_lines, douyu_lines, xianggang_lines, aomen_lines, china_lines, guoji_lines, gangaotai_lines,
 dianshiju_lines, radio_lines, donghuapian_lines, jilupian_lines, tiyu_lines, tiyusaishi_lines,
 youxi_lines, xiqu_lines, yinyue_lines, chunwan_lines, zhibozhongguo_lines) = all_category_lists

# å®Œæ•´ç‰ˆ åˆå¹¶æ‰€æœ‰å¯¹è±¡ä¸­çš„è¡Œæ–‡æœ¬
all_lines_full = ["ğŸŒå¤®è§†é¢‘é“,#genre#"] + sort_data(yangshi_dictionary, correct_name_data(corrections_name, yangshi_lines)) + ['\n'] + \
    ["ğŸ“¡å«è§†é¢‘é“,#genre#"] + sort_data(weishi_dictionary, correct_name_data(corrections_name, weishi_lines)) + ['\n'] + \
    ["ğŸ™ï¸åŒ—äº¬é¢‘é“,#genre#"] + sort_data(beijing_dictionary, set(correct_name_data(corrections_name, beijing_lines))) + ['\n'] + \
    ["ğŸ™ï¸ä¸Šæµ·é¢‘é“,#genre#"] + sort_data(shanghai_dictionary, set(correct_name_data(corrections_name, shanghai_lines))) + ['\n'] + \
    ["ğŸ™ï¸å¤©æ´¥é¢‘é“,#genre#"] + sort_data(tianjin_dictionary, set(correct_name_data(corrections_name, tianjin_lines))) + ['\n'] + \
    ["ğŸ™ï¸é‡åº†é¢‘é“,#genre#"] + sort_data(chongqing_dictionary, set(correct_name_data(corrections_name, chongqing_lines))) + ['\n'] + \
    ["ğŸ™ï¸å¹¿ä¸œé¢‘é“,#genre#"] + sort_data(guangdong_dictionary, set(correct_name_data(corrections_name, guangdong_lines))) + ['\n'] + \
    ["ğŸ™ï¸æ±Ÿè‹é¢‘é“,#genre#"] + sort_data(jiangsu_dictionary, set(correct_name_data(corrections_name, jiangsu_lines))) + ['\n'] + \
    ["ğŸ™ï¸æµ™æ±Ÿé¢‘é“,#genre#"] + sort_data(zhejiang_dictionary, set(correct_name_data(corrections_name, zhejiang_lines))) + ['\n'] + \
    ["ğŸ™ï¸å±±ä¸œé¢‘é“,#genre#"] + sort_data(shandong_dictionary, set(correct_name_data(corrections_name, shandong_lines))) + ['\n'] + \
    ["ğŸ™ï¸æ²³å—é¢‘é“,#genre#"] + sort_data(henan_dictionary, set(correct_name_data(corrections_name, henan_lines))) + ['\n'] + \
    ["ğŸ™ï¸å››å·é¢‘é“,#genre#"] + sort_data(sichuan_dictionary, set(correct_name_data(corrections_name, sichuan_lines))) + ['\n'] + \
    ["ğŸ™ï¸æ²³åŒ—é¢‘é“,#genre#"] + sort_data(hebei_dictionary, set(correct_name_data(corrections_name, hebei_lines))) + ['\n'] + \
    ["ğŸ™ï¸æ¹–å—é¢‘é“,#genre#"] + sort_data(hunan_dictionary, set(correct_name_data(corrections_name, hunan_lines))) + ['\n'] + \
    ["ğŸ™ï¸æ¹–åŒ—é¢‘é“,#genre#"] + sort_data(hubei_dictionary, set(correct_name_data(corrections_name, hubei_lines))) + ['\n'] + \
    ["ğŸ™ï¸å®‰å¾½é¢‘é“,#genre#"] + sort_data(anhui_dictionary, set(correct_name_data(corrections_name, anhui_lines))) + ['\n'] + \
    ["ğŸ™ï¸ç¦å»ºé¢‘é“,#genre#"] + sort_data(fujian_dictionary, set(correct_name_data(corrections_name, fujian_lines))) + ['\n'] + \
    ["ğŸ™ï¸é™•è¥¿é¢‘é“,#genre#"] + sort_data(shanxi1_dictionary, set(correct_name_data(corrections_name, shanxi1_lines))) + ['\n'] + \
    ["ğŸ™ï¸è¾½å®é¢‘é“,#genre#"] + sort_data(liaoning_dictionary, set(correct_name_data(corrections_name, liaoning_lines))) + ['\n'] + \
    ["ğŸ™ï¸æ±Ÿè¥¿é¢‘é“,#genre#"] + sort_data(jiangxi_dictionary, set(correct_name_data(corrections_name, jiangxi_lines))) + ['\n'] + \
    ["ğŸ™ï¸é»‘é¾™æ±Ÿé¢‘é“,#genre#"] + sort_data(heilongjiang_dictionary, set(correct_name_data(corrections_name, heilongjiang_lines))) + ['\n'] + \
    ["ğŸ™ï¸å‰æ—é¢‘é“,#genre#"] + sort_data(jilin_dictionary, set(correct_name_data(corrections_name, jilin_lines))) + ['\n'] + \
    ["ğŸ™ï¸å±±è¥¿é¢‘é“,#genre#"] + sort_data(shanxi2_dictionary, set(correct_name_data(corrections_name, shanxi2_lines))) + ['\n'] + \
    ["ğŸ™ï¸å¹¿è¥¿é¢‘é“,#genre#"] + sort_data(guangxi_dictionary, set(correct_name_data(corrections_name, guangxi_lines))) + ['\n'] + \
    ["ğŸ™ï¸äº‘å—é¢‘é“,#genre#"] + sort_data(yunnan_dictionary, set(correct_name_data(corrections_name, yunnan_lines))) + ['\n'] + \
    ["ğŸ™ï¸è´µå·é¢‘é“,#genre#"] + sort_data(guizhou_dictionary, set(correct_name_data(corrections_name, guizhou_lines))) + ['\n'] + \
    ["ğŸ™ï¸ç”˜è‚ƒé¢‘é“,#genre#"] + sort_data(gansu_dictionary, set(correct_name_data(corrections_name, gansu_lines))) + ['\n'] + \
    ["ğŸ™ï¸å†…è’™é¢‘é“,#genre#"] + sort_data(neimenggu_dictionary, set(correct_name_data(corrections_name, neimenggu_lines))) + ['\n'] + \
    ["ğŸ™ï¸æ–°ç–†é¢‘é“,#genre#"] + sort_data(xinjiang_dictionary, set(correct_name_data(corrections_name, xinjiang_lines))) + ['\n'] + \
    ["ğŸ™ï¸æµ·å—é¢‘é“,#genre#"] + sort_data(hainan_dictionary, set(correct_name_data(corrections_name, hainan_lines))) + ['\n'] + \
    ["ğŸ™ï¸å®å¤é¢‘é“,#genre#"] + sort_data(ningxia_dictionary, set(correct_name_data(corrections_name, ningxia_lines))) + ['\n'] + \
    ["ğŸ™ï¸é’æµ·é¢‘é“,#genre#"] + sort_data(qinghai_dictionary, set(correct_name_data(corrections_name, qinghai_lines))) + ['\n'] + \
    ["ğŸ™ï¸è¥¿è—é¢‘é“,#genre#"] + sort_data(xizang_dictionary, set(correct_name_data(corrections_name, xizang_lines))) + ['\n'] + \
    ["ğŸ“°æ–°é—»é¢‘é“,#genre#"] + sort_data(news_dictionary, set(correct_name_data(corrections_name, news_lines))) + ['\n'] + \
    ["ğŸ”¢æ•°å­—é¢‘é“,#genre#"] + sort_data(shuzi_dictionary, set(correct_name_data(corrections_name, shuzi_lines))) + ['\n'] + \
    ["ğŸ¬ç”µå½±é¢‘é“,#genre#"] + sort_data(dianying_dictionary, set(correct_name_data(corrections_name, dianying_lines))) + ['\n'] + \
    ["ğŸ™ï¸è§£è¯´é¢‘é“,#genre#"] + sort_data(jieshuo_dictionary, set(correct_name_data(corrections_name, jieshuo_lines))) + ['\n'] + \
    ["ğŸ­ç»¼è‰ºé¢‘é“,#genre#"] + sort_data(zongyi_dictionary, set(correct_name_data(corrections_name, zongyi_lines))) + ['\n'] + \
    ["ğŸ¯è™ç‰™ç›´æ’­,#genre#"] + sort_data(huya_dictionary, set(correct_name_data(corrections_name, huya_lines))) + ['\n'] + \
    ["ğŸ¬æ–—é±¼ç›´æ’­,#genre#"] + sort_data(douyu_dictionary, set(correct_name_data(corrections_name, douyu_lines))) + ['\n'] + \
    ["ğŸ‡­ğŸ‡°é¦™æ¸¯é¢‘é“,#genre#"] + sort_data(xianggang_dictionary, set(correct_name_data(corrections_name, xianggang_lines))) + ['\n'] + \
    ["ğŸ‡²ğŸ‡´æ¾³é—¨é¢‘é“,#genre#"] + sort_data(aomen_dictionary, set(correct_name_data(corrections_name, aomen_lines))) + ['\n'] + \
    ["ğŸ‡¨ğŸ‡³ä¸­å›½é¢‘é“,#genre#"] + sort_data(china_dictionary, set(correct_name_data(corrections_name, china_lines))) + ['\n'] + \
    ["ğŸŒå›½é™…é¢‘é“,#genre#"] + sort_data(guoji_dictionary, set(correct_name_data(corrections_name, guoji_lines))) + ['\n'] + \
    ["ğŸ‡¨ğŸ‡³æ¸¯æ¾³å°,#genre#"] + sort_data(gangaotai_dictionary, set(correct_name_data(corrections_name, gangaotai_lines))) + ['\n'] + \
    ["ğŸ“ºç”µè§†å‰§,#genre#"] + sort_data(dianshiju_dictionary, set(correct_name_data(corrections_name, dianshiju_lines))) + ['\n'] + \
    ["ğŸ“»æ”¶éŸ³æœº,#genre#"] + sort_data(radio_dictionary, set(correct_name_data(corrections_name, radio_lines))) + ['\n'] + \
    ["ğŸ¶åŠ¨ç”»ç‰‡,#genre#"] + sort_data(donghuapian_dictionary, set(correct_name_data(corrections_name, donghuapian_lines))) + ['\n'] + \
    ["ğŸï¸è®°å½•ç‰‡,#genre#"] + sort_data(jilupian_dictionary, set(correct_name_data(corrections_name, jilupian_lines))) + ['\n'] + \
    ["âš½ä½“è‚²é¢‘é“,#genre#"] + sort_data(tiyu_dictionary, set(correct_name_data(corrections_name, tiyu_lines))) + ['\n'] + \
    ["ğŸ†ä½“è‚²èµ›äº‹,#genre#"] + normalized_tiyusaishi_lines + ['\n'] + \
    ["ğŸ®æ¸¸æˆé¢‘é“,#genre#"] + sort_data(youxi_dictionary, set(correct_name_data(corrections_name, youxi_lines))) + ['\n'] + \
    ["ğŸ­æˆæ›²é¢‘é“,#genre#"] + sort_data(xiqu_dictionary, set(correct_name_data(corrections_name, xiqu_lines))) + ['\n'] + \
    ["ğŸµéŸ³ä¹é¢‘é“,#genre#"] + sort_data(yinyue_dictionary, set(correct_name_data(corrections_name, yinyue_lines))) + ['\n'] + \
    ["ğŸ‰æ˜¥æ™šé¢‘é“,#genre#"] + sort_data(chunwan_dictionary, set(correct_name_data(corrections_name, chunwan_lines))) + ['\n'] + \
    ["ğŸ“¹ç›´æ’­ä¸­å›½,#genre#"] + sort_data(zhibozhongguo_dictionary, set(correct_name_data(corrections_name, zhibozhongguo_lines))) + ['\n'] + \
    ["ğŸ•’æ›´æ–°æ—¶é—´,#genre#"] + [version] + [about] + [daily_mtv] + [daily_mtv1] + [daily_mtv2] + [daily_mtv3] + [daily_mtv4] + read_txt_to_array('scripts/livesource/æ‰‹å·¥åŒº/about.txt') + ['\n']

# ç˜¦èº«ç‰ˆ
all_lines_lite = ["å¤®è§†é¢‘é“,#genre#"] + sort_data(yangshi_dictionary, correct_name_data(corrections_name, yangshi_lines)) + ['\n'] + \
    ["å«è§†é¢‘é“,#genre#"] + sort_data(weishi_dictionary, correct_name_data(corrections_name, weishi_lines)) + ['\n'] + \
    ["åœ°æ–¹é¢‘é“,#genre#"] + \
    sort_data(hubei_dictionary, set(correct_name_data(corrections_name, hubei_lines))) + \
    sort_data(hunan_dictionary, set(correct_name_data(corrections_name, hunan_lines))) + \
    sort_data(zhejiang_dictionary, set(correct_name_data(corrections_name, zhejiang_lines))) + \
    sort_data(guangdong_dictionary, set(correct_name_data(corrections_name, guangdong_lines))) + \
    sort_data(shandong_dictionary, set(correct_name_data(corrections_name, shandong_lines))) + \
    sorted(set(correct_name_data(corrections_name, jiangsu_lines))) + \
    sorted(set(correct_name_data(corrections_name, anhui_lines))) + \
    sorted(set(correct_name_data(corrections_name, hainan_lines))) + \
    sorted(set(correct_name_data(corrections_name, neimenggu_lines))) + \
    sorted(set(correct_name_data(corrections_name, liaoning_lines))) + \
    sorted(set(correct_name_data(corrections_name, shanxi1_lines))) + \
    sorted(set(correct_name_data(corrections_name, shanxi2_lines))) + \
    sorted(set(correct_name_data(corrections_name, yunnan_lines))) + \
    sorted(set(correct_name_data(corrections_name, beijing_lines))) + \
    sorted(set(correct_name_data(corrections_name, chongqing_lines))) + \
    sorted(set(correct_name_data(corrections_name, fujian_lines))) + \
    sorted(set(correct_name_data(corrections_name, gansu_lines))) + \
    sorted(set(correct_name_data(corrections_name, guangxi_lines))) + \
    sorted(set(correct_name_data(corrections_name, guizhou_lines))) + \
    sorted(set(correct_name_data(corrections_name, hebei_lines))) + \
    sorted(set(correct_name_data(corrections_name, henan_lines))) + \
    sorted(set(correct_name_data(corrections_name, jilin_lines))) + \
    sorted(set(correct_name_data(corrections_name, jiangxi_lines))) + \
    sorted(set(correct_name_data(corrections_name, ningxia_lines))) + \
    sorted(set(correct_name_data(corrections_name, qinghai_lines))) + \
    sorted(set(correct_name_data(corrections_name, sichuan_lines))) + \
    sorted(set(correct_name_data(corrections_name, tianjin_lines))) + \
    sorted(set(correct_name_data(corrections_name, xinjiang_lines))) + \
    sorted(set(correct_name_data(corrections_name, heilongjiang_lines))) + \
    ['\n'] + \
    ["æ›´æ–°æ—¶é—´,#genre#"] + [version] + ['\n']

# customå®šåˆ¶
all_lines_custom = ["ğŸŒå¤®è§†é¢‘é“,#genre#"] + sort_data(yangshi_dictionary, correct_name_data(corrections_name, yangshi_lines)) + ['\n'] + \
    ["ğŸ“¡å«è§†é¢‘é“,#genre#"] + sort_data(weishi_dictionary, correct_name_data(corrections_name, weishi_lines)) + ['\n'] + \
    ["ğŸ åœ°æ–¹é¢‘é“,#genre#"] + \
    sort_data(hubei_dictionary, set(correct_name_data(corrections_name, hubei_lines))) + \
    sort_data(hunan_dictionary, set(correct_name_data(corrections_name, hunan_lines))) + \
    sort_data(zhejiang_dictionary, set(correct_name_data(corrections_name, zhejiang_lines))) + \
    sort_data(guangdong_dictionary, set(correct_name_data(corrections_name, guangdong_lines))) + \
    sort_data(shandong_dictionary, set(correct_name_data(corrections_name, shandong_lines))) + \
    sorted(set(correct_name_data(corrections_name, jiangsu_lines))) + \
    sorted(set(correct_name_data(corrections_name, anhui_lines))) + \
    sorted(set(correct_name_data(corrections_name, hainan_lines))) + \
    sorted(set(correct_name_data(corrections_name, neimenggu_lines))) + \
    sorted(set(correct_name_data(corrections_name, liaoning_lines))) + \
    sorted(set(correct_name_data(corrections_name, shanxi1_lines))) + \
    sorted(set(correct_name_data(corrections_name, shanxi2_lines))) + \
    sorted(set(correct_name_data(corrections_name, yunnan_lines))) + \
    sorted(set(correct_name_data(corrections_name, beijing_lines))) + \
    sorted(set(correct_name_data(corrections_name, chongqing_lines))) + \
    sorted(set(correct_name_data(corrections_name, fujian_lines))) + \
    sorted(set(correct_name_data(corrections_name, gansu_lines))) + \
    sorted(set(correct_name_data(corrections_name, guangxi_lines))) + \
    sorted(set(correct_name_data(corrections_name, guizhou_lines))) + \
    sorted(set(correct_name_data(corrections_name, hebei_lines))) + \
    sorted(set(correct_name_data(corrections_name, henan_lines))) + \
    sorted(set(correct_name_data(corrections_name, jilin_lines))) + \
    sorted(set(correct_name_data(corrections_name, jiangxi_lines))) + \
    sorted(set(correct_name_data(corrections_name, ningxia_lines))) + \
    sorted(set(correct_name_data(corrections_name, qinghai_lines))) + \
    sorted(set(correct_name_data(corrections_name, sichuan_lines))) + \
    sorted(set(correct_name_data(corrections_name, tianjin_lines))) + \
    sorted(set(correct_name_data(corrections_name, xinjiang_lines))) + \
    sorted(set(correct_name_data(corrections_name, heilongjiang_lines))) + \
    ['\n'] + \
    ["ğŸ”¢æ•°å­—é¢‘é“,#genre#"] + sort_data(shuzi_dictionary, set(correct_name_data(corrections_name, shuzi_lines))) + ['\n'] + \
    ["ğŸŒå›½é™…é¢‘é“,#genre#"] + sort_data(guoji_dictionary, set(correct_name_data(corrections_name, guoji_lines))) + ['\n'] + \
    ["âš½ä½“è‚²é¢‘é“,#genre#"] + sort_data(tiyu_dictionary, set(correct_name_data(corrections_name, tiyu_lines))) + ['\n'] + \
    ["ğŸ†ä½“è‚²èµ›äº‹,#genre#"] + normalized_tiyusaishi_lines + ['\n'] + \
    ["ğŸ¬æ–—é±¼ç›´æ’­,#genre#"] + sort_data(douyu_dictionary, set(correct_name_data(corrections_name, douyu_lines))) + ['\n'] + \
    ["ğŸ¯è™ç‰™ç›´æ’­,#genre#"] + sort_data(huya_dictionary, set(correct_name_data(corrections_name, huya_lines))) + ['\n'] + \
    ["ğŸ™ï¸è§£è¯´é¢‘é“,#genre#"] + sort_data(jieshuo_dictionary, set(correct_name_data(corrections_name, jieshuo_lines))) + ['\n'] + \
    ["ğŸ¬ç”µå½±é¢‘é“,#genre#"] + sort_data(dianying_dictionary, set(correct_name_data(corrections_name, dianying_lines))) + ['\n'] + \
    ["ğŸ“ºç”µè§†å‰§,#genre#"] + sort_data(dianshiju_dictionary, set(correct_name_data(corrections_name, dianshiju_lines))) + ['\n'] + \
    ["ğŸï¸è®°å½•ç‰‡,#genre#"] + sort_data(jilupian_dictionary, set(correct_name_data(corrections_name, jilupian_lines))) + ['\n'] + \
    ["ğŸ¶åŠ¨ç”»ç‰‡,#genre#"] + sort_data(donghuapian_dictionary, set(correct_name_data(corrections_name, donghuapian_lines))) + ['\n'] + \
    ["ğŸ“»æ”¶éŸ³æœº,#genre#"] + sort_data(radio_dictionary, set(correct_name_data(corrections_name, radio_lines))) + ['\n'] + \
    ["ğŸ‡¨ğŸ‡³æ¸¯æ¾³å°,#genre#"] + sort_data(gangaotai_dictionary, set(correct_name_data(corrections_name, gangaotai_lines))) + ['\n'] + \
    ["ğŸ‡­ğŸ‡°é¦™æ¸¯é¢‘é“,#genre#"] + sort_data(xianggang_dictionary, set(correct_name_data(corrections_name, xianggang_lines))) + ['\n'] + \
    ["ğŸ‡²ğŸ‡´æ¾³é—¨é¢‘é“,#genre#"] + sort_data(aomen_dictionary, set(correct_name_data(corrections_name, aomen_lines))) + ['\n'] + \
    ["ğŸ­æˆæ›²é¢‘é“,#genre#"] + sort_data(xiqu_dictionary, set(correct_name_data(corrections_name, xiqu_lines))) + ['\n'] + \
    ["ğŸµéŸ³ä¹é¢‘é“,#genre#"] + sort_data(yinyue_dictionary, set(correct_name_data(corrections_name, yinyue_lines))) + ['\n'] + \
    ["ğŸ­ç»¼è‰ºé¢‘é“,#genre#"] + sorted(set(correct_name_data(corrections_name, zongyi_lines))) + ['\n'] + \
    ["ğŸ®æ¸¸æˆé¢‘é“,#genre#"] + sorted(set(correct_name_data(corrections_name, youxi_lines))) + ['\n'] + \
    ["ğŸ“¹ç›´æ’­ä¸­å›½,#genre#"] + sort_data(zhibozhongguo_dictionary, set(correct_name_data(corrections_name, zhibozhongguo_lines))) + ['\n'] + \
    ["ğŸ‰æ˜¥æ™šé¢‘é“,#genre#"] + sort_data(chunwan_dictionary, set(correct_name_data(corrections_name, chunwan_lines))) + ['\n'] + \
    ["ğŸ•’æ›´æ–°æ—¶é—´,#genre#"] + [version] + [about] + [daily_mtv] + [daily_mtv1] + [daily_mtv2] + [daily_mtv3] + [daily_mtv4] + read_txt_to_array('scripts/livesource/æ‰‹å·¥åŒº/about.txt') + ['\n']

print("å¯¹æœ€ç»ˆè¾“å‡ºè¿›è¡Œå»é‡...")
all_lines_full = final_deduplicate_lines(all_lines_full)
all_lines_lite = final_deduplicate_lines(all_lines_lite)
all_lines_custom = final_deduplicate_lines(all_lines_custom)
other_lines = final_deduplicate_lines(other_lines)

# å°†åˆå¹¶åçš„æ–‡æœ¬å†™å…¥æ–‡ä»¶
output_full = "output/livesource/full.txt"
output_lite = "output/livesource/lite.txt"
output_custom = "output/livesource/custom.txt"
output_other = "output/livesource/other.txt"

try:
    # å®Œæ•´ç‰ˆ
    with open(output_full, 'w', encoding='utf-8') as f:
        for line in all_lines_full:
            f.write(line + '\n')
    print(f"å®Œæ•´ç‰ˆå·²ä¿å­˜åˆ°æ–‡ä»¶: {output_full}")

    # ç²¾ç®€ç‰ˆ
    with open(output_lite, 'w', encoding='utf-8') as f:
        for line in all_lines_lite:
            f.write(line + '\n')
    print(f"ç²¾ç®€ç‰ˆå·²ä¿å­˜åˆ°æ–‡ä»¶: {output_lite}")

    # å®šåˆ¶ç‰ˆ
    with open(output_custom, 'w', encoding='utf-8') as f:
        for line in all_lines_custom:
            f.write(line + '\n')
    print(f"å®šåˆ¶ç‰ˆå·²ä¿å­˜åˆ°æ–‡ä»¶: {output_custom}")

    # å…¶å®ƒæº
    with open(output_other, 'w', encoding='utf-8') as f:
        f.write("å…¶å®ƒé¢‘é“,#genre#\n")
        channel_count = 0
        for line in other_lines:
            if "," in line and "://" in line:  # åªç»Ÿè®¡çœŸæ­£çš„é¢‘é“è¡Œ
                f.write(line + '\n')
                channel_count += 1
            else:
                f.write(line + '\n')
        # å¯é€‰ï¼šåœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        f.write(f"\n# å…¶å®ƒé¢‘é“æ€»è®¡: {channel_count} ä¸ªé¢‘é“\n")
    print(f"å…¶å®ƒæºå·²ä¿å­˜åˆ°æ–‡ä»¶: {output_other}")

except Exception as e:
    print(f"ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

########## æ·»åŠ ç”Ÿæˆm3uæ–‡ä»¶
channels_logos=read_txt_to_array('scripts/livesource/logo.txt') #è¯»å…¥logoåº“
def get_logo_by_channel_name(channel_name):
    
    # éå†æ•°ç»„æŸ¥æ‰¾é¢‘é“åç§°
    for line in channels_logos:
        # å»é™¤é¦–å°¾ç©ºç™½å¹¶æ£€æŸ¥æ˜¯å¦ä¸ºç©ºè¡Œ(æ²¡æœ‰è¿™ä¸ªåˆ¤æ–­logoä¸­å¦‚æœå‡ºç°ç©ºè¡Œä¼šå‡ºé”™)
        if not line.strip():
            continue
        name, url = line.split(',')
        if name == channel_name:
            return url
    return None

def make_m3u(txt_file, m3u_file):
    try:
        output_text = '#EXTM3U x-tvg-url="https://live.fanmingming.cn/e.xml"\n'

        with open(txt_file, "r", encoding='utf-8') as file:
            input_text = file.read()

        lines = input_text.strip().split("\n")
        group_name = ""
        for line in lines:
            parts = line.split(",")
            if len(parts) == 2 and "#genre#" in line:
                group_name = parts[0]
            elif len(parts) == 2:
                channel_name = parts[0]
                channel_url = parts[1]
                logo_url=get_logo_by_channel_name(channel_name)
                if logo_url is None:  #not found logo
                    output_text += f"#EXTINF:-1 group-title=\"{group_name}\",{channel_name}\n"
                    output_text += f"{channel_url}\n"
                else:
                    output_text += f"#EXTINF:-1  tvg-name=\"{channel_name}\" tvg-logo=\"{logo_url}\"  group-title=\"{group_name}\",{channel_name}\n"
                    output_text += f"{channel_url}\n"

        with open(f"{m3u_file}", "w", encoding='utf-8') as file:
            file.write(output_text)

        print(f"M3Uæ–‡ä»¶ '{m3u_file}' ç”ŸæˆæˆåŠŸã€‚")
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")

make_m3u(output_full, output_full.replace(".txt", ".m3u"))
make_m3u(output_lite, output_lite.replace(".txt", ".m3u"))
make_m3u(output_custom, output_custom.replace(".txt", ".m3u"))

# æ‰§è¡Œç»“æŸæ—¶é—´
timeend = datetime.now()

# è®¡ç®—æ—¶é—´å·®
elapsed_time = timeend - timestart
total_seconds = elapsed_time.total_seconds()

# è½¬æ¢ä¸ºåˆ†é’Ÿå’Œç§’
minutes = int(total_seconds // 60)
seconds = int(total_seconds % 60)
# æ ¼å¼åŒ–å¼€å§‹å’Œç»“æŸæ—¶é—´
timestart_str = timestart.strftime("%Y%m%d_%H_%M_%S")
timeend_str = timeend.strftime("%Y%m%d_%H_%M_%S")

print(f"å¼€å§‹æ—¶é—´: {timestart_str}")
print(f"ç»“æŸæ—¶é—´: {timeend_str}")
print(f"æ‰§è¡Œæ—¶é—´: {minutes} åˆ† {seconds} ç§’")

# è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
print("\n" + "="*50)
print("ğŸ“Š è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯")
print("="*50)

# è®¡ç®—å®é™…é¢‘é“æ•°é‡ï¼ˆæ’é™¤åˆ†ç±»æ ‡é¢˜å’Œç©ºè¡Œï¼‰
def count_actual_channels(lines):
    count = 0
    for line in lines:
        if line and "#genre#" not in line and line != '\n' and "," in line and "://" in line:
            count += 1
    return count

# ä¸»é¢‘é“ç»Ÿè®¡
yangshi_count = count_actual_channels(yangshi_lines)
weishi_count = count_actual_channels(weishi_lines)

# åœ°æ–¹å°ç»Ÿè®¡
local_counts = {
    "åŒ—äº¬": count_actual_channels(beijing_lines),
    "ä¸Šæµ·": count_actual_channels(shanghai_lines),
    "å¤©æ´¥": count_actual_channels(tianjin_lines),
    "é‡åº†": count_actual_channels(chongqing_lines),
    "å¹¿ä¸œ": count_actual_channels(guangdong_lines),
    "æ±Ÿè‹": count_actual_channels(jiangsu_lines),
    "æµ™æ±Ÿ": count_actual_channels(zhejiang_lines),
    "å±±ä¸œ": count_actual_channels(shandong_lines),
    "æ²³å—": count_actual_channels(henan_lines),
    "å››å·": count_actual_channels(sichuan_lines),
    "æ²³åŒ—": count_actual_channels(hebei_lines),
    "æ¹–å—": count_actual_channels(hunan_lines),
    "æ¹–åŒ—": count_actual_channels(hubei_lines),
    "å®‰å¾½": count_actual_channels(anhui_lines),
    "ç¦å»º": count_actual_channels(fujian_lines),
    "é™•è¥¿": count_actual_channels(shanxi1_lines),
    "è¾½å®": count_actual_channels(liaoning_lines),
    "æ±Ÿè¥¿": count_actual_channels(jiangxi_lines),
    "é»‘é¾™æ±Ÿ": count_actual_channels(heilongjiang_lines),
    "å‰æ—": count_actual_channels(jilin_lines),
    "å±±è¥¿": count_actual_channels(shanxi2_lines),
    "å¹¿è¥¿": count_actual_channels(guangxi_lines),
    "äº‘å—": count_actual_channels(yunnan_lines),
    "è´µå·": count_actual_channels(guizhou_lines),
    "ç”˜è‚ƒ": count_actual_channels(gansu_lines),
    "å†…è’™": count_actual_channels(neimenggu_lines),
    "æ–°ç–†": count_actual_channels(xinjiang_lines),
    "æµ·å—": count_actual_channels(hainan_lines),
    "å®å¤": count_actual_channels(ningxia_lines),
    "é’æµ·": count_actual_channels(qinghai_lines),
    "è¥¿è—": count_actual_channels(xizang_lines)
}

# å®šåˆ¶é¢‘é“ç»Ÿè®¡
custom_counts = {
    "æ–°é—»": count_actual_channels(news_lines),
    "æ•°å­—": count_actual_channels(shuzi_lines),
    "ç”µå½±": count_actual_channels(dianying_lines),
    "è§£è¯´": count_actual_channels(jieshuo_lines),
    "ç»¼è‰º": count_actual_channels(zongyi_lines),
    "è™ç‰™": count_actual_channels(huya_lines),
    "æ–—é±¼": count_actual_channels(douyu_lines),
    "é¦™æ¸¯": count_actual_channels(xianggang_lines),
    "æ¾³é—¨": count_actual_channels(aomen_lines),
    "ä¸­å›½": count_actual_channels(china_lines),
    "å›½é™…": count_actual_channels(guoji_lines),
    "æ¸¯æ¾³å°": count_actual_channels(gangaotai_lines),
    "ç”µè§†å‰§": count_actual_channels(dianshiju_lines),
    "æ”¶éŸ³æœº": count_actual_channels(radio_lines),
    "åŠ¨ç”»ç‰‡": count_actual_channels(donghuapian_lines),
    "è®°å½•ç‰‡": count_actual_channels(jilupian_lines),
    "ä½“è‚²": count_actual_channels(tiyu_lines),
    "ä½“è‚²èµ›äº‹": count_actual_channels(tiyusaishi_lines),
    "æ¸¸æˆ": count_actual_channels(youxi_lines),
    "æˆæ›²": count_actual_channels(xiqu_lines),
    "éŸ³ä¹": count_actual_channels(yinyue_lines),
    "æ˜¥æ™š": count_actual_channels(chunwan_lines),
    "ç›´æ’­ä¸­å›½": count_actual_channels(zhibozhongguo_lines)
}

# è®¡ç®—æ€»æ•°
total_local_channels = sum(local_counts.values())
total_custom_channels = sum(custom_counts.values())
total_channels = yangshi_count + weishi_count + total_local_channels + total_custom_channels
other_channels_count = count_actual_channels(other_lines)

# ç‰ˆæœ¬ç»Ÿè®¡
full_channels_count = count_actual_channels(all_lines_full)
lite_channels_count = count_actual_channels(all_lines_lite)
custom_channels_count = count_actual_channels(all_lines_custom)

print(f"ğŸ”§ é»‘åå•æ•°é‡: {len(combined_blacklist)}")
print(f"ğŸ“º æ€»é¢‘é“æ•°é‡: {total_channels}")
print(f"ğŸ“‹ å…¶å®ƒæºæ•°é‡: {other_channels_count}")
print()

print("ğŸ“ˆ ç‰ˆæœ¬ç»Ÿè®¡:")
print(f"  âœ… å®Œæ•´ç‰ˆé¢‘é“æ•°: {full_channels_count}")
print(f"  ğŸ”¸ ç²¾ç®€ç‰ˆé¢‘é“æ•°: {lite_channels_count}")
print(f"  ğŸ¯ å®šåˆ¶ç‰ˆé¢‘é“æ•°: {custom_channels_count}")
print()

print("ğŸ  ä¸»é¢‘é“ç»Ÿè®¡:")
print(f"  ğŸ“¡ å¤®è§†é¢‘é“: {yangshi_count}")
print(f"  ğŸŒŸ å«è§†é¢‘é“: {weishi_count}")
print()

print("ğŸ“ åœ°æ–¹å°ç»Ÿè®¡ (å‰10):")
# æŒ‰æ•°é‡æ’åºå¹¶æ˜¾ç¤ºå‰10
sorted_local = sorted(local_counts.items(), key=lambda x: x[1], reverse=True)[:10]
for region, count in sorted_local:
    if count > 0:
        print(f"  {region}: {count}")
print(f"  ... å…¶å®ƒ {len(local_counts) - 10} ä¸ªåœ°åŒº")
print(f"  ğŸ“Š åœ°æ–¹å°æ€»æ•°: {total_local_channels}")

print()
print("ğŸ­ å®šåˆ¶é¢‘é“ç»Ÿè®¡:")
# æŒ‰æ•°é‡æ’åºå¹¶æ˜¾ç¤ºå‰10
sorted_custom = sorted(custom_counts.items(), key=lambda x: x[1], reverse=True)[:10]
for category, count in sorted_custom:
    if count > 0:
        print(f"  {category}: {count}")
print(f"  ... å…¶å®ƒ {len(custom_counts) - 10} ä¸ªåˆ†ç±»")
print(f"  ğŸ“Š å®šåˆ¶é¢‘é“æ€»æ•°: {total_custom_channels}")

print("\n" + "="*50)
print("ğŸ‰ å¤„ç†å®Œæˆ!")
print("="*50)