"""
ç›´æ’­æºèšåˆå¤„ç†è„šæœ¬-å…‹éš†Bç‰ˆé€»è¾‘
åŠŸèƒ½ï¼šä»å¤šä¸ªæ¥æºè·å–ç›´æ’­æºï¼Œè¿›è¡Œåˆ†ç±»ã€è¿‡æ»¤ã€æ ¼å¼è½¬æ¢ï¼Œç”Ÿæˆæ’­æ”¾åˆ—è¡¨
ä½œè€…ï¼šåŸºäºAç‰ˆé€»è¾‘å…‹éš†
ç‰ˆæœ¬ï¼š2025
"""

import urllib.request
from urllib.parse import urlparse
import re
import os
from datetime import datetime, timedelta, timezone
import random
import opencc
import socket
import time

# ======= å·¥å…·å‡½æ•°æ¨¡å— =======

def traditional_to_simplified(text: str) -> str:
    """ç¹ä½“è½¬ç®€ä½“"""
    converter = opencc.OpenCC('t2s')
    return converter.convert(text)

def read_txt_to_array(file_name):
    """è¯»å–æ–‡æœ¬æ–‡ä»¶åˆ°æ•°ç»„ï¼Œè·³è¿‡ç©ºè¡Œ"""
    try:
        with open(file_name, 'r', encoding='utf-8') as file:
            lines = file.readlines()
            return [line.strip() for line in lines if line.strip()]
    except FileNotFoundError:
        print(f"File '{file_name}' not found.")
        return []
    except Exception as e:
        print(f"An error occurred: {e}")
        return []

def read_blacklist_from_txt(file_path):
    """ä»é»‘åå•æ–‡ä»¶è¯»å–URLåˆ—è¡¨"""
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    return [line.split(',')[1].strip() for line in lines if ',' in line]

# ======= é…ç½®å’Œåˆå§‹åŒ– =======

timestart = datetime.now()
print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y%m%d_%H_%M_%S')}")

# è¯»å–é»‘åå•
blacklist_auto = read_blacklist_from_txt('scripts/livesource0/blacklist/blacklist_auto.txt')
blacklist_manual = read_blacklist_from_txt('scripts/livesource0/blacklist/blacklist_manual.txt')
combined_blacklist = set(blacklist_auto + blacklist_manual)

# ======= é¢‘é“åˆ†ç±»å­˜å‚¨å¯¹è±¡ =======

# å®šä¹‰å„åˆ†ç±»é¢‘é“çš„å­˜å‚¨åˆ—è¡¨
yangshi_lines = []      # å¤®è§†
weishi_lines = []       # å«è§†
beijing_lines = []      # åŒ—äº¬
shanghai_lines = []     # ä¸Šæµ·
tianjin_lines = []      # å¤©æ´¥
chongqing_lines = []    # é‡åº†
guangdong_lines = []    # å¹¿ä¸œ
jiangsu_lines = []      # æ±Ÿè‹
zhejiang_lines = []     # æµ™æ±Ÿ
shandong_lines = []     # å±±ä¸œ
henan_lines = []        # æ²³å—
sichuan_lines = []      # å››å·
hebei_lines = []        # æ²³åŒ—
hunan_lines = []        # æ¹–å—
hubei_lines = []        # æ¹–åŒ—
anhui_lines = []        # å®‰å¾½
fujian_lines = []       # ç¦å»º
shanxi1_lines = []      # é™•è¥¿
liaoning_lines = []     # è¾½å®
jiangxi_lines = []      # æ±Ÿè¥¿
heilongjiang_lines = [] # é»‘é¾™æ±Ÿ
jilin_lines = []        # å‰æ—
shanxi2_lines = []      # å±±è¥¿
guangxi_lines = []      # å¹¿è¥¿
yunnan_lines = []       # äº‘å—
guizhou_lines = []      # è´µå·
gansu_lines = []        # ç”˜è‚ƒ
neimenggu_lines = []    # å†…è’™å¤
xinjiang_lines = []     # æ–°ç–†
hainan_lines = []       # æµ·å—
ningxia_lines = []      # å®å¤
qinghai_lines = []      # é’æµ·
xizang_lines = []       # è¥¿è—

news_lines = []         # æ–°é—»
shuzi_lines = []        # æ•°å­—
dianying_lines = []     # ç”µå½±
jieshuo_lines = []      # è§£è¯´
zongyi_lines = []       # ç»¼è‰º
huya_lines = []         # è™ç‰™
douyu_lines = []        # æ–—é±¼
xianggang_lines = []    # é¦™æ¸¯
aomen_lines = []        # æ¾³é—¨
china_lines = []        # ä¸­å›½
guoji_lines = []        # å›½é™…
gangaotai_lines = []    # æ¸¯æ¾³å°
dianshiju_lines = []    # ç”µè§†å‰§
radio_lines = []        # æ”¶éŸ³æœº
donghuapian_lines = []  # åŠ¨ç”»ç‰‡
jilupian_lines = []       # çºªå½•ç‰‡
tiyu_lines = []         # ä½“è‚²
youxi_lines = []        # æ¸¸æˆ
xiqu_lines = []         # æˆæ›²
yinyue_lines = []       # éŸ³ä¹
chunwan_lines = []      # æ˜¥æ™š
tyss_lines = []          # ä½“è‚²èµ›äº‹
mgss_lines = []         # å’ªå’•èµ›äº‹
zhibozhongguo_lines = [] # ç›´æ’­ä¸­å›½

other_lines = []        # å…¶ä»–
other_lines_url = []    # å…¶ä»–é¢‘é“URLï¼ˆç”¨äºå»é‡ï¼‰

# ======= é¢‘é“åç§°å¤„ç†å‡½æ•° =======

def process_name_string(input_str):
    """å¤„ç†é¢‘é“åç§°å­—ç¬¦ä¸²"""
    parts = input_str.split(',')
    processed_parts = []
    for part in parts:
        processed_part = process_part(part)
        processed_parts.append(processed_part)
    result_str = ','.join(processed_parts)
    return result_str

def process_part(part_str):
    """å¤„ç†å•ä¸ªé¢‘é“åç§°éƒ¨åˆ†"""
    # å¤„ç†CCTVé¢‘é“åç§°
    if "CCTV" in part_str and "://" not in part_str:
        part_str = part_str.replace("IPV6", "").replace("PLUS", "+").replace("1080", "")
        filtered_str = ''.join(char for char in part_str if char.isdigit() or char == 'K' or char == '+')
        
        # å¤„ç†ç‰¹æ®Šæƒ…å†µï¼šæ²¡æœ‰æ‰¾åˆ°é¢‘é“æ•°å­—
        if not filtered_str.strip():
            filtered_str = part_str.replace("CCTV", "")

        # å¤„ç†4K/8Kç‰¹æ®Šæ ¼å¼
        if len(filtered_str) > 2 and re.search(r'4K|8K', filtered_str):
            filtered_str = re.sub(r'(4K|8K).*', r'\1', filtered_str)
            if len(filtered_str) > 2: 
                filtered_str = re.sub(r'(4K|8K)', r'(\1)', filtered_str)

        return "CCTV" + filtered_str 
        
    elif "å«è§†" in part_str:
        # æ¸…ç†å«è§†é¢‘é“åç§°ä¸­çš„é™„åŠ ä¿¡æ¯
        pattern = r'å«è§†ã€Œ.*ã€'
        result_str = re.sub(pattern, 'å«è§†', part_str)
        return result_str
    
    return part_str

# ======= æ–‡ä»¶æ ¼å¼å¤„ç† =======

def get_url_file_extension(url):
    """è·å–URLæ–‡ä»¶æ‰©å±•å"""
    parsed_url = urlparse(url)
    path = parsed_url.path
    extension = os.path.splitext(path)[1]
    return extension

def convert_m3u_to_txt(m3u_content):
    """å°†M3Uæ ¼å¼è½¬æ¢ä¸ºTXTæ ¼å¼"""
    lines = m3u_content.split('\n')
    txt_lines = []
    channel_name = ""
    
    for line in lines:
        # è¿‡æ»¤M3Uå¤´ä¿¡æ¯
        if line.startswith("#EXTM3U"):
            continue
        # å¤„ç†é¢‘é“ä¿¡æ¯è¡Œ
        if line.startswith("#EXTINF"):
            channel_name = line.split(',')[-1].strip()
        # å¤„ç†URLè¡Œ
        elif line.startswith("http") or line.startswith("rtmp") or line.startswith("p3p"):
            txt_lines.append(f"{channel_name},{line.strip()}")
        
        # å¤„ç†æ ¼å¼ä¸ºtxtä½†åç¼€ä¸ºm3uçš„æ–‡ä»¶
        if "#genre#" not in line and "," in line and "://" in line:
            pattern = r'^[^,]+,[^\s]+://[^\s]+$'
            if bool(re.match(pattern, line)):
                txt_lines.append(line)
    
    return '\n'.join(txt_lines)

# ======= URLå¤„ç†å’ŒéªŒè¯ =======

def check_url_existence(data_list, url):
    """æ£€æŸ¥URLæ˜¯å¦åœ¨åˆ—è¡¨ä¸­å·²å­˜åœ¨"""
    urls = [item.split(',')[1] for item in data_list]
    return url not in urls  # å¦‚æœä¸å­˜åœ¨è¿”å›True

def clean_url(url):
    """æ¸…ç†URLï¼Œç§»é™¤$ç¬¦å·åçš„å†…å®¹"""
    last_dollar_index = url.rfind('$')
    if last_dollar_index != -1:
        return url[:last_dollar_index]
    return url

# ======= é¢‘é“åç§°æ¸…ç† =======

# éœ€è¦ä»é¢‘é“åç§°ä¸­ç§»é™¤çš„å­—ç¬¦åˆ—è¡¨
removal_list = ["_ç”µä¿¡", "ç”µä¿¡", "é«˜æ¸…", "é¢‘é“", "ï¼ˆHDï¼‰", "-HD", "è‹±é™†", "_ITV", "(åŒ—ç¾)", "(HK)", "AKtv", "ã€ŒIPV4ã€", "ã€ŒIPV6ã€", "[HD]", "[BD]", "[SD]", "[VGA]",
                "é¢‘é™†", "å¤‡é™†", "å£¹é™†", "è´°é™†", "åé™†", "è‚†é™†", "ä¼é™†", "é™†é™†", "æŸ’é™†", "é¢‘æ™´", "é¢‘ç²¤", "[è¶…æ¸…]", "é«˜æ¸…", "è¶…æ¸…", "æ ‡æ¸…", "æ–¯ç‰¹",
                "ç²¤é™†", "å›½é™†", "è‚†æŸ’", "é¢‘è‹±", "é¢‘ç‰¹", "é¢‘å›½", "é¢‘å£¹", "é¢‘è´°", "è‚†è´°", "é¢‘æµ‹", "å’ªå’•", "é—½ç‰¹", "é«˜ç‰¹", "é¢‘é«˜", "é¢‘æ ‡", "æ±é˜³",
                "4Gtv", "é¢‘æ•ˆ", "å›½æ ‡", "ç²¤æ ‡", "é¢‘æ¨", "é¢‘æµ", "ç²¤é«˜", "é¢‘é™", "å®æ—¶", "ç¾æ¨", "é¢‘ç¾"]

def clean_channel_name(channel_name, removal_list):
    """æ¸…ç†é¢‘é“åç§°ä¸­çš„ç‰¹å®šå­—ç¬¦"""
    for item in removal_list:
        channel_name = channel_name.replace(item, "")

    # ç§»é™¤æœ«å°¾çš„'HD'å’Œ'å°'
    if channel_name.endswith("HD"):
        channel_name = channel_name[:-2]
    if channel_name.endswith("å°") and len(channel_name) > 3:
        channel_name = channel_name[:-1]

    return channel_name

# ======= æ ¸å¿ƒåˆ†å‘é€»è¾‘ =======

def process_channel_line(line):
    """å¤„ç†å•è¡Œé¢‘é“æ•°æ®å¹¶è¿›è¡Œåˆ†ç±»"""
    # æ£€æŸ¥è¡Œæ ¼å¼æ˜¯å¦ç¬¦åˆè¦æ±‚
    if "#genre#" not in line and "#EXTINF:" not in line and "," in line and "://" in line:
        channel_name = line.split(',')[0].strip()
        channel_name = clean_channel_name(channel_name, removal_list)  # æ¸…ç†åç§°
        channel_name = traditional_to_simplified(channel_name)  # ç¹è½¬ç®€
        channel_address = clean_url(line.split(',')[1].strip())  # æ¸…ç†URL
        line = channel_name + "," + channel_address  # é‡æ–°ç»„ç»‡è¡Œ
        
        # æ£€æŸ¥æ˜¯å¦åœ¨é»‘åå•ä¸­
        if channel_address not in combined_blacklist:
            # æ ¹æ®é¢‘é“åç§°è¿›è¡Œåˆ†ç±»åˆ†å‘
            #if "CCTV" in channel_name and check_url_existence(yangshi_lines, channel_address):  # æ³¨é‡Šè¿™ä¸€è¡Œç”¨ä¸‹é¢æ›¿æ¢
            if any(cctv_name in channel_name for cctv_name in yangshi_dictionary) and check_url_existence(yangshi_lines, channel_address):
                yangshi_lines.append(process_name_string(line.strip()))
            elif channel_name in weishi_dictionary and check_url_existence(weishi_lines, channel_address):
                weishi_lines.append(process_name_string(line.strip()))

            # åœ°æ–¹å°åˆ†å‘é€»è¾‘
            elif channel_name in beijing_dictionary and check_url_existence(beijing_lines, channel_address):
                beijing_lines.append(process_name_string(line.strip()))
            elif channel_name in shanghai_dictionary and check_url_existence(shanghai_lines, channel_address):
                shanghai_lines.append(process_name_string(line.strip()))
            elif channel_name in tianjin_dictionary and check_url_existence(tianjin_lines, channel_address):
                tianjin_lines.append(process_name_string(line.strip()))
            elif channel_name in chongqing_dictionary and check_url_existence(chongqing_lines, channel_address):
                chongqing_lines.append(process_name_string(line.strip()))
            elif channel_name in guangdong_dictionary and check_url_existence(guangdong_lines, channel_address):
                guangdong_lines.append(process_name_string(line.strip()))
            elif channel_name in jiangsu_dictionary and check_url_existence(jiangsu_lines, channel_address):
                jiangsu_lines.append(process_name_string(line.strip()))
            elif channel_name in zhejiang_dictionary and check_url_existence(zhejiang_lines, channel_address):
                zhejiang_lines.append(process_name_string(line.strip()))
            elif channel_name in shandong_dictionary and check_url_existence(shandong_lines, channel_address):
                shandong_lines.append(process_name_string(line.strip()))
            elif channel_name in henan_dictionary and check_url_existence(henan_lines, channel_address):
                henan_lines.append(process_name_string(line.strip()))
            elif channel_name in sichuan_dictionary and check_url_existence(sichuan_lines, channel_address):
                sichuan_lines.append(process_name_string(line.strip()))
            elif channel_name in hebei_dictionary and check_url_existence(hebei_lines, channel_address):
                hebei_lines.append(process_name_string(line.strip()))
            elif channel_name in hunan_dictionary and check_url_existence(hunan_lines, channel_address):
                hunan_lines.append(process_name_string(line.strip()))
            elif channel_name in hubei_dictionary and check_url_existence(hubei_lines, channel_address):
                hubei_lines.append(process_name_string(line.strip()))
            elif channel_name in anhui_dictionary and check_url_existence(anhui_lines, channel_address):
                anhui_lines.append(process_name_string(line.strip()))
            elif channel_name in fujian_dictionary and check_url_existence(fujian_lines, channel_address):
                fujian_lines.append(process_name_string(line.strip()))
            elif channel_name in shanxi1_dictionary and check_url_existence(shanxi1_lines, channel_address):
                shanxi1_lines.append(process_name_string(line.strip()))
            elif channel_name in liaoning_dictionary and check_url_existence(liaoning_lines, channel_address):
                liaoning_lines.append(process_name_string(line.strip()))
            elif channel_name in jiangxi_dictionary and check_url_existence(jiangxi_lines, channel_address):
                jiangxi_lines.append(process_name_string(line.strip()))
            elif channel_name in heilongjiang_dictionary and check_url_existence(heilongjiang_lines, channel_address):
                heilongjiang_lines.append(process_name_string(line.strip()))
            elif channel_name in jilin_dictionary and check_url_existence(jilin_lines, channel_address):
                jilin_lines.append(process_name_string(line.strip()))
            elif channel_name in shanxi2_dictionary and check_url_existence(shanxi2_lines, channel_address):
                shanxi2_lines.append(process_name_string(line.strip()))
            elif channel_name in guangxi_dictionary and check_url_existence(guangxi_lines, channel_address):
                guangxi_lines.append(process_name_string(line.strip()))
            elif channel_name in yunnan_dictionary and check_url_existence(yunnan_lines, channel_address):
                yunnan_lines.append(process_name_string(line.strip()))
            elif channel_name in guizhou_dictionary and check_url_existence(guizhou_lines, channel_address):
                guizhou_lines.append(process_name_string(line.strip()))
            elif channel_name in gansu_dictionary and check_url_existence(gansu_lines, channel_address):
                gansu_lines.append(process_name_string(line.strip()))
            elif channel_name in neimenggu_dictionary and check_url_existence(neimenggu_lines, channel_address):
                neimenggu_lines.append(process_name_string(line.strip()))
            elif channel_name in xinjiang_dictionary and check_url_existence(xinjiang_lines, channel_address):
                xinjiang_lines.append(process_name_string(line.strip()))
            elif channel_name in hainan_dictionary and check_url_existence(hainan_lines, channel_address):
                hainan_lines.append(process_name_string(line.strip()))
            elif channel_name in ningxia_dictionary and check_url_existence(ningxia_lines, channel_address):
                ningxia_lines.append(process_name_string(line.strip()))
            elif channel_name in qinghai_dictionary and check_url_existence(qinghai_lines, channel_address):
                qinghai_lines.append(process_name_string(line.strip()))
            elif channel_name in xizang_dictionary and check_url_existence(xizang_lines, channel_address):
                xizang_lines.append(process_name_string(line.strip()))

            # ä¸»é¢‘é“åˆ†å‘é€»è¾‘
            elif channel_name in news_dictionary and check_url_existence(news_lines, channel_address):
                news_lines.append(process_name_string(line.strip()))
            elif channel_name in shuzi_dictionary and check_url_existence(shuzi_lines, channel_address):
                shuzi_lines.append(process_name_string(line.strip()))
            elif channel_name in dianying_dictionary and check_url_existence(dianying_lines, channel_address):
                dianying_lines.append(process_name_string(line.strip()))
            elif channel_name in jieshuo_dictionary and check_url_existence(jieshuo_lines, channel_address):
                jieshuo_lines.append(process_name_string(line.strip()))
            elif channel_name in zongyi_dictionary and check_url_existence(zongyi_lines, channel_address):
                zongyi_lines.append(process_name_string(line.strip()))
            elif channel_name in huya_dictionary and check_url_existence(huya_lines, channel_address):
                huya_lines.append(process_name_string(line.strip()))
            elif channel_name in douyu_dictionary and check_url_existence(douyu_lines, channel_address):
                douyu_lines.append(process_name_string(line.strip()))
            elif channel_name in xianggang_dictionary and check_url_existence(xianggang_lines, channel_address):
                xianggang_lines.append(process_name_string(line.strip()))
            elif channel_name in aomen_dictionary and check_url_existence(aomen_lines, channel_address):
                aomen_lines.append(process_name_string(line.strip()))
            elif channel_name in china_dictionary and check_url_existence(china_lines, channel_address):
                china_lines.append(process_name_string(line.strip()))
            elif channel_name in guoji_dictionary and check_url_existence(guoji_lines, channel_address):
                guoji_lines.append(process_name_string(line.strip()))
            elif channel_name in gangaotai_dictionary and check_url_existence(gangaotai_lines, channel_address):
                gangaotai_lines.append(process_name_string(line.strip()))
            elif channel_name in dianshiju_dictionary and check_url_existence(dianshiju_lines, channel_address):
                dianshiju_lines.append(process_name_string(line.strip()))
            elif channel_name in radio_dictionary and check_url_existence(radio_lines, channel_address):
                radio_lines.append(process_name_string(line.strip()))
            elif channel_name in donghuapian_dictionary and check_url_existence(donghuapian_lines, channel_address):
                donghuapian_lines.append(process_name_string(line.strip()))
            elif channel_name in jilupian_dictionary and check_url_existence(jilupian_lines, channel_address):
                jilupian_lines.append(process_name_string(line.strip()))
            elif channel_name in tiyu_dictionary and check_url_existence(tiyu_lines, channel_address):
                tiyu_lines.append(process_name_string(line.strip()))
            elif channel_name in youxi_dictionary and check_url_existence(youxi_lines, channel_address):
                youxi_lines.append(process_name_string(line.strip()))
            elif channel_name in xiqu_dictionary and check_url_existence(xiqu_lines, channel_address):
                xiqu_lines.append(process_name_string(line.strip()))
            elif channel_name in yinyue_dictionary and check_url_existence(yinyue_lines, channel_address):
                yinyue_lines.append(process_name_string(line.strip()))
            elif channel_name in chunwan_dictionary and check_url_existence(chunwan_lines, channel_address):
                chunwan_lines.append(process_name_string(line.strip()))
            elif any(tyss_dictionary in channel_name for tyss_dictionary in tyss_dictionary) and check_url_existence(tyss_lines, channel_address):  #ä½“è‚²èµ›äº‹ï¼ˆ2025æ–°å¢ï¼‰
                tyss_lines.append(process_name_string(line.strip()))
            elif any(mgss_dictionary in channel_name for mgss_dictionary in mgss_dictionary) and check_url_existence(mgss_lines, channel_address):  #å’ªå’•èµ›äº‹ï¼ˆ2025æ–°å¢ï¼‰
                mgss_lines.append(process_name_string(line.strip()))
            elif channel_name in zhibozhongguo_dictionary and check_url_existence(zhibozhongguo_lines, channel_address):
                zhibozhongguo_lines.append(process_name_string(line.strip()))
            else:
                # æœªåˆ†ç±»çš„é¢‘é“æ”¾å…¥å…¶ä»–
                if channel_address not in other_lines_url:
                    other_lines_url.append(channel_address)
                    other_lines.append(line.strip())

# ======= ç½‘ç»œè¯·æ±‚ç›¸å…³ =======

def get_random_user_agent():
    """è·å–éšæœºUser-Agent"""
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36",
    ]
    return random.choice(USER_AGENTS)

def get_http_response(url, timeout=8, retries=2, backoff_factor=1.0):
    """å¸¦é‡è¯•æœºåˆ¶çš„HTTPè¯·æ±‚"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
    }
    for attempt in range(retries):
        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=timeout) as response:
                data = response.read()
                return data.decode('utf-8')
        except urllib.error.HTTPError as e:
            print(f"[HTTPError] Code: {e.code}, URL: {url}")
            break  # HTTPé”™è¯¯ä¸ä¼šåœ¨é‡è¯•ä¸­æ¢å¤
        except urllib.error.URLError as e:
            print(f"[URLError] Reason: {e.reason}, Attempt: {attempt + 1}")
        except socket.timeout:
            print(f"[Timeout] URL: {url}, Attempt: {attempt + 1}")
        except Exception as e:
            print(f"[Exception] {type(e).__name__}: {e}, Attempt: {attempt + 1}")
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
        if attempt < retries - 1:
            time.sleep(backoff_factor * (2 ** attempt))
    
    return None  # æ‰€æœ‰å°è¯•å¤±è´¥åè¿”å›None

def process_url(url):
    """å¤„ç†å•ä¸ªURLæº"""
    try:
        other_lines.append("â—†â—†â—†ã€€" + url)  # åœ¨otherä¸­æ ‡è®°å¤„ç†çš„URL
        
        # åˆ›å»ºè¯·æ±‚å¯¹è±¡
        req = urllib.request.Request(url)
        req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3')

        # æ‰“å¼€URLå¹¶è¯»å–å†…å®¹
        with urllib.request.urlopen(req) as response:
            data = response.read()
            text = data.decode('utf-8')
            text = text.strip()

            # å¤„ç†M3Uæ ¼å¼
            is_m3u = text.startswith("#EXTM3U") or text.startswith("#EXTINF")
            if get_url_file_extension(url) == ".m3u" or get_url_file_extension(url) == ".m3u8" or is_m3u:
                text = convert_m3u_to_txt(text)

            # é€è¡Œå¤„ç†å†…å®¹
            lines = text.split('\n')
            print(f"è¡Œæ•°: {len(lines)}")
            for line in lines:
                # è¿‡æ»¤æ— æ•ˆè¡Œï¼šä¸åŒ…å«åˆ†ç±»æ ‡è®°ï¼ŒåŒ…å«é€—å·å’Œåè®®ï¼Œæ’é™¤tvbuså’Œç»„æ’­
                if "#genre#" not in line and "," in line and "://" in line and "tvbus://" not in line and "/udp/" not in line:
                    # æ‹†åˆ†æˆé¢‘é“åå’ŒURLéƒ¨åˆ†
                    channel_name, channel_address = line.split(',', 1)
                    # å¤„ç†åŠ é€Ÿæºï¼ˆåŒ…å«#å·çš„å¤šä¸ªURLï¼‰
                    if "#" not in channel_address:
                        process_channel_line(line)  # æ™®é€šæºç›´æ¥å¤„ç†
                    else: 
                        # åŠ é€ŸæºæŒ‰#åˆ†éš”ååˆ†åˆ«å¤„ç†
                        url_list = channel_address.split('#')
                        for channel_url in url_list:
                            newline = f'{channel_name},{channel_url}'
                            process_channel_line(newline)

            other_lines.append('\n')  # URLå¤„ç†å®Œæˆåˆ†éš”ç¬¦

    except Exception as e:
        print(f"å¤„ç†URLæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

# ======= æ•°æ®æ ¡æ­£å’Œæ’åº =======

def load_corrections_name(filename):
    """åŠ è½½é¢‘é“åç§°æ ¡æ­£å­—å…¸"""
    corrections = {}
    with open(filename, 'r', encoding='utf-8') as f:
        for line in f:
            if not line.strip():  # è·³è¿‡ç©ºè¡Œ
                continue
            parts = line.strip().split(',')
            correct_name = parts[0]
            for name in parts[1:]:
                corrections[name] = correct_name
    return corrections

def correct_name_data(corrections, data):
    """æ ¡æ­£é¢‘é“åç§°æ•°æ®"""
    corrected_data = []
    for line in data:
        line = line.strip()
        if ',' not in line:
            continue  # è¡Œæ ¼å¼é”™è¯¯ï¼šè·³è¿‡
        name, url = line.split(',', 1)
        # å¦‚æœåç§°éœ€è¦æ ¡æ­£ä¸”ä¸ç­‰äºæ­£ç¡®åç§°
        if name in corrections and name != corrections[name]:
            name = corrections[name]
        corrected_data.append(f"{name},{url}")
    return corrected_data

def sort_data(order, data):
    """æŒ‰ç…§æŒ‡å®šé¡ºåºæ’åºæ•°æ®"""
    # åˆ›å»ºé¡ºåºå­—å…¸
    order_dict = {name: i for i, name in enumerate(order)}
    
    # å®šä¹‰æ’åºé”®å‡½æ•°
    def sort_key(line):
        name = line.split(',')[0]
        return order_dict.get(name, len(order))  # ä¸åœ¨å­—å…¸ä¸­çš„æ’åœ¨æœ€å
    
    # æŒ‰ç…§é¡ºåºå¯¹æ•°æ®è¿›è¡Œæ’åº
    sorted_data = sorted(data, key=sort_key)
    return sorted_data

# ======= ä½“è‚²èµ›äº‹ä¸“ç”¨å‡½æ•° =======

def normalize_date_to_md(text):
    """å°†æ—¥æœŸç»Ÿä¸€æ ¼å¼åŒ–ä¸ºMM-DDæ ¼å¼"""
    text = text.strip()

    def format_md(m):
        """æ ¼å¼åŒ–æ—¥æœŸåŒ¹é…ç»„"""
        month = int(m.group(1))
        day = int(m.group(2))
        after = m.group(3) or ''
        # ç¡®ä¿åé¢æœ‰ç©ºæ ¼åˆ†éš”
        if not after.startswith(' '):
            after = ' ' + after
        return f"{month:02d}-{day:02d}{after}"

    # å¤„ç†å„ç§æ—¥æœŸæ ¼å¼
    text = re.sub(r'^0?(\d{1,2})/0?(\d{1,2})(.*)', format_md, text)  # MM/DDæ ¼å¼
    text = re.sub(r'^\d{4}-0?(\d{1,2})-0?(\d{1,2})(.*)', format_md, text)  # YYYY-MM-DDæ ¼å¼
    text = re.sub(r'^0?(\d{1,2})æœˆ0?(\d{1,2})æ—¥(.*)', format_md, text)  # ä¸­æ–‡æ—¥æœŸæ ¼å¼

    return text

def filter_lines(lines, exclude_keywords):
    """
    è¿‡æ»¤æ‰åŒ…å«ä»»ä¸€å…³é”®å­—çš„è¡Œ
    :param lines: åŸå§‹å­—ç¬¦ä¸²æ•°ç»„
    :param exclude_keywords: éœ€è¦å‰”é™¤çš„å…³é”®è¯åˆ—è¡¨
    :return: è¿‡æ»¤åçš„æ–°åˆ—è¡¨
    """
    return [line for line in lines if not any(keyword in line for keyword in exclude_keywords)]

def generate_playlist_html(data_list, output_file='playlist.html'):
    """ç”Ÿæˆä½“è‚²èµ›äº‹HTMLé¡µé¢"""
    html_head = '''
    <!DOCTYPE html>
    <html lang="zh">
    <head>
        <meta charset="UTF-8">        
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6061710286208572"
     crossorigin="anonymous"></script>
        <!-- Setup Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-BS1Z4F5BDN"></script>
        <script> 
        window.dataLayer = window.dataLayer || []; 
        function gtag(){dataLayer.push(arguments);} 
        gtag('js', new Date()); 
        gtag('config', 'G-BS1Z4F5BDN'); 
        </script>
        <title>æœ€æ–°ä½“è‚²èµ›äº‹</title>
        <style>
            body { font-family: sans-serif; padding: 20px; background: #f9f9f9; }
            .item { margin-bottom: 20px; padding: 12px; background: #fff; border-radius: 8px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.06); }
            .title { font-weight: bold; font-size: 1.1em; color: #333; margin-bottom: 5px; }
            .url-wrapper { display: flex; align-items: center; gap: 10px; }
            .url {
                max-width: 80%;
                white-space: nowrap;
                overflow: hidden;
                text-overflow: ellipsis;
                font-size: 0.9em;
                color: #555;
                background: #f0f0f0;
                padding: 6px;
                border-radius: 4px;
                flex-grow: 1;
            }
            .copy-btn {
                background-color: #007BFF;
                border: none;
                color: white;
                padding: 6px 10px;
                border-radius: 4px;
                cursor: pointer;
                font-size: 0.8em;
            }
            .copy-btn:hover {
                background-color: #0056b3;
            }
        </style>
    </head>
    <body>
    <h2>ğŸ“‹ æœ€æ–°ä½“è‚²èµ›äº‹åˆ—è¡¨</h2>
    '''

    html_body = ''
    for idx, entry in enumerate(data_list):
        if ',' not in entry:
            continue
        info, url = entry.split(',', 1)
        url_id = f"url_{idx}"
        html_body += f'''
        <div class="item">
            <div class="title">ğŸ•’ {info}</div>
            <div class="url-wrapper">
                <div class="url" id="{url_id}">{url}</div>
                <button class="copy-btn" onclick="copyToClipboard('{url_id}')">å¤åˆ¶</button>
            </div>
        </div>
        '''

    html_tail = '''
    <script>
        function copyToClipboard(id) {
            const el = document.getElementById(id);
            const text = el.textContent;
            navigator.clipboard.writeText(text).then(() => {
                alert("å·²å¤åˆ¶é“¾æ¥ï¼");
            }).catch(err => {
                alert("å¤åˆ¶å¤±è´¥: " + err);
            });
        }
    </script>
    </body>
    </html>
    '''

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(html_head + html_body + html_tail)
    print(f"âœ… ç½‘é¡µå·²ç”Ÿæˆï¼š{output_file}")

def custom_tyss_sort(lines):
    """ä½“è‚²èµ›äº‹ä¸“ç”¨æ’åºï¼šæ•°å­—å¼€å¤´å€’åºï¼Œå…¶ä»–å‡åº"""
    digit_prefix = []
    others = []

    for line in lines:
        # æ‹†åˆ†å‡ºåç§°éƒ¨åˆ†ç”¨äºåˆ¤æ–­æ˜¯å¦ä»¥æ•°å­—å¼€å¤´
        name_part = line.split(',')[0].strip()
        if name_part and name_part[0].isdigit():
            digit_prefix.append(line)
        else:
            others.append(line)

    # åˆ†åˆ«æ’åºï¼šæ•°å­—å¼€å¤´å€’åºï¼Œå…¶ä»–å‡åº
    digit_prefix_sorted = sorted(digit_prefix, reverse=True)
    others_sorted = sorted(others)

    return digit_prefix_sorted + others_sorted

def get_random_url(file_path):
    """ä»æ–‡ä»¶ä¸­éšæœºè·å–ä¸€ä¸ªURL"""
    urls = []
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            # æŸ¥æ‰¾é€—å·åé¢çš„éƒ¨åˆ†ï¼Œå³URL
            url = line.strip().split(',')[-1]
            urls.append(url)    
    # éšæœºè¿”å›ä¸€ä¸ªURL
    return random.choice(urls) if urls else None

# ======= M3Uæ–‡ä»¶ç”Ÿæˆ =======

def get_logo_by_channel_name(channel_name):
    """æ ¹æ®é¢‘é“åç§°è·å–logo URL"""
    for line in channels_logos:
        if not line.strip():
            continue
        name, url = line.split(',')
        if name == channel_name:
            return url
    return None

def make_m3u(txt_file, m3u_file):
    """å°†TXTæ–‡ä»¶è½¬æ¢ä¸ºM3Uæ ¼å¼"""
    try:
        output_text = '#EXTM3U x-tvg-url="https://live.fanmingming.cn/e.xml"\n'

        with open(txt_file, "r", encoding='utf-8') as file:
            input_text = file.read()

        lines = input_text.strip().split("\n")
        group_name = ""
        for line in lines:
            parts = line.split(",")
            if len(parts) == 2 and "#genre#" in line:
                group_name = parts[0]  # æ›´æ–°åˆ†ç»„åç§°
            elif len(parts) == 2:
                channel_name = parts[0]
                channel_url = parts[1]
                logo_url = get_logo_by_channel_name(channel_name)
                if logo_url is None:  # æœªæ‰¾åˆ°logo
                    output_text += f"#EXTINF:-1 group-title=\"{group_name}\",{channel_name}\n"
                    output_text += f"{channel_url}\n"
                else:
                    output_text += f"#EXTINF:-1  tvg-name=\"{channel_name}\" tvg-logo=\"{logo_url}\"  group-title=\"{group_name}\",{channel_name}\n"
                    output_text += f"{channel_url}\n"

        with open(f"{m3u_file}", "w", encoding='utf-8') as file:
            file.write(output_text)

        print(f"M3Uæ–‡ä»¶ '{m3u_file}' ç”ŸæˆæˆåŠŸã€‚")
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")

# ======= ä¸»æ‰§è¡Œæµç¨‹ =======

# è·å–å½“å‰å·¥ä½œç›®å½•
current_directory = os.getcwd()

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
output_dir = 'output'
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
    print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")

# 1. åˆå§‹åŒ–å­—å…¸æ•°æ®
print("åˆå§‹åŒ–é¢‘é“å­—å…¸...")
yangshi_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/CCTV.txt')
weishi_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/å«è§†.txt')

beijing_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/åŒ—äº¬.txt')
shanghai_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/ä¸Šæµ·.txt')
tianjin_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/å¤©æ´¥.txt')
chongqing_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/é‡åº†.txt')
guangdong_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/å¹¿ä¸œ.txt')
jiangsu_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/æ±Ÿè‹.txt')
zhejiang_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/æµ™æ±Ÿ.txt')
shandong_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/å±±ä¸œ.txt')
henan_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/æ²³å—.txt')
sichuan_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/å››å·.txt')
hebei_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/æ²³åŒ—.txt')
hunan_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/æ¹–å—.txt')
hubei_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/æ¹–åŒ—.txt')
anhui_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/å®‰å¾½.txt')
fujian_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/ç¦å»º.txt')
shanxi1_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/é™•è¥¿.txt')
liaoning_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/è¾½å®.txt')
jiangxi_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/æ±Ÿè¥¿.txt')
heilongjiang_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/é»‘é¾™æ±Ÿ.txt')
jilin_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/å‰æ—.txt')
shanxi2_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/å±±è¥¿.txt')
guangxi_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/å¹¿è¥¿.txt')
yunnan_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/äº‘å—.txt')
guizhou_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/è´µå·.txt')
gansu_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/ç”˜è‚ƒ.txt')
neimenggu_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/å†…è’™.txt')
xinjiang_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/æ–°ç–†.txt')
hainan_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/æµ·å—.txt')
ningxia_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/å®å¤.txt')
qinghai_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/é’æµ·.txt')
xizang_dictionary = read_txt_to_array('scripts/livesource0/åœ°æ–¹å°/è¥¿è—.txt')

news_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/æ–°é—».txt')
shuzi_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/æ•°å­—.txt')
dianying_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/ç”µå½±.txt')
jieshuo_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/è§£è¯´.txt')
zongyi_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/ç»¼è‰º.txt')
huya_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/è™ç‰™.txt')
douyu_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/æ–—é±¼.txt')
xianggang_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/é¦™æ¸¯.txt')
aomen_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/æ¾³é—¨.txt')
china_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/ä¸­å›½.txt')
guoji_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/å›½é™….txt')
gangaotai_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/æ¸¯æ¾³å°.txt')
dianshiju_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/ç”µè§†å‰§.txt')
radio_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/æ”¶éŸ³æœº.txt')
donghuapian_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/åŠ¨ç”»ç‰‡.txt')
jilupian_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/è®°å½•ç‰‡.txt')
tiyu_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/ä½“è‚².txt')
youxi_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/æ¸¸æˆ.txt')
xiqu_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/æˆæ›².txt')
yinyue_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/éŸ³ä¹.txt')
chunwan_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/æ˜¥æ™š.txt')
tyss_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/ä½“è‚²èµ›äº‹.txt')
mgss_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/å’ªå’•èµ›äº‹.txt')
zhibozhongguo_dictionary = read_txt_to_array('scripts/livesource0/ä¸»é¢‘é“/ç›´æ’­ä¸­å›½.txt')

# 2. åŠ è½½åç§°æ ¡æ­£
corrections_name = load_corrections_name('scripts/livesource0/corrections_name.txt')

# 3. å¤„ç†URLæº
print("å¼€å§‹å¤„ç†URLæº...")
urls = read_txt_to_array('scripts/livesource0/urls-daily.txt')
for url in urls:
    if url.startswith("http"):
        # å¤„ç†æ—¥æœŸå˜é‡
        if "{MMdd}" in url:  # ç‰¹åˆ«å¤„ç†113æ ¼å¼
            current_date_str = datetime.now().strftime("%m%d")
            url = url.replace("{MMdd}", current_date_str)
        if "{MMdd-1}" in url:  # ç‰¹åˆ«å¤„ç†113æ ¼å¼ï¼ˆå‰ä¸€å¤©ï¼‰
            yesterday_date_str = (datetime.now() - timedelta(days=1)).strftime("%m%d")
            url = url.replace("{MMdd-1}", yesterday_date_str)
            
        print(f"å¤„ç†URL: {url}")
        process_url(url)

# 4. å¤„ç†ç™½åå•
print(f"ADD whitelist_auto.txt")
whitelist_auto_lines = read_txt_to_array('scripts/livesource0/blacklist/whitelist_auto.txt')
for whitelist_line in whitelist_auto_lines:
    if "#genre#" not in whitelist_line and "," in whitelist_line and "://" in whitelist_line:
        whitelist_parts = whitelist_line.split(",")
        try:
            response_time = float(whitelist_parts[0].replace("ms", ""))
        except ValueError:
            print(f"response_timeè½¬æ¢å¤±è´¥: {whitelist_line}")
            response_time = 60000  # å•ä½æ¯«ç§’ï¼Œè½¬æ¢å¤±è´¥ç»™ä¸ª60ç§’
        if response_time < 2000:  # 2sä»¥å†…çš„é«˜å“åº”æº
            process_channel_line(",".join(whitelist_parts[1:]))

# ======= ä½“è‚²èµ›äº‹æ•°æ®å¤„ç† =======

# 5. å¤„ç†ä½“è‚²èµ›äº‹æ•°æ®
# å°†æ—¥æœŸç»Ÿä¸€æ ¼å¼åŒ–ä¸ºMM-DDæ ¼å¼
normalized_tyss_lines = [normalize_date_to_md(s) for s in tyss_lines]

# 6. å¤„ç†AKTVæº
aktv_lines = []  # AKTV
aktv_url = "https://aktv.space/live.m3u"  # AKTV

aktv_text = get_http_response(aktv_url)
if aktv_text:
    print("AKTVæˆåŠŸè·å–å†…å®¹")
    aktv_text = convert_m3u_to_txt(aktv_text)
    aktv_lines = aktv_text.strip().split('\n')
else:
    print("AKTVè¯·æ±‚å¤±è´¥ï¼Œä»æœ¬åœ°è·å–ï¼")
    aktv_lines = read_txt_to_array('scripts/livesource0/æ‰‹å·¥åŒº/AKTV.txt')

# 7. è¿‡æ»¤å’Œç”Ÿæˆä½“è‚²èµ›äº‹é¡µé¢
# è¿‡æ»¤txtä¸­ä½“è‚²èµ›äº‹
keywords_to_exclude_tiyu_txt = ["ç‰ç‰è½¯ä»¶", "æ¦´èŠ’ç”µè§†","å…¬ä¼—å·","éº»è±†","ã€Œå›çœ‹ã€"]
normalized_tyss_lines = filter_lines(normalized_tyss_lines, keywords_to_exclude_tiyu_txt)
normalized_tyss_lines = custom_tyss_sort(set(normalized_tyss_lines))

# è¿‡æ»¤tiyué¡µé¢ä¸­ä½“è‚²èµ›äº‹
keywords_to_exclude_tiyu = ["ç‰ç‰è½¯ä»¶", "æ¦´èŠ’ç”µè§†","å…¬ä¼—å·","å’ªè§†é€š","éº»è±†","ã€Œå›çœ‹ã€"]
filtered_tyss_lines = filter_lines(normalized_tyss_lines, keywords_to_exclude_tiyu)
generate_playlist_html(filtered_tyss_lines, 'output/sport.html')

# ======= ç»“æŸä½“è‚²èµ›äº‹æ•°æ®å¤„ç† =======
# 8. å‡†å¤‡ä»Šæ—¥æ¨èå’Œç‰ˆæœ¬ä¿¡æ¯
daily_mtv = "ä»Šæ—¥æ¨è," + get_random_url('scripts/livesource0/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt')

# è·å–å½“å‰çš„UTCæ—¶é—´å¹¶è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´
utc_time = datetime.now(timezone.utc)
beijing_time = utc_time + timedelta(hours=8)
formatted_time = beijing_time.strftime("%Y%m%d %H:%M:%S")

daily_mtv = "ğŸ’¯æ¨è," + get_random_url('scripts/livesource0/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt')
daily_mtv1 = "ğŸ¤«ä½è°ƒ," + get_random_url('scripts/livesource0/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt')
daily_mtv2 = "ğŸŸ¢ä½¿ç”¨," + get_random_url('scripts/livesource0/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt')
daily_mtv3 = "âš ï¸ç¦æ­¢," + get_random_url('scripts/livesource0/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt')
daily_mtv4 = "ğŸš«è´©å–," + get_random_url('scripts/livesource0/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨è.txt')

about_video1 = "https://gitee.com/xiaoran67/update/raw/master/scripts/livesource0/about1080p.mp4"
about_video2 = "https://gitlab.com/xiaoran67/update/-/raw/main/scripts/livesource0/about1080p.mp4"

version = formatted_time + "," + get_random_url('scripts/livesource0/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨å°.txt')
about = "ğŸ‘¨æ½‡ç„¶," + get_random_url('scripts/livesource0/æ‰‹å·¥åŒº/ä»Šæ—¥æ¨å°.txt')

# 9. å¢åŠ æ‰‹å·¥åŒº
print(f"å¤„ç†æ‰‹å·¥åŒº...")
# ä½¿ç”¨æ‚¨çš„æ‰‹å·¥åŒºè·¯å¾„
hubei_lines = hubei_lines + read_txt_to_array('scripts/livesource0/æ‰‹å·¥åŒº/æ¹–åŒ—é¢‘é“.txt')

# 10. å®šä¹‰è¾“å‡ºå†…å®¹
# ======= å®Œæ•´ç‰ˆå†…å®¹å®šä¹‰ =======
# å®Œæ•´ç‰ˆå†…å®¹ ğŸ“¡ åŒ…å«æ‰€æœ‰é¢‘é“åˆ†ç±»

all_lines = ["ğŸŒå¤®è§†é¢‘é“,#genre#"] + sort_data(["CCTV1", "CCTV2", "CCTV3", "CCTV4", "CCTV5", "CCTV6", "CCTV7", "CCTV8", "CCTV9", "CCTV10", "CCTV11", "CCTV12", "CCTV13", "CCTV14", "CCTV15", "CCTV16", "CCTV17"], set(correct_name_data(corrections_name, yangshi_lines))) + ['\n'] + \
    ["ğŸ“¡å«è§†é¢‘é“,#genre#"] + sort_data(weishi_dictionary, set(correct_name_data(corrections_name, weishi_lines))) + ['\n'] + \
    ["ğŸ åŒ—äº¬é¢‘é“,#genre#"] + sort_data(beijing_dictionary,set(correct_name_data(corrections_name,beijing_lines))) + ['\n'] + \
    ["ğŸ™ï¸ä¸Šæµ·é¢‘é“,#genre#"] + sort_data(shanghai_dictionary,set(correct_name_data(corrections_name,shanghai_lines))) + ['\n'] + \
    ["ğŸ¡å¤©æ´¥é¢‘é“,#genre#"] + sort_data(tianjin_dictionary,set(correct_name_data(corrections_name,tianjin_lines))) + ['\n'] + \
    ["ğŸï¸é‡åº†é¢‘é“,#genre#"] + sort_data(chongqing_dictionary,set(correct_name_data(corrections_name,chongqing_lines))) + ['\n'] + \
    ["ğŸ…å¹¿ä¸œé¢‘é“,#genre#"] + sort_data(guangdong_dictionary,set(correct_name_data(corrections_name,guangdong_lines))) + ['\n'] + \
    ["ğŸæ±Ÿè‹é¢‘é“,#genre#"] + sort_data(jiangsu_dictionary,set(correct_name_data(corrections_name,jiangsu_lines))) + ['\n'] + \
    ["ğŸŒŠæµ™æ±Ÿé¢‘é“,#genre#"] + sort_data(zhejiang_dictionary,set(correct_name_data(corrections_name,zhejiang_lines))) + ['\n'] + \
    ["â›°ï¸å±±ä¸œé¢‘é“,#genre#"] + sort_data(shandong_dictionary,set(correct_name_data(corrections_name,shandong_lines))) + ['\n'] + \
    ["ğŸŒ¾æ²³å—é¢‘é“,#genre#"] + sort_data(henan_dictionary,set(correct_name_data(corrections_name,henan_lines))) + ['\n'] + \
    ["ğŸ¼å››å·é¢‘é“,#genre#"] + sort_data(sichuan_dictionary,set(correct_name_data(corrections_name,sichuan_lines))) + ['\n'] + \
    ["ğŸŒ‰æ²³åŒ—é¢‘é“,#genre#"] + sort_data(hebei_dictionary,set(correct_name_data(corrections_name,hebei_lines))) + ['\n'] + \
    ["ğŸŒ¶ï¸æ¹–å—é¢‘é“,#genre#"] + sort_data(hunan_dictionary,set(correct_name_data(corrections_name,hunan_lines))) + ['\n'] + \
    ["ğŸ¯æ¹–åŒ—é¢‘é“,#genre#"] + sort_data(hubei_dictionary,set(correct_name_data(corrections_name,hubei_lines))) + ['\n'] + \
    ["ğŸ¨å®‰å¾½é¢‘é“,#genre#"] + sort_data(anhui_dictionary,set(correct_name_data(corrections_name,anhui_lines))) + ['\n'] + \
    ["ğŸµç¦å»ºé¢‘é“,#genre#"] + sort_data(fujian_dictionary,set(correct_name_data(corrections_name,fujian_lines))) + ['\n'] + \
    ["ğŸ—¿é™•è¥¿é¢‘é“,#genre#"] + sort_data(shanxi1_dictionary,set(correct_name_data(corrections_name,shanxi1_lines))) + ['\n'] + \
    ["ğŸ¯è¾½å®é¢‘é“,#genre#"] + sort_data(liaoning_dictionary, set(correct_name_data(corrections_name, liaoning_lines))) + ['\n'] + \
    ["â›©ï¸æ±Ÿè¥¿é¢‘é“,#genre#"] + sort_data(jiangxi_dictionary, set(correct_name_data(corrections_name, jiangxi_lines))) + ['\n'] + \
    ["â„ï¸é»‘é¾™æ±Ÿå°,#genre#"] + sort_data(heilongjiang_dictionary,set(correct_name_data(corrections_name,heilongjiang_lines))) + ['\n'] + \
    ["ğŸå‰æ—é¢‘é“,#genre#"] + sort_data(jilin_dictionary,set(correct_name_data(corrections_name,jilin_lines))) + ['\n'] + \
    ["ğŸ®å±±è¥¿é¢‘é“,#genre#"] + sort_data(shanxi2_dictionary,set(correct_name_data(corrections_name,shanxi2_lines))) + ['\n'] + \
    ["ğŸ˜å¹¿è¥¿é¢‘é“,#genre#"] + sort_data(guangxi_dictionary,set(correct_name_data(corrections_name,guangxi_lines))) + ['\n'] + \
    ["â˜ï¸äº‘å—é¢‘é“,#genre#"] + sort_data(yunnan_dictionary,set(correct_name_data(corrections_name,yunnan_lines))) + ['\n'] + \
    ["ğŸ¶è´µå·é¢‘é“,#genre#"] + sort_data(guizhou_dictionary,set(correct_name_data(corrections_name,guizhou_lines))) + ['\n'] + \
    ["ğŸ«ç”˜è‚ƒé¢‘é“,#genre#"] + sort_data(gansu_dictionary,set(correct_name_data(corrections_name,gansu_lines))) + ['\n'] + \
    ["ğŸå†…è’™å¤å°,#genre#"] + sort_data(neimenggu_dictionary,set(correct_name_data(corrections_name,neimenggu_lines))) + ['\n'] + \
    ["ğŸ‡æ–°ç–†é¢‘é“,#genre#"] + sort_data(xinjiang_dictionary,set(correct_name_data(corrections_name,xinjiang_lines))) + ['\n'] + \
    ["ğŸŒ´æµ·å—é¢‘é“,#genre#"] + sort_data(hainan_dictionary,set(correct_name_data(corrections_name,hainan_lines))) + ['\n'] + \
    ["ğŸœï¸å®å¤é¢‘é“,#genre#"] + sort_data(ningxia_dictionary,set(correct_name_data(corrections_name,ningxia_lines))) + ['\n'] + \
    ["ğŸ”ï¸é’æµ·é¢‘é“,#genre#"] + sort_data(qinghai_dictionary,set(correct_name_data(corrections_name,qinghai_lines))) + ['\n'] + \
    ["â›°ï¸è¥¿è—é¢‘é“,#genre#"] + sort_data(xizang_dictionary,set(correct_name_data(corrections_name,xizang_lines))) + ['\n'] + \
    ["ğŸ“°æ–°é—»é¢‘é“,#genre#"] + sort_data(news_dictionary,set(correct_name_data(corrections_name,news_lines))) + ['\n'] + \
    ["ğŸ”¢æ•°å­—é¢‘é“,#genre#"] + sort_data(shuzi_dictionary,set(correct_name_data(corrections_name,shuzi_lines))) + ['\n'] + \
    ["ğŸ¬ç”µå½±é¢‘é“,#genre#"] + sort_data(dianying_dictionary,set(correct_name_data(corrections_name,dianying_lines))) + ['\n'] + \
    ["ğŸ™ï¸è§£è¯´é¢‘é“,#genre#"] + sort_data(jieshuo_dictionary,set(correct_name_data(corrections_name,jieshuo_lines))) + ['\n'] + \
    ["ğŸ­ç»¼è‰ºé¢‘é“,#genre#"] + sort_data(zongyi_dictionary,set(correct_name_data(corrections_name,zongyi_lines))) + ['\n'] + \
    ["ğŸ¯è™ç‰™ç›´æ’­,#genre#"] + sort_data(huya_dictionary,set(correct_name_data(corrections_name,huya_lines))) + ['\n'] + \
    ["ğŸ¬æ–—é±¼ç›´æ’­,#genre#"] + sort_data(douyu_dictionary,set(correct_name_data(corrections_name,douyu_lines))) + ['\n'] + \
    ["ğŸ‡­ğŸ‡°é¦™æ¸¯é¢‘é“,#genre#"] + sort_data(xianggang_dictionary,set(correct_name_data(corrections_name,xianggang_lines))) + ['\n'] + \
    ["ğŸ‡²ğŸ‡´æ¾³é—¨é¢‘é“,#genre#"] + sort_data(aomen_dictionary,set(correct_name_data(corrections_name,aomen_lines))) + ['\n'] + \
    ["ğŸ‡¨ğŸ‡³ä¸­å›½é¢‘é“,#genre#"] + sort_data(china_dictionary,set(correct_name_data(corrections_name,china_lines))) + ['\n'] + \
    ["ğŸŒå›½é™…é¢‘é“,#genre#"] + sort_data(guoji_dictionary,set(correct_name_data(corrections_name,guoji_lines))) + ['\n'] + \
    ["ğŸ‡¨ğŸ‡³æ¸¯Â·æ¾³Â·å°,#genre#"] + sort_data(gangaotai_dictionary,set(correct_name_data(corrections_name,gangaotai_lines))) + ['\n'] + \
    ["ğŸ“ºç”µÂ·è§†Â·å‰§,#genre#"] + sort_data(dianshiju_dictionary,set(correct_name_data(corrections_name,dianshiju_lines))) + ['\n'] + \
    ["ğŸ“»æ”¶Â·éŸ³Â·æœº,#genre#"] + sort_data(radio_dictionary,set(correct_name_data(corrections_name,radio_lines))) + ['\n'] + \
    ["ğŸ¶åŠ¨Â·ç”»Â·ç‰‡,#genre#"] + sort_data(donghuapian_dictionary,set(correct_name_data(corrections_name,donghuapian_lines))) + ['\n'] + \
    ["ğŸï¸çºªÂ·å½•Â·ç‰‡,#genre#"] + sort_data(jilupian_dictionary,set(correct_name_data(corrections_name,jilupian_lines))) + ['\n'] + \
    ["ğŸ®æ¸¸æˆé¢‘é“,#genre#"] + sort_data(youxi_dictionary,set(correct_name_data(corrections_name,youxi_lines))) + ['\n'] + \
    ["ğŸ­æˆæ›²é¢‘é“,#genre#"] + sort_data(xiqu_dictionary,set(correct_name_data(corrections_name,xiqu_lines))) + ['\n'] + \
    ["ğŸµéŸ³ä¹é¢‘é“,#genre#"] + sort_data(yinyue_dictionary,set(correct_name_data(corrections_name,yinyue_lines))) + ['\n'] + \
    ["ğŸ‰æ˜¥æ™šé¢‘é“,#genre#"] + sort_data(chunwan_dictionary,set(correct_name_data(corrections_name,chunwan_lines))) + ['\n'] + \
    ["ğŸ†ä½“è‚²èµ›äº‹,#genre#"] + normalized_tyss_lines + ['\n'] + \
    ["âš½ä½“è‚²é¢‘é“,#genre#"] + sort_data(tiyu_dictionary,set(correct_name_data(corrections_name,tiyu_lines))) + ['\n'] + \
    ["ğŸ€å’ªå’•èµ›äº‹,#genre#"] + mgss_lines + ['\n'] + \
    ["ğŸ“¹ç›´æ’­ä¸­å›½,#genre#"] + sort_data(zhibozhongguo_dictionary,set(correct_name_data(corrections_name,zhibozhongguo_lines))) + ['\n'] + \
    ["â“å…¶ä»–é¢‘é“,#genre#"] + sorted(set(correct_name_data(corrections_name,other_lines))) + ['\n'] + \
    ["ğŸ•’æ›´æ–°æ—¶é—´,#genre#"] + [version] + [about] + [daily_mtv] + [daily_mtv1] + [daily_mtv2] + [daily_mtv3] + [daily_mtv4] + read_txt_to_array('scripts/livesource0/æ‰‹å·¥åŒº/about.txt') + ['\n']

# ======= ç²¾ç®€ç‰ˆå†…å®¹å®šä¹‰ =======
# ç²¾ç®€ç‰ˆå†…å®¹ ğŸ›°ï¸ åŒ…å«æ ¸å¿ƒé¢‘é“åˆ†ç±»
all_lines_simple = ["ğŸŒå¤®  è§†,#genre#"] + sort_data(["CCTV1", "CCTV2", "CCTV3", "CCTV4", "CCTV5", "CCTV6", "CCTV7", "CCTV8", "CCTV9", "CCTV10", "CCTV11", "CCTV12", "CCTV13", "CCTV14", "CCTV15", "CCTV16", "CCTV17"], set(correct_name_data(corrections_name, yangshi_lines))) + ['\n'] + \
    ["ğŸ“¡å«  è§†,#genre#"] + sort_data(weishi_dictionary, set(correct_name_data(corrections_name, weishi_lines))) + ['\n'] + \
    ["ğŸ åœ°æ–¹å°,#genre#"] + \
    sort_data(beijing_dictionary,set(correct_name_data(corrections_name,beijing_lines))) + \
    sort_data(shanghai_dictionary,set(correct_name_data(corrections_name,shanghai_lines))) + \
    sort_data(tianjin_dictionary,set(correct_name_data(corrections_name,tianjin_lines))) + \
    sort_data(chongqing_dictionary,set(correct_name_data(corrections_name,chongqing_lines))) + \
    sort_data(guangdong_dictionary,set(correct_name_data(corrections_name,guangdong_lines))) + \
    sort_data(jiangsu_dictionary,set(correct_name_data(corrections_name,jiangsu_lines))) + \
    sort_data(zhejiang_dictionary,set(correct_name_data(corrections_name,zhejiang_lines))) + \
    sort_data(shandong_dictionary,set(correct_name_data(corrections_name,shandong_lines))) + \
    sort_data(henan_dictionary,set(correct_name_data(corrections_name,henan_lines))) + \
    sort_data(sichuan_dictionary,set(correct_name_data(corrections_name,sichuan_lines))) + \
    sort_data(hebei_dictionary,set(correct_name_data(corrections_name,hebei_lines))) + \
    sort_data(hunan_dictionary,set(correct_name_data(corrections_name,hunan_lines))) + \
    sort_data(hubei_dictionary,set(correct_name_data(corrections_name,hubei_lines))) + \
    sort_data(anhui_dictionary,set(correct_name_data(corrections_name,anhui_lines))) + \
    sort_data(fujian_dictionary,set(correct_name_data(corrections_name,fujian_lines))) + \
    sort_data(shanxi1_dictionary,set(correct_name_data(corrections_name,shanxi1_lines))) + \
    sort_data(liaoning_dictionary,set(correct_name_data(corrections_name,liaoning_lines))) + \
    sort_data(jiangxi_dictionary,set(correct_name_data(corrections_name,jiangxi_lines))) + \
    sort_data(heilongjiang_dictionary,set(correct_name_data(corrections_name,heilongjiang_lines))) + \
    sort_data(jilin_dictionary,set(correct_name_data(corrections_name,jilin_lines))) + \
    sort_data(shanxi2_dictionary,set(correct_name_data(corrections_name,shanxi2_lines))) + \
    sort_data(guangxi_dictionary,set(correct_name_data(corrections_name,guangxi_lines))) + \
    sort_data(yunnan_dictionary,set(correct_name_data(corrections_name,yunnan_lines))) + \
    sort_data(guizhou_dictionary,set(correct_name_data(corrections_name,guizhou_lines))) + \
    sort_data(gansu_dictionary,set(correct_name_data(corrections_name,gansu_lines))) + \
    sort_data(neimenggu_dictionary,set(correct_name_data(corrections_name,neimenggu_lines))) + \
    sort_data(xinjiang_dictionary,set(correct_name_data(corrections_name,xinjiang_lines))) + \
    sort_data(hainan_dictionary,set(correct_name_data(corrections_name,hainan_lines))) + \
    sort_data(ningxia_dictionary,set(correct_name_data(corrections_name,ningxia_lines))) + \
    sort_data(qinghai_dictionary,set(correct_name_data(corrections_name,qinghai_lines))) + \
    sort_data(xizang_dictionary,set(correct_name_data(corrections_name,xizang_lines))) + ['\n'] + \
    ["ğŸ•’æ›´æ–°æ—¶é—´,#genre#"] + [version] + [about] + [daily_mtv] + [daily_mtv1] + [daily_mtv2] + [daily_mtv3] + [daily_mtv4] + read_txt_to_array('scripts/livesource0/æ‰‹å·¥åŒº/about.txt') + ['\n']
            
# ======= å®šåˆ¶ç‰ˆå†…å®¹å®šä¹‰ =======
# å®šåˆ¶ç‰ˆå†…å®¹ ğŸŒğŸ“¡ğŸ›°ï¸ğŸ“ºğŸ™ï¸ğŸ ğŸ§§ğŸ® åŒ…å«å®šåˆ¶é¢‘é“åˆ†ç±»

all_lines_custom = ["ğŸŒå¤®è§†é¢‘é“,#genre#"] + sort_data(["CCTV1", "CCTV2", "CCTV3", "CCTV4", "CCTV5", "CCTV6", "CCTV7", "CCTV8", "CCTV9", "CCTV10", "CCTV11", "CCTV12", "CCTV13", "CCTV14", "CCTV15", "CCTV16", "CCTV17"], set(correct_name_data(corrections_name, yangshi_lines))) + ['\n'] + \
    ["ğŸ“¡å«è§†é¢‘é“,#genre#"] + sort_data(weishi_dictionary, set(correct_name_data(corrections_name, weishi_lines))) + ['\n'] + \
    ["ğŸ åœ°Â·æ–¹Â·å°,#genre#"] + \
    sort_data(beijing_dictionary,set(correct_name_data(corrections_name,beijing_lines))) + \
    sort_data(shanghai_dictionary,set(correct_name_data(corrections_name,shanghai_lines))) + \
    sort_data(tianjin_dictionary,set(correct_name_data(corrections_name,tianjin_lines))) + \
    sort_data(chongqing_dictionary,set(correct_name_data(corrections_name,chongqing_lines))) + \
    sort_data(guangdong_dictionary,set(correct_name_data(corrections_name,guangdong_lines))) + \
    sort_data(jiangsu_dictionary,set(correct_name_data(corrections_name,jiangsu_lines))) + \
    sort_data(zhejiang_dictionary,set(correct_name_data(corrections_name,zhejiang_lines))) + \
    sort_data(shandong_dictionary,set(correct_name_data(corrections_name,shandong_lines))) + \
    sort_data(henan_dictionary,set(correct_name_data(corrections_name,henan_lines))) + \
    sort_data(sichuan_dictionary,set(correct_name_data(corrections_name,sichuan_lines))) + \
    sort_data(hebei_dictionary,set(correct_name_data(corrections_name,hebei_lines))) + \
    sort_data(hunan_dictionary,set(correct_name_data(corrections_name,hunan_lines))) + \
    sort_data(hubei_dictionary,set(correct_name_data(corrections_name,hubei_lines))) + \
    sort_data(anhui_dictionary,set(correct_name_data(corrections_name,anhui_lines))) + \
    sort_data(fujian_dictionary,set(correct_name_data(corrections_name,fujian_lines))) + \
    sort_data(shanxi1_dictionary,set(correct_name_data(corrections_name,shanxi1_lines))) + \
    sort_data(liaoning_dictionary,set(correct_name_data(corrections_name,liaoning_lines))) + \
    sort_data(jiangxi_dictionary,set(correct_name_data(corrections_name,jiangxi_lines))) + \
    sort_data(heilongjiang_dictionary,set(correct_name_data(corrections_name,heilongjiang_lines))) + \
    sort_data(jilin_dictionary,set(correct_name_data(corrections_name,jilin_lines))) + \
    sort_data(shanxi2_dictionary,set(correct_name_data(corrections_name,shanxi2_lines))) + \
    sort_data(guangxi_dictionary,set(correct_name_data(corrections_name,guangxi_lines))) + \
    sort_data(yunnan_dictionary,set(correct_name_data(corrections_name,yunnan_lines))) + \
    sort_data(guizhou_dictionary,set(correct_name_data(corrections_name,guizhou_lines))) + \
    sort_data(gansu_dictionary,set(correct_name_data(corrections_name,gansu_lines))) + \
    sort_data(neimenggu_dictionary,set(correct_name_data(corrections_name,neimenggu_lines))) + \
    sort_data(xinjiang_dictionary,set(correct_name_data(corrections_name,xinjiang_lines))) + \
    sort_data(hainan_dictionary,set(correct_name_data(corrections_name,hainan_lines))) + \
    sort_data(ningxia_dictionary,set(correct_name_data(corrections_name,ningxia_lines))) + \
    sort_data(qinghai_dictionary,set(correct_name_data(corrections_name,qinghai_lines))) + \
    sort_data(xizang_dictionary,set(correct_name_data(corrections_name,xizang_lines))) + ['\n'] + \
    ["ğŸ“°æ–°é—»é¢‘é“,#genre#"] + sort_data(news_dictionary,set(correct_name_data(corrections_name,news_lines))) + ['\n'] + \
    ["ğŸ”¢æ•°å­—é¢‘é“,#genre#"] + sort_data(shuzi_dictionary,set(correct_name_data(corrections_name,shuzi_lines))) + ['\n'] + \
    ["ğŸ¬ç”µå½±é¢‘é“,#genre#"] + sort_data(dianying_dictionary,set(correct_name_data(corrections_name,dianying_lines))) + ['\n'] + \
    ["ğŸ™ï¸è§£è¯´é¢‘é“,#genre#"] + sort_data(jieshuo_dictionary,set(correct_name_data(corrections_name,jieshuo_lines))) + ['\n'] + \
    ["ğŸ­ç»¼è‰ºé¢‘é“,#genre#"] + sort_data(zongyi_dictionary,set(correct_name_data(corrections_name,zongyi_lines))) + ['\n'] + \
    ["ğŸ¯è™ç‰™ç›´æ’­,#genre#"] + sort_data(huya_dictionary,set(correct_name_data(corrections_name,huya_lines))) + ['\n'] + \
    ["ğŸ¬æ–—é±¼ç›´æ’­,#genre#"] + sort_data(douyu_dictionary,set(correct_name_data(corrections_name,douyu_lines))) + ['\n'] + \
    ["ğŸ‡­ğŸ‡°é¦™æ¸¯é¢‘é“,#genre#"] + sort_data(xianggang_dictionary,set(correct_name_data(corrections_name,xianggang_lines))) + ['\n'] + \
    ["ğŸ‡²ğŸ‡´æ¾³é—¨é¢‘é“,#genre#"] + sort_data(aomen_dictionary,set(correct_name_data(corrections_name,aomen_lines))) + ['\n'] + \
    ["ğŸ‡¨ğŸ‡³ä¸­å›½é¢‘é“,#genre#"] + sort_data(china_dictionary,set(correct_name_data(corrections_name,china_lines))) + ['\n'] + \
    ["ğŸŒå›½é™…é¢‘é“,#genre#"] + sort_data(guoji_dictionary,set(correct_name_data(corrections_name,guoji_lines))) + ['\n'] + \
    ["ğŸ‡¨ğŸ‡³æ¸¯Â·æ¾³Â·å°,#genre#"] + sort_data(gangaotai_dictionary,set(correct_name_data(corrections_name,gangaotai_lines))) + ['\n'] + \
    ["ğŸ“ºç”µÂ·è§†Â·å‰§,#genre#"] + sort_data(dianshiju_dictionary,set(correct_name_data(corrections_name,dianshiju_lines))) + ['\n'] + \
    ["ğŸ“»æ”¶Â·éŸ³Â·æœº,#genre#"] + sort_data(radio_dictionary,set(correct_name_data(corrections_name,radio_lines))) + ['\n'] + \
    ["ğŸ¶åŠ¨Â·ç”»Â·ç‰‡,#genre#"] + sort_data(donghuapian_dictionary,set(correct_name_data(corrections_name,donghuapian_lines))) + ['\n'] + \
    ["ğŸï¸çºªÂ·å½•Â·ç‰‡,#genre#"] + sort_data(jilupian_dictionary,set(correct_name_data(corrections_name,jilupian_lines))) + ['\n'] + \
    ["ğŸ®æ¸¸æˆé¢‘é“,#genre#"] + sort_data(youxi_dictionary,set(correct_name_data(corrections_name,youxi_lines))) + ['\n'] + \
    ["ğŸ­æˆæ›²é¢‘é“,#genre#"] + sort_data(xiqu_dictionary,set(correct_name_data(corrections_name,xiqu_lines))) + ['\n'] + \
    ["ğŸµéŸ³ä¹é¢‘é“,#genre#"] + sort_data(yinyue_dictionary,set(correct_name_data(corrections_name,yinyue_lines))) + ['\n'] + \
    ["ğŸ‰æ˜¥æ™šé¢‘é“,#genre#"] + sort_data(chunwan_dictionary,set(correct_name_data(corrections_name,chunwan_lines))) + ['\n'] + \
    ["ğŸ†ä½“è‚²èµ›äº‹,#genre#"] + normalized_tyss_lines + ['\n'] + \
    ["âš½ä½“è‚²é¢‘é“,#genre#"] + sort_data(tiyu_dictionary,set(correct_name_data(corrections_name,tiyu_lines))) + ['\n'] + \
    ["ğŸ€å’ªå’•èµ›äº‹,#genre#"] + mgss_lines + ['\n'] + \
    ["ğŸ“¹ç›´æ’­ä¸­å›½,#genre#"] + sort_data(zhibozhongguo_dictionary,set(correct_name_data(corrections_name,zhibozhongguo_lines))) + ['\n'] + \
    ["â“å…¶ä»–é¢‘é“,#genre#"] + sorted(set(correct_name_data(corrections_name,other_lines))) + ['\n'] + \
    ["ğŸ•’æ›´æ–°æ—¶é—´,#genre#"] + [version] + [about] + [daily_mtv] + [daily_mtv1] + [daily_mtv2] + [daily_mtv3] + [daily_mtv4] + read_txt_to_array('scripts/livesource0/æ‰‹å·¥åŒº/about.txt') + ['\n']

# æ‰‹å·¥ä¸“åŒºç±»å‹ï¼šè¯»å–é¢„è®¾çš„é™æ€ä¼˜è´¨æºæ–‡ä»¶ï¼Œæ‰‹å·¥ç»´æŠ¤
# æ ¼å¼ï¼šread_txt_to_array('æ‰‹å·¥åŒº/æ–‡ä»¶å.txt')

# åŠ¨æ€èµ›äº‹ç±»å‹ï¼šè‡ªåŠ¨è·å–å¹¶å¤„ç†çš„å®æ—¶èµ›äº‹æ•°æ®  
# æ ¼å¼ï¼šnormalized_tyss_linesï¼ˆä½“è‚²èµ›äº‹ï¼‰ / mgss_linesï¼ˆå’ªå’•èµ›äº‹ï¼‰

# è‡ªåŠ¨åˆ†ç±»ç±»å‹ï¼šè„šæœ¬è‡ªåŠ¨åˆ†ç±»å¾—åˆ°çš„åœ°æ–¹é¢‘é“æ•°æ®
# æ ¼å¼1ï¼šsort_data(æ’åºå­—å…¸, set(correct_name_data(æ ¡æ­£å­—å…¸, æ•°æ®))) - æŒ‰æŒ‡å®šé¡ºåºæ’åº
# æ ¼å¼2ï¼šsorted(set(correct_name_data(æ ¡æ­£å­—å…¸, æ•°æ®))) - æŒ‰å­—æ¯é¡ºåºæ’åº

# å¤„ç†æµç¨‹è¯´æ˜ï¼š
# - read_txt_to_array(): ä»æ–‡ä»¶è¯»å–é™æ€é¢‘é“åˆ—è¡¨
# - å˜é‡å: ä½¿ç”¨åŠ¨æ€å¤„ç†çš„é¢‘é“æ•°æ®
# - sort_data(): æŒ‰è‡ªå®šä¹‰å­—å…¸é¡ºåºæ’åº
# - sorted(): æŒ‰å­—æ¯é¡ºåºæ’åº  
# - set(): æ•°æ®å»é‡
# - correct_name_data(): é¢‘é“åç§°æ ‡å‡†åŒ–æ ¡æ­£

# ç¤ºä¾‹è¯´æ˜
# æ‰‹å·¥ä¸“åŒº
# ["é¦™æ¸¯å°,#genre#"] + read_txt_to_array('ä¸“åŒº/â™ªé¦™æ¸¯å°.txt') + ['\n'] + \

# åŠ¨æ€èµ›äº‹  
# ["ä½“è‚²èµ›äº‹,#genre#"] + normalized_tyss_lines + ['\n'] + \

# è‡ªåŠ¨åˆ†ç±»ï¼ˆå­—å…¸æ’åºï¼‰
# ["å±±ä¸œ,#genre#"] + sort_data(shandong_dictionary,set(correct_name_data(corrections_name,shandong_lines))) + ['\n'] + \

# è‡ªåŠ¨åˆ†ç±»ï¼ˆå­—æ¯æ’åºï¼‰
# ["â˜˜ï¸æ±Ÿè‹,#genre#"] + sorted(set(correct_name_data(corrections_name,jsu_lines))) + ['\n'] + \

# 11. ä¿å­˜è¾“å‡ºæ–‡ä»¶
output_full = "output/full.txt"
output_lite = "output/lite.txt" 
output_custom = "output/custom.txt"
others_file = "output/others.txt"

try:
    # ä¿å­˜å®Œæ•´ç‰ˆ
    with open(output_full, 'w', encoding='utf-8') as f:
        for line in all_lines:
            f.write(line + '\n')
    print(f"å®Œæ•´ç‰ˆå·²ä¿å­˜åˆ°æ–‡ä»¶: {output_full}")

    # ä¿å­˜ç²¾ç®€ç‰ˆ
    with open(output_lite, 'w', encoding='utf-8') as f:
        for line in all_lines_simple:
            f.write(line + '\n')
    print(f"ç²¾ç®€ç‰ˆå·²ä¿å­˜åˆ°æ–‡ä»¶: {output_lite}")

    # ä¿å­˜å®šåˆ¶ç‰ˆ
    with open(output_custom, 'w', encoding='utf-8') as f:
        for line in all_lines_custom:
            f.write(line + '\n')
    print(f"å®šåˆ¶ç‰ˆå·²ä¿å­˜åˆ°æ–‡ä»¶: {output_custom}")

    # ä¿å­˜å…¶ä»–é¢‘é“
    with open(others_file, 'w', encoding='utf-8') as f:
        for line in other_lines:
            f.write(line + '\n')
    print(f"å…¶ä»–é¢‘é“å·²ä¿å­˜åˆ°æ–‡ä»¶: {others_file}")

except Exception as e:
    print(f"ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

# 12. ç”ŸæˆM3Uæ–‡ä»¶
channels_logos = read_txt_to_array('scripts/livesource0/logo.txt')  # è¯»å…¥logoåº“
make_m3u(output_full, output_full.replace(".txt", ".m3u"))
make_m3u(output_lite, output_lite.replace(".txt", ".m3u"))
make_m3u(output_custom, output_custom.replace(".txt", ".m3u"))

# ======= æ‰§è¡Œç»Ÿè®¡å’Œæ—¥å¿— =======

# æ‰§è¡Œç»“æŸæ—¶é—´
timeend = datetime.now()

# è®¡ç®—æ—¶é—´å·®
elapsed_time = timeend - timestart
total_seconds = elapsed_time.total_seconds()

# è½¬æ¢ä¸ºåˆ†é’Ÿå’Œç§’
minutes = int(total_seconds // 60)
seconds = int(total_seconds % 60)

# æ ¼å¼åŒ–å¼€å§‹å’Œç»“æŸæ—¶é—´
timestart_str = timestart.strftime("%Y%m%d_%H_%M_%S")
timeend_str = timeend.strftime("%Y%m%d_%H_%M_%S")

print(f"å¼€å§‹æ—¶é—´: {timestart_str}")
print(f"ç»“æŸæ—¶é—´: {timeend_str}")
print(f"æ‰§è¡Œæ—¶é—´: {minutes} åˆ† {seconds} ç§’")

# ç»Ÿè®¡ä¿¡æ¯
combined_blacklist_hj = len(combined_blacklist)
all_lines_hj = len(all_lines)
other_lines_hj = len(other_lines)
print(f"é»‘åå•è¡Œæ•°: {combined_blacklist_hj} ")
print(f"å®Œæ•´æºè¡Œæ•°: {all_lines_hj} ")
print(f"å…¶å®ƒæºè¡Œæ•°: {other_lines_hj} ")

print("\n=== è¾“å‡ºæ–‡ä»¶ç»Ÿè®¡ ===")
output_files = [
    'output/full.txt', 'output/lite.txt', 'output/custom.txt', 
    'output/others.txt', 'output/full.m3u', 'output/lite.m3u', 
    'output/custom.m3u', 'output/sport.html'  # âœ… sport.htmlåœ¨å½“å‰ç›®å½•
]
for file_path in output_files:
    if os.path.exists(file_path):
        file_size = os.path.getsize(file_path)
        print(f"âœ… {file_path} - {file_size} å­—èŠ‚")
    else:
        print(f"âŒ {file_path} - æ–‡ä»¶æœªæ‰¾åˆ°")
